\documentclass[specialist, subf, href, colorlinks=true, 12pt, times, mtpro, final]{disser}
\usepackage [russian] {babel}
\usepackage [utf8] {inputenc}
\usepackage {amsmath}
\usepackage {amsthm}
\usepackage {amssymb}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{amsfonts}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage{algorithm}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{pdfpages}

\theoremstyle{definition}
\newtheorem{defn}{Определение}[section]
\newtheorem{example}{Пример}[section]
\newtheorem{state}{Утверждение}[section]

\definecolor{linkcolor}{HTML}{0000FF}
\definecolor{urlcolor}{HTML}{0000FF}
\hypersetup{pdfstartview = FitH, linkcolor = linkcolor, urlcolor = urlcolor, colorlinks = true}

\begin{document}

\tableofcontents

\section* {Вопросы}
\begin{center}
    Вопросы по курсу <<Численные методы>> \\ 4 курс, II поток
\end{center}
{\small
\noindent 1. Погрешность метода и вычислительная погрешность. Пример неустойчивого алгоритма.\\
\noindent 2. Алгебраическая интерполяция. Многочлен Лагранжа.\\
\noindent 3. Константа Лебега интерполяционного процесса для равноотстоящих узлов.\\
\noindent 4. Многочлены Чебышёва и их свойства.\\
\noindent 5. Интерполяционные сплайны. Конструкция и обоснование кубического сплайна.\\
\noindent 6. Понятие об аппроксимационных сплайнах.\\
\noindent 7. Наилучшее приближение в линейном нормированном пространстве.\\
\noindent 8. Наилучшее приближение в гильбертовом пространстве.\\
\noindent 9. Дискретное преобразование Фурье. Идея быстрого дискретного преобразования Фурье.\\
\noindent 10. Наилучшее равномерное приближение многочленами.\\
\noindent 11. Квадратурные формулы интерполяционного типа.\\
\noindent 12. Ортотональные многочлены и квадратуры Гаусса.\\
\noindent 13. Составные квадратурные формулы. Правило Рунге для оценки погрешности.\\
\noindent 14. Основные приёмы для вычисления нерегулярных интегралов.\\
\noindent 15. Метод прогонки для решения трёхдиагональных систем. Корректность и устойчивость метода прогонки.\\
\noindent 16. Прямые методы решения систем линейных уравнений. Методы Гаусса и Холецкого.\\
\noindent 17. Прямые методы решения систем линейных уравнений. Методы отражений и вращений.\\
\noindent 18. Число обусловленности. Неравенства для ошибки и невязки.\\
\noindent 19. Метод простой итерации решения систем линейных уравнений.\\
\noindent 20. Оптимальный одношаговый итерационный метод.\\
\noindent 21. Оптимальный циклический итерационный метод.\\
\noindent 22. Обобщённый метод простой итерации.\\
\noindent 23. Методы Якоби и Гаусса -- Зейделя.\\
\noindent 24. Метод верхней релаксации.\\
\noindent 25. Метод наискорейшего градиентного спуска.\\
\noindent 26. Линейная задача наименьших квадратов. Метод нормального уравнения.\\
\noindent 27. Линейная задача наименьших квадратов. Методы QR-разложения и сингулярного разложения.\\
\noindent 28. Общая идея и примеры проекционных методов.\\
\noindent 29. Пространства Крылова. Понятие о методе сопряженных градиентов.\\
\noindent 30. Частичная проблема собственных значений.\\
\noindent 31. Полная проблема собственных значений. QR-алгоритм.\\
\noindent 32. Метод простой итерации для нелинейных уравнений.\\
\noindent 33. Метод Ньютона.\\
\noindent 34. Явный метод Эйлера для обыкновенных дифференциальных уравнений (ОДУ). Устойчивость. Локальная и глобальная ошибки.\\
\noindent 35. Явные методы Рунге -- Кутты.\\
\noindent 36. Неявные одношаговые методы решения ОДУ.\\
\noindent 37. Многошаговые методы решения ОДУ.\\
\noindent 38. Основы метода конечных элементов: вариационная постановка задачи, метод Ритца, базисные функции.\\
\noindent 39. Оценка точности приближения кусочно -- линейными функциями.\\
\noindent 40. Проекционная теорема в методе конечных элементов.\\
\noindent 41. Система уравнений в методе конечных элементов.\\
\noindent 42. Решение модельной задачи методом Фурье.\\
\noindent 43. Исследование устойчивости модельной задачи методом Фурье.\\
\noindent 44. Метод стрельбы для решения трехдиагональных систем.\\
\noindent 45. Пример аппроксимации уравнения и краевых условий.\\
\noindent 46. Определения аппроксимации и устойчивости.\\
\noindent 47. Определение сходимости. Теорема А.Ф.Филиппова.\\
\noindent 48. Интегро -- интерполяционный метод.\\
\noindent 49. Исследование устойчивости методом априорных оценок.\\
\noindent 50. Метод конечных разностей для уравнения Пуассона.\\
\noindent 51. Спектральный признак устойчивости и примеры его применения для аппроксимаций гиперболического уравнения.\\
\noindent 52. Принцип замороженных коэффициентов.\\
\noindent 53. Исследование устойчивости простейших схем для уравнения теплопроводности в равномерной метрике.\\
\noindent 54. Исследование устойчивости схемы с весами для уравнения теплопроводности в интегральной метрике.\\
}

\section {Погрешность метода и вычислительная погрешность. Пример неустойчивого алгоритма.}
    \hyperlink {lects.1}{Лекции} \\
    \textbf{Описание численного метода:}\\
    Постановка задачи $\rightarrow$ Приближенный метод решения $\rightarrow$ Оценка погрешности (\textit{погрешность метода}) $\rightarrow$ Оценка погрешности с учетом округлений (\textit{влияние вычислительной погрешности})  

	\begin{example}
    	Пример неустойчивого алгоритма \\
    	\hyperlink {lects.14}{Лекции}\\
    	Пусть требуется вычислить последовательность интегралов:
    	$$
    	    \int_0^{2\pi}{x^n e^{x-1}dx}, n = 1,2,3,...,N.
    	$$
    	Для построения численного алгоритма проведем интегрирование по частям:
    	$$
    	\begin{aligned}
    	    & I_n = \int_0^{2\pi}{x^n d(e^{x-1})} = x^n e^{x-1} \big|_0^1 - \int_0^{2\pi}{n x^{n-1} e^{x-1}dx} = 1 - n 	I_{n-1} \\
        	& I_1 = \frac{1}{e}
    	\end{aligned}
    	$$
    	Легко заметить, что при отсутствии ошибок округления погрешность метода равна нулю (точный метод!). Что будет при реальных вычислениях? Рассмотрим ситуацию, когда погрешность возникает только вследствие определения величины $I_n$. Введем обозначение для ошибки $z_n = I_n - I_n^*$. Тогда
    	$$
    	\begin{aligned}
    	    & I_n^* = 1 - n I_{n-1}^* \\
        	& z_n = - n z_{n-1} = n! (-1)^{n+1} z_1
    	\end{aligned}
    	$$
    	Погрешность очень быстро (факториально!) растёт. Очень скоро это приведет к сильному искажению искомого результата.
	\end{example}

\section {Алгебраическая интерполяция. Многочлен Лагранжа.}
	\hyperlink {lects.15}{Лекции}\\
	Отрезок $[a,b]$ разбит на $n$ узлов $a = x_1 < x_2 < ... < x_n = b$ и в них заданы $f_i$ - значения функции $f$ в $x_i$. Требуется найти многочлен $L_{n-1}(x)$ степени $n - 1$ чтоб $L_{n - 1}(x_i) = f_i$.

	\textbf{Многочлен Лагранжа $L_{n - 1}(x)$}
	$$
		\begin {array}{lr}
		L_{n - 1}(x) = \sum\limits_{i = 1}^{n} f_i \Phi_i (x), & \Phi_i(x) = \prod\limits_{j = 1, j\ne i}^{n} \frac{x-x_j}{x_i - x_j} \\
		\end {array}
	$$

	\begin{state}
		(\hyperlink {lects.16}{Лекции})Пусть n-я производная функции $f(x)$ непрерывна на $[a,b]$, тогда $\forall x \in [a,b] \ \  \exists \xi \in [a,b]$, что справедливо
		$$
			\begin{array}{lr}
			f(x) - L_{n-1}(x) = \frac {f^{(n)}(\xi)}{n!} \omega_n(x), & \omega_n(x) = \prod\limits_{i - 1}^{n}(x-x_i)
			\end{array}
		$$
	\end{state}
	Следствие:
	$$
		||f(x) - L_{n-1}(x) || \le \frac{||f^{(n)}(x)||}{n!} ||\omega_n||
	$$
	$$
		||g(x)|| = \underset{x\in [a,b]}{sup} |g(x)|
	$$


\section {Константа Лебега интерполяционного процесса для равноотстоящих узлов.}
	\hyperlink {lects.17}{Лекции}\\
	\textbf{Константа Лебега интерполяционного процесса}(\hyperlink {lects.17}{Подробно})
	$$
		\begin{array}{lr}
		\lambda_n = \underset{x\in [a,b]}{max} \sum\limits_{i = 1}^n |\Phi_i(x)|, & \Phi_i(x) = \prod\limits_{j = 1, j\ne i}^{n} \frac{x-x_j}{x_i - x_j} \\
		\end{array}
	$$
	
	\begin{state}
	$\lambda_n$ не зависит от длины отрезка интерполяции (\hyperlink {lects.17}{Утверждение 1})
	\end{state}
	
	Оценка снизу  для $\lambda_n (n \ge 2)$ (\hyperlink {lects.17}{Утверждение 2})
	$$
		\lambda \ge K \frac {2^n}{n^{3/2}}, 
	$$ 
	где $K$ не зависит от $n$. \\
	

\section {Многочлены Чебышёва и их свойства.}
	\hyperlink {lects.18}{Лекции}\\
	\textbf{Способы определения}
	\begin{enumerate}
		\item Рекуррентно 
			  $$
			  	\begin{array}{lcr}
			  		T_0(x) = 1, & T_1(x) = x, & T_{n+1}(x) = 2xT_n(x) - T_{n-1}(x)
			  	\end{array}
			  $$
		\item Тригонометрически $x\in[-1,1]$
			  $$
			  	T_n(x) = cos(n \ arccos(x))
			  $$
		\item Разностное уравнение
			  $$
			  	T_n(x) = \frac{1}{2}\left( \left( x + \sqrt{x^2 - 1} \right)^n + \left( x - \sqrt{x^2 - 1} \right)^n \right)
			  $$
	\end{enumerate}
	\textbf{Свойства}
	\begin{enumerate}
		\item $|T_n(x)| \le 1$ при $x \in [-1,1]$
		\item $T_n(-x) = (-1)^nT_n(x)$
		\item Коэффициент при старшем члене равен $2^{n - 1}$
		\item Все нули находятся на $[-1,1]$ в точках
			$$
				x_k = cos \frac{\pi (2k -1)}{2n} \ \ \ k = 1, ... , n
			$$
		\item Экстремумов на $[-1,1]$ $n+1$ штук в точках
			$$
				x_m = cos \frac{\pi m}{n} \ \ \ m = 0, ... , n
			$$ 
		\item Отображение на отрезок 
			$$
				T_n^{[a,b]} (x) = (b - a)^n 2^{1-2n} T_n \left( \frac{2x - (b+a)}{b-a} \right)
			$$
	\end{enumerate}

	Приведенный многочлен Чебышёва (со старшим коэффициентом 1)
	$$
		\bar {T_n} = 2^{1 - n}T_n
	$$
	
	\begin{state} (\hyperlink {lects.19}{Утверждение 3})
		Приведенный многочлен Чебышёва на $[-1,1]$ наименее отклоняется от нуля. То есть 
		$$
			\underset{[-1,1]}{max}|P_n(x)| \ge \underset{[-1,1]}{max} |\bar{T_n(x)}| = 2^{1-n},
		$$
		где $P_n(x)$ - любой многочлен степени $n$ с коэффициентом 1 при $n$-ой степени.
	\end{state}

	При фиксированном $n$ выбор корней $T_n(x)$ в качестве узлов для многочлена лагранжа является оптимальным.
	(\hyperlink {lects.20}{Минимизация оценки погрешности})

	

\section {Интерполяционные сплайны. Конструкция и обоснование кубического сплайна.}
	\hyperlink {lects.22}{Лекции}\\
	$a = x_0 < x_1 < ... < x_n = b$\\
	$P_m(x)$ - множество всех многочленов, степени не выше $m$.
	\begin{defn}
		Полиномиальный и интерполяционный сплайн \hyperlink {lects.22}{Лекции}
	\end{defn}

	\begin{defn}
		Естественный сплайн \hyperlink {lects.23}{Лекции, 2ой абзац} ($M_i$ - значения второй производной сплайна в $x_i$)
	\end{defn}

\section {Понятие об аппроксимационных сплайнах.}
	\hyperlink {lects.25}{Лекции}\\
	В отличии от интерполяционных не требуют пересчета всех коэффициентов при изменении одного (нескольких) значений $f_i$. Потому что строятся локально на каждом $[x_{i-1}, x_i]$ и зависят лишь от некотрого постоянного числа соседних $f_i$ (1, 2, 3 - обычно немного).
	
	Построение аппроксимационного сплайна 3ей степени (\hyperlink {lects.25}{Лекции}).
	
	Оценки для второй и первой производной и для разности (для сплайна 3ей степени) (\hyperlink {lects.27}{Лекции}):
	$$
		f(x) \in C^2[0,1], \ \ \ \ \ |f''(x)|_{[0,1]} < A_2
	$$
	\begin{itemize}
		\item Для второй производной $|B''_2(x_k)| \le A_2$
		\item Для первой производной $\underset{x}{max}|f'(x)-B'_2(x)| \le C_1 h A_2 \ \ \ \ \ \  \ C_1 = \frac{5}{2}$
		\item Просто для разности $\underset{x}{max}|f(x)-B_2(x)| \le C_0 A_2 h^2$
			
	\end{itemize}
	
	
	

\section {Наилучшее приближение в линейном нормированном пространстве.}
	\hyperlink {lects.28}{Лекции}\\
	\begin{defn}
	Задан элемент $f$ линейного нормированного пространства $\mathcal{L}$. Хотим найти его наилучшее приближение комбинацией заданных независимых элементов $g_1, ..., g_n \in \mathcal{L}$. Т.е. найти $x = \sum\limits_{j = 1}^{n}c_j^0g_j$ такой, что
	$$
		||f - x|| = ||f - \sum\limits_{j = 1}^{n}c_j^0g_j|| = \underset{c_1,...,c_n}{inf} ||f - \sum\limits_{j = 1}^{n}c_jg_j||
	$$
	\end{defn}

	Всегда существует, но не обязательно единственный. (\hyperlink {lects.28}{утв 1})
	
	\begin{defn}  
		Пространство $\mathcal{L}$ строго нормированно, если из условия
		$$
			||f+g|| = ||f||+||g|| \ \ \ \ \ \ \ \ \ \ ||f||,||g|| \ne 0
		$$
		следует $f = \alpha g, \alpha \ne 0$.
	\end{defn}

	В случае строго нормированного элемент наилучшего приближения единственный (\hyperlink {lects.29}{утв 2})

\section {Наилучшее приближение в гильбертовом пространстве.}
	\hyperlink {lects.29}{Лекции}\\
	Т.е. есть скалярное произведение $(x,y)$ и норма $||x||=\sqrt{(x,x)}$. \\
	Гильбертово пространство - строго нормированное (\hyperlink {lects.29}{утв 3}) \\
	
	Поиск наилучшего приближения - решение СЛАУ. (всегда имеет единственное решение \hyperlink {lects.30}{утв 4})
	$$
		Ac = b, \ \ \ \ \ \ a_{ij} = (g_i, g_j) \ \ \ b_i = (f, g_i)
	$$
	
	Пример, приводящий к матрице Гильберта (\hyperlink {lects.31}{Лекции}):
	
	$\mathcal{L}$ - пространство вещественных функций с ограниченным интегралом $\int\limits_0^1 f^2(x)dx < \inf $ и скалярным произведением $(f,g) = \int\limits_0^1 f(x)g(x)dx$. В качестве $g_1, ..., g_n$ выберем систему многочленов $1, x, ..., x^{n-1}$. Тогда элементы матрицы $a_{ij} = (x^{i-1},x^{j-1}) = \frac{1}{i+j-1}$. Это матрица Гильберта, на ней решения находятся с большой погрешностью из-за влияния машинной точности. 

\section {Дискретное преобразование Фурье. Идея быстрого дискретного преобразования Фурье.}
	\hyperlink {lects.32}{Лекции}\\

\section {Наилучшее равномерное приближение многочленами.}
	\hyperlink {lects.35}{Лекции}\\

\section {Квадратурные формулы интерполяционного типа.}
	\hyperlink {lects.37}{Лекции}\\

\section {Ортотональные многочлены и квадратуры Гаусса.}
	\hyperlink {lects.40}{Лекции}\\

\section {Составные квадратурные формулы. Правило Рунге для оценки погрешности.}
	\hyperlink {lects.44}{Лекции}\\

\section {Основные приёмы для вычисления нерегулярных интегралов.}
	\hyperlink {lects.45}{Лекции}\\

\section {Метод прогонки для решения трёхдиагональных систем. Корректность и устойчивость метода прогонки.}
	\hyperlink {lects.48}{Лекции}\\

\section {Прямые методы решения систем линейных уравнений. Методы Гаусса и Холецкого.}
	\hyperlink {lects.51}{Лекции}\\

\section {Прямые методы решения систем линейных уравнений. Методы отражений и вращений.}
	\hyperlink {lects.54}{Лекции}\\

\section {Число обусловленности. Неравенства для ошибки и невязки.}
	\hyperlink {lects.56}{Лекции}\\

\section {Метод простой итерации решения систем линейных уравнений.}
	\hyperlink {lects.58}{Лекции}\\
	Дана СЛАУ $Ax = b$. Преобразуем её к виду $x = Gx + c$. Если решение этой системы находится как предел последовательности
	$$
	    x^{k+1} = Gx^{k} + c,
	$$
	то такой процесс называется {\it методом простой итерации} (далее МПИ). $G$ - {\it оператор перехода}.\\
	Для систем со знакоопредленными матрицами МПИ обычно строится в виде
	$$
	    \frac{x^{k+1} - x^{k}}{\tau} + Ax^{k} = b,
	$$
	т.е. $G = I - \tau A, \,\, c = \tau b$. $\tau$ - итерационный параметр.
	\begin{state} (\hyperlink {lects.58}{Достаточное условие сходимости МПИ})\\
	Если $||G|| < 1$, то система имеет единственное решение и итерационный процесс сходится
	к решению со скоростью геометрической прогрессии.
	\end{state}
    \begin{state} (\hyperlink {lects.59}{Критерий сходимости МПИ})\\
    Пусть система имеет единственное решение. Итерационный процесс сходится к решению системы тогда и только тогда, когда все собственные значения матрицы $G$ по модулю меньше 1. ($|\lambda_{G}| < 1$)
    \end{state}
    Пусть $A = A^* > 0$. Б.о.о. можем считать, что $\lambda(A) \in [m, M],\,\, m > 0$.
    Метод $ \frac{x^{k+1} - x^{k}}{\tau} + Ax^{k} = b$ сходится при $0 < \tau < \frac{2}{M}$.

\newpage
\section {Оптимальный одношаговый итерационный метод.}
	\hyperlink {lects.60}{Лекции}\\
	Хотим выяснить, при каком $\tau$ сходимость метода
	$\frac{x^{k+1} - x^{k}}{\tau} + Ax^{k} = b,$ будет наилучшей.
	\begin{state} (\hyperlink {lects.60}{Оптимальный $\tau$})\\
	При условии $A = A^T > 0, \,\, \lambda(A) \in [m, M]$ оптимальное значение
	$$\tau_0 = \frac{2}{m+M}$$
	При этом имеет место скорость сходимости $q_0 = \frac{M-m}{M+m} < 1$ (в норме 2).
	\end{state}

\section {Оптимальный циклический итерационный метод.}
	\hyperlink {lects.61}{Лекции}\\
	Рассмотрим следующий алгоритм с переменным итерационным параметром:
	$$
	    \frac{x^{k+1} - x^{k}}{\tau_{k+1}} + Ax^{k} = b.
	$$
	Будем считать, что допускается изменение параметра $\tau$ в зависимости от номера итерации циклическим образом с периодом $N$.
	\begin{state} (\hyperlink {lects.61}{Оптимальный $\tau_{k}$})\\
	При условии $A = A^T > 0, \,\, \lambda(A) \in [m, M]$ оптимальные значения $\tau_k$
	равны обратным величинам корней многочлена Чебышева степени $N$ на отрезке $[m, M]$:
	$$
	\tau_k^{-1} = \frac{M+m}{2} + \frac{M-m}{2}\cos\frac{\pi(2k-1)}{2N}, \,\, k = 1,...,N.
	$$
	При этом имеет место скорость сходимости
	$q_1 = \frac{\sqrt{M}-\sqrt{m}}{\sqrt{M}+\sqrt{m}}$ (в норме 2).
	\end{state}
	Здесь появляется важный аспект {\bf упорядочивания шагов}. В каком порядке брать $\tau_k$?
	Ответ: \hyperlink {lects.62}{процедура упорядочивания шагов}.

\section {Обобщённый метод простой итерации.}
	\hyperlink {lects.63}{Лекции}\\
	Если $M/m \gg 1$, то для "улучшения"{} исходной задачи можно перейти к некоторой равносильной системе $B^{-1}Ax = B^{-1}b$ при условии невырожденности $B$:
	$\det(B) \ne 0$.\\
	Эта процедура называется {\it предобуславливанием}.(\hyperlink {lects.63}{подробнее})\\
	ОМПИ часто записывают в виде:
	$$
	    B\frac{x^{k+1} - x^{k}}{\tau} + Ax^{k} = b.
	$$
	Требование $B = B^T > 0$ для сходимости в общем случае не обязательно.
	\begin{state} (\hyperlink {lects.63}{Условие сходимости ОМПИ})\\
	Пусть $A = A^T > 0, \,\, \tau > 0$. Тогда ОМПИ сходится для любого начального
	приближения при условии $B - \frac{\tau}{2}A > 0$.
	\end{state}
	Кратко о \hyperlink {lects.64}{методах релаксации} (Якоби, Гаусса-Зейделя и SOR).
	Подробно эти методы будут описаны в следующих билетах.

\section {Методы Якоби и Гаусса -- Зейделя.}
	\hyperlink {lects.65}{Лекции}\\
	Общий вид этих методов - см. билет 22.\\
	Представим матрицу системы $Ax = b$ в виде $A = L + D + R$, $D$ - диагональ, $L$ и
	$R$ - соотв. левая нижняя и правая верхняя треугольные матрицы с нулевыми диагоналями.
	Будем считать, что все диагональные элементы отличны от нуля.\\
	\begin{defn}
	Невырожденная матрица $A$ обладает свойством {\it диагонального преобладания}, если для
	всех $i$ справедливо
	$$
	    \sum_{j=1,j\ne i}^{n}|a_{ij}| \le q|a_{ii}|, \,\, 0 \le q < 1.
	$$
	\end{defn}
	\noindent{\bf Метод Якоби} ($B = D, \, \tau = 1$):
	$$
	    Dx^{k+1} + (L+R)x^{k} = b.
	$$
	\begin{state} (\hyperlink {lects.65}{Условие сходимости метода Якоби})\\
	Если матрица системы $A$ обладает диагональным преобладанием, то метод Якоби сходится
	с произвольного начального приближения.
	\end{state}
	\noindent{\bf Метод Гаусса-Зейделя} ($B = D + L, \, \tau = 1$):
	$$
	    (D+L)x^{k+1} + Rx^{k} = b.
	$$
	\begin{state} (\hyperlink {lects.65}{Условие сходимости метода Гаусса-Зейделя})\\
	Если матрица системы $A$ обладает диагональным преобладанием, то метод Гаусса-Зейделя
	сходится с произвольного начального приближения.
	\end{state}

\section {Метод верхней релаксации.}
	\hyperlink {lects.67}{Лекции}\\
	Общий вид этих методов - см. билет 22.\\
	\noindent{\bf Метод верхней релаксации (SOR)} ($B = D + \omega L, \, \tau = \omega$):
	$$
	    (D+\omega L)x^{k+1} + (\omega R + (\omega - 1)D)x^{k} = \omega b.
	$$
	Здесь итерационный параметр $\omega$ традиционно называется {\it параметром релаксации}.
	Другая запись:
	$$
	    (D+\omega L)\frac{x^{k+1}-x^{k}}{\omega} + Ax^{k} = b.
	$$
	\begin{state} (\hyperlink {lects.67}{Условие сходимости метода SOR})\\
	Пусть $A = A^T > 0$. Тогда для сходимости метода SOR с произвольного начального
	приближения необходимо и достаточно выполнение неравенства $0 < \omega < 2$.
	\end{state}
	В качестве частного случая ($\omega = 1$) имеем сходимость метода Гаусса-Зейделя для
	симметричных положительно определенных матриц.

\section {Метод наискорейшего градиентного спуска.}
	\hyperlink {lects.68}{Лекции}\\
	Недостатком оптимального одношагового МПИ является необходимость знания $M$ и $m$.
	Наша цель - построение алгоритма (наискорейшего градиентного спуска), имеющего
	аналогичную скорость сходимости, но не использующего информацию о границах спектра.\\
	Заменим исходную СЛАУ задачей отыскания минимума функционала
	$$
	   F(x) = (Ax, x) - 2(b,x).
    $$
    Простейшими из известных методов минимизации функционала являются методы градиентного
    спуска, в которых приближения определяются формулой
    $$
        x^{k+1} = x^k - \delta_k\cdot grad(F(x^k)) = x^k - \Delta_k (Ax^k - b), \,
        \Delta_k = 2\delta_k.
    $$
    Здесь $\delta_k$ - параметр метода, и его выбор определяет конкретный алгоритм.
    Например, его можно определить из условия
    $$
        \delta_k: F(x^{k+1}) = F(x^k - \delta_k\cdot grad(F(x^k))) \rightarrow \min.
    $$
    В этом случае метод называется {\bf методом наискорейшего градиентного спуска} (МНГС).\\
    Обозначим $F_0(y) = (A(y-x), y-x) = ||y-x||_A^2$, где $x$ - точное решение.
    \begin{state} (\hyperlink {lects.69}{Скорость сходимости МНГС})\\
	Пусть $A = A^T > 0, \,\, \lambda(A) \in [m, M], \,\, m > 0$. Тогда приближения $x^k$
	в МНГС удовлетворяют оценке
	$$
	    F_0(x^k) \le \left(\frac{M-m}{M+m}\right)^{2k}F_0(x^0).
	$$
	Заметим, что ассимптотическая скорость тут такая же как у оптимального одношагового МПИ,
	но информация о границах спектра не требуется.
	\end{state}
	Рассчетные формулы в МНГС:
	$$
	    r^k = Ax^k - b,\,\,\, \Delta_k = \frac{(r^k, r^k)}{(Ar^k, r^k)}, \,\,\,
	    x^{k+1} = x^k - \Delta_k r^{k}.
	$$
	\hyperlink {lects.70}{Подробнее} об их недостатках и возможных улучшениях
	(внизу страницы).

\section {Линейная задача наименьших квадратов. Метод нормального уравнения.}
	\hyperlink {lects.71}{Лекции}\\

\section {Линейная задача наименьших квадратов. Методы QR-разложения и сингулярного разложения.}
	\hyperlink {lects.72}{Лекции}\\

\section {Общая идея и примеры проекционных методов.}
	\hyperlink {lects.74}{Лекции}\\

\section {Пространства Крылова. Понятие о методе сопряженных градиентов.}
	\hyperlink {lects.74}{Лекции}\\

\section {Частичная проблема собственных значений.}
	\hyperlink {lects.78}{Лекции}\\
	$A$ \--- матрица $n*n$. Ищем собственный вектор $x \ne 0$ и собственное значение $\lambda$ : $Ax = \lambda x$.
	\textbf{Степенной метод вычисления максимального по модулю с.з.}:\\
	\begin{center}
	\label{step_met}
	$x^{k+1} = Ax^k$,  $\lambda^{(k)} = \frac{(x^{k+1}, x^k)}{\|x^k\|^2_2}$, $x^k \ne 0$,  $k=0,1,2...$.
	\end{center}
	\textbf{Утверждение}\\
	Пусть $A$ \--- матрица простой структуры (базисные векторы $\{e_i\}^n_1$ - образуют простой базис в $C^n$). Пусть далее $|\lambda_1| > |\lambda_2| \geq |\lambda_3| \geq ... \geq |\lambda_n|$ и $L=span\{e_2,e_3,...,e_n\}$. Тогда для степенного метода (\ref{step_met}), при условии $x^0 \notin L$ справедлива оценка $\lambda^{(k)} = \lambda_1 + O(|\lambda_2/\lambda_1|^k)$.
	
	$\square$ Кратко.\\
	Разложим начальное приближение по собственным векторам, оттуда получим такие же разложения для $x^k, x^{k+1}$. Оцениваем рост их скалярного произведения: $(x^{k+1}, x^k) = c^2_1 \lambda^{2k+1}_1 (e_1, e_1) + 0(|\lambda^{k+1}_1\lambda^k_2|)$.
	Рассматриваем величину $\lambda^{(k)}$, как отношение этой штуки к квадрату нормы $x^k$,  получаем $\lambda_1 + O(|\lambda2/\lambda_1|^k)$. А если матрица симметричная, то можно, учитывая ортогональность с.в. получить степень $2k$. 
	$\square$
	
	
	Для поиска наименьшего с.з. матрицы можно использовать \textbf{метод обратной итерации}. (степенной метод для $A^{-1}$):\\
	\begin{center}
	$x^k := x^k / \|x^k\|_2, Ax^{k+1} = x^k, \lambda^{(k)} = \frac{(x^k, x^{k+1})}{(x^{k+1}, x^{k+1})}$
	\end{center}
	
	\textbf{Замечание}: На каждом шаге придется решать систему: $Ax^{k+1} = x^k$.
\\
Если у нас вырожденная или близкая к тому матрица, то можно использовать эти методы со сдвигом, т.е. применить их к матрице $A-cE$, с достаточно малым $c$.  Если известно приближение к собственному значению $\lambda$ \--- $\overline{\lambda}$, то метод простой итерации с $c = \overline{\lambda}$ - очень быстро сходится. Также, в качестве сдвига $c$ можно взять \textbf{отношение Рэлея}: $R_A(x) = \frac{(Ax, x)}{(x, x)}$, что приводит к кубической сходимости.
	
	

\section {Полная проблема собственных значений. QR-алгоритм.}
	\hyperlink {lects.79}{Лекции}\\
	
Пусть $A$ - матрица $n*n$.
\\
\\
\textbf{$QR$-алгоритм}:
\begin{itemize}
\item Процесс итерационный
\item Начальное условие: $A_0 = A$
\item На каждом шаге находим разложение $A_k = Q_kR_k$ и  вычисляем $A_{k+1} = R_k Q_k $
\item Цель алгоритма - получение предельной матрицы $R_{\infty}$, из диагонали которой извлекается информация о  модулях с.з. исходной матрицы. Сами же значения определяются из структуры $Q_{\infty}$

\item В общем случае алгоритм сходится \textit{по форме} к блочно-треугольной матрице $R$, на диагонали которой модули с.з.
\end{itemize}
\textbf{Утверждение }:
Если $А$ - нормальная вещественная матрица ($A^TA = AA^T$), то последовательность верхнетреугольных матриц $\{R_k\}$ из $QR$-алгоритма сходится к диагональной матрице.
\\
$\square$Кратко. (Все нормы считаем $\|\|_2$, диагональные элементы матрицы R - больше нуля)\\

Рассматриваем две матрицы с соседних шагов алгоритма. Для краткости - $A$ и $B$. \\
Переход от $A$ к $B$: $A = QR$, $B = RQ$\\

Обозначим $b^i, a^i, r^i$ \--- столбцы, а $b_i, a_i, r_i$ - строки матриц $B, A$ и $R$, соответственно. 
Заметим:\\
1) $\|b^i\| = \|r^i\|, \|a_i\| = \|r_i\|$ \--- евклидова норма инвариантна относительно ортогональных преобразований\\
2) $\|a_i\| = \|a^i\|, \|b_i\| = \|b^i\|$ \--- нормальность матриц\\
\\
Вводим $\triangle_m$, как сумму разностей квадратов норм $b_i$ и $a_i$, и выражаем ее через $r_{ij}$:
\begin{center}
$\triangle_m = \sum\limits^m_{i=1} \sum\limits^n_{j=m+1} |r_{ij}|^2, m =2, ... , n-1$
\end{center}

Для каждой матрицы $A_k$ задаем последовательность $\delta^{(k)}_m = \sum\limits^m_{i=1} \|a^{(k)}_i\|$. Из предыдущего равенства, эти последовательности сходятся, и соответствующие им последовательности $\triangle^{(k)}_m = \delta^{(k+1)}_m - \delta^{(k)}_m$ \--- сходятся к нулю, а вместе с ними \--- все наддиагональные элементы матриц $R_k$.

Далее выводим сходимость последовательностей $\{r^{(k)}_{ii}\}_k$, из чего следует существование предела последовательности $\{R_k\}$. $\square$



\section {Метод простой итерации для нелинейных уравнений.}
	\hyperlink {lects.82}{Лекции}\\
	
$H$	 - полное метрическое пространство с метрикой $\rho(x,y)$.\\
$g:H \rightarrow H$.\\

\textbf{Метод простой итерации} для решения уравнения $x = g(x)$ \--- это алгоритм вида $x^{n+1} = g(x^n)$ с некоторым заданным начальным приближением $x^0$.\\
\\
Отображение $g(x)$ называется \textbf{сжимающим}, если для любых $x, y \in H$ справедливо неравенство $\rho(g(x), g(y)) \leq q\rho(x,y)$ с постоянной $0 \leq q < 1$.\\
\\
\textbf{Утверждение}: Если отображение $g(x)$ \--- сжимающее, то уравнение $x=g(x)$  имеет единственное решение $z$ и справедливо неравенство:
\begin{center}
$\rho (z, x^n) \leq \frac{q^na}{1-q}$, где $a = \rho (x^0, x^1)$
\end{center}

Наибольшее число $k$ называется \textbf{порядком метода}, если существуют положительные конечные постоянные $C_1$ и $C_2$ такие, что справедливо неравенство:
\begin{center}
$\rho(x^{n+1}, z) \leq C_2 [\rho(x^n,z)]^k$ 
при условии
 $ \rho (x^n,z) \leq C_1, \forall n \geq 0$
\end{center}

Метод простой итерации - первый порядок с $C_2 = q < 1$\\

Больше порядок - больше сходимость к решению.
	

\section {Метод Ньютона.}
	\hyperlink {lects.83}{Лекции}\\

\section {Явный метод Эйлера для обыкновенных дифференциальных уравнений (ОДУ). Устойчивость. Локальная и глобальная ошибки.}
	\hyperlink {lects.87}{Лекции}\\

\section {Явные методы Рунге -- Кутты.}
	\hyperlink {lects.89}{Лекции}\\

\section {Неявные одношаговые методы решения ОДУ.}
	\hyperlink {lects.91}{Лекции}\\

\section {Многошаговые методы решения ОДУ.}
	\hyperlink {lects.92}{Лекции}\\

\section {Основы метода конечных элементов: вариационная постановка задачи, метод Ритца, базисные функции.}
	\hyperlink {lects.97}{Лекции}\\

\section {Оценка точности приближения кусочно -- линейными функциями.}
	\hyperlink {lects.102}{Лекции}\\

\section {Проекционная теорема в методе конечных элементов.}
	\hyperlink {lects.103}{Лекции}\\

\section {Система уравнений в методе конечных элементов.}
	\hyperlink {lects.106}{Лекции}\\

\section {Решение модельной задачи методом Фурье.}
	\hyperlink {lects.108}{Лекции}\\

\section {Исследование устойчивости модельной задачи методом Фурье.}
	\hyperlink {lects.111}{Лекции}\\

\section {Метод стрельбы для решения трехдиагональных систем.}
	\hyperlink {lects.112}{Лекции}\\

\section {Пример аппроксимации уравнения и краевых условий.}
	\hyperlink {lects.115}{Лекции}\\

\section {Определения аппроксимации и устойчивости.}
	\hyperlink {lects.118}{Лекции}\\

\section {Определение сходимости. Теорема А.Ф.Филиппова.}
	\hyperlink {lects.120}{Лекции}\\

\section {Интегро -- интерполяционный метод.}
	\hyperlink {lects.121}{Лекции}\\

\section {Исследование устойчивости методом априорных оценок.}
	\hyperlink {lects.125}{Лекции}\\

\section {Метод конечных разностей для уравнения Пуассона.}
	\hyperlink {lects.128}{Лекции}\\

\section {Спектральный признак устойчивости и примеры его применения для аппроксимаций гиперболического уравнения.}
	\hyperlink {lects.130}{Лекции}\\

\section {Принцип замороженных коэффициентов.}
	\hyperlink {lects.132}{Лекции}\\

\section {Исследование устойчивости простейших схем для уравнения теплопроводности в равномерной метрике.}
	\hyperlink {lects.134}{Лекции}\\

\section {Исследование устойчивости схемы с весами для уравнения теплопроводности в интегральной метрике.}
	\hyperlink {lects.136}{Лекции}\\




\includepdf[pages=-, link, linkname = lects]{ch-m_II-20.pdf}
\end{document}
