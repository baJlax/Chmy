\documentclass[specialist, subf, href, colorlinks=true, 12pt, times, mtpro, final]{disser}
\usepackage [russian] {babel}
\usepackage [utf8] {inputenc}
\usepackage {amsmath}
\usepackage {amsthm}
\usepackage {amssymb}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{amsfonts}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage{algorithm}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{pdfpages}
\usepackage{dsfont}

\theoremstyle{definition}
\newtheorem{defn}{Определение}[section]
\newtheorem{example}{Пример}[section]
\newtheorem{state}{Утверждение}[section]
\newtheorem{theorem}{Теорема}[section]

\definecolor{linkcolor}{HTML}{0000FF}
\definecolor{urlcolor}{HTML}{0000FF}
\hypersetup{pdfstartview = FitH, linkcolor = linkcolor, urlcolor = urlcolor, colorlinks = true}

\def\rk{\text{rank}}
\def\mcL{\mathcal{L}}
\def\mcK{\mathcal{K}}

\begin{document}

\tableofcontents

\section {Погрешность метода и вычислительная погрешность. Пример неустойчивого алгоритма.}
    \hyperlink {lects.1}{Лекции} \\
    \textbf{Описание численного метода:}\\
    Постановка задачи $\rightarrow$ Приближенный метод решения $\rightarrow$ Оценка погрешности (\textit{погрешность метода}) $\rightarrow$ Оценка погрешности с учетом округлений (\textit{влияние вычислительной погрешности})  

	\begin{example}
    	Пример неустойчивого алгоритма (\hyperlink {lects.14}{Лекции})\\
    	Пусть требуется вычислить последовательность интегралов:
    	$$
    	    \int_0^{2\pi}{x^n e^{x-1}dx}, n = 1,2,3,...,N.
    	$$
    	Для построения численного алгоритма проведем интегрирование по частям:
    	$$
    	\begin{aligned}
    	    & I_n = \int_0^{2\pi}{x^n d(e^{x-1})} = x^n e^{x-1} \big|_0^1 - \int_0^{2\pi}{n x^{n-1} e^{x-1}dx} = 1 - n 	I_{n-1} \\
        	& I_1 = \frac{1}{e}
    	\end{aligned}
    	$$
    	Легко заметить, что при отсутствии ошибок округления погрешность метода равна нулю (точный метод!). Что будет при реальных вычислениях? Рассмотрим ситуацию, когда погрешность возникает только вследствие определения величины $I_n$. Введем обозначение для ошибки $z_n = I_n - I_n^*$. Тогда
    	$$
    	\begin{aligned}
    	    & I_n^* = 1 - n I_{n-1}^* \\
        	& z_n = - n z_{n-1} = n! (-1)^{n+1} z_1
    	\end{aligned}
    	$$
    	Погрешность очень быстро (факториально!) растёт. Очень скоро это приведет к сильному искажению искомого результата.
	\end{example}
	\noindent Можно ли исправить эту ситуацию? Ответ: да, \hyperlink {lects.15}{вот так}.\\
	{\it(Если кратко, нужно просто вычислять их в обратном
	порядке, тогда ошибка на каждом шаге будет убывать. А начальное значение можно взять
	любым (например нулем), т.к. пока дойдем до нужного номера, ошибка исчезнет.)}

\section {Алгебраическая интерполяция. Многочлен Лагранжа.}
	\hyperlink {lects.15}{Лекции}\\
	Отрезок $[a,b]$ разбит на $n$ узлов $a = x_1 < x_2 < ... < x_n = b$ и в них заданы $f_i$ - значения функции $f$ в $x_i$. Требуется найти многочлен $L_{n-1}(x)$ степени $n - 1$ чтоб $L_{n - 1}(x_i) = f_i$.

	\noindent\textbf{Многочлен Лагранжа $L_{n - 1}(x)$}
	$$
		\begin {array}{lr}
		L_{n - 1}(x) = \sum\limits_{i = 1}^{n} f_i \Phi_i (x), & \Phi_i(x) = \prod\limits_{j = 1, j\ne i}^{n} \frac{x-x_j}{x_i - x_j} \\
		\end {array}
	$$

	\begin{state} (\hyperlink {lects.16}{Лекции})\\
	Пусть n-я производная функции $f(x)$ непрерывна на $[a,b]$, тогда $\forall x \in [a,b] \ \  \exists \xi \in [a,b]$, что справедливо
		$$
			\begin{array}{lr}
			f(x) - L_{n-1}(x) = \frac {f^{(n)}(\xi)}{n!} \omega_n(x), & \omega_n(x) = \prod\limits_{i - 1}^{n}(x-x_i)
			\end{array}
		$$
	\end{state}
	\noindent{\bfСледствие}:
	$$
		||f(x) - L_{n-1}(x) || \le \frac{||f^{(n)}(x)||}{n!} ||\omega_n||
	$$
	$$
		||g(x)|| = \underset{x\in [a,b]}{sup} |g(x)|
	$$


\section {Константа Лебега интерполяционного процесса для равноотстоящих узлов.}
	\hyperlink {lects.17}{Лекции}\\
	{\bfКонстанта Лебега интерполяционного процесса} (\hyperlink {lects.17}{Подробно})
	$$
		\begin{array}{lr}
		\lambda_n = \underset{x\in [a,b]}{max} \sum\limits_{i = 1}^n |\Phi_i(x)|, & \Phi_i(x) = \prod\limits_{j = 1, j\ne i}^{n} \frac{x-x_j}{x_i - x_j} \\
		\end{array}
	$$
	
	\begin{state} (\hyperlink {lects.17}{Утверждение 1})\\
	$\lambda_n$ не зависит от длины отрезка интерполяции.
	\end{state}
	
	\noindent \hyperlink{lects.17}{Утверждение 2.} (Оценка снизу для $\lambda_n\,(n\ge 2)$)
	$$
		\lambda \ge K \frac {2^n}{n^{3/2}}, 
	$$ 
	где $K$ не зависит от $n$. \\
	

\section {Многочлены Чебышёва и их свойства.}
	\hyperlink {lects.18}{Лекции}\\
	\textbf{Способы определения:}
	\begin{enumerate}
		\item Рекуррентно 
			  $$
			  	\begin{array}{lcr}
			  		T_0(x) = 1, & T_1(x) = x, & T_{n+1}(x) = 2xT_n(x) - T_{n-1}(x)
			  	\end{array}
			  $$
		\item Тригонометрически $x\in[-1,1]$
			  $$
			  	T_n(x) = cos(n \ arccos(x))
			  $$
		\item Разностное уравнение
			  $$
			  	T_n(x) = \frac{1}{2}\left( \left( x + \sqrt{x^2 - 1} \right)^n + \left( x - \sqrt{x^2 - 1} \right)^n \right)
			  $$
	\end{enumerate}
	\textbf{Свойства}
	\begin{enumerate}
		\item $|T_n(x)| \le 1$ при $x \in [-1,1]$
		\item $T_n(-x) = (-1)^nT_n(x)$
		\item Коэффициент при старшем члене равен $2^{n - 1}$
		\item Все нули находятся на $[-1,1]$ в точках
			$$
				x_k = cos \frac{\pi (2k -1)}{2n} \ \ \ k = 1, ... , n
			$$
		\item Экстремумов на $[-1,1]$ $n+1$ штук в точках
			$$
				x_m = cos \frac{\pi m}{n} \ \ \ m = 0, ... , n
			$$ 
		\item Отображение на отрезок 
			$$
				T_n^{[a,b]} (x) = (b - a)^n 2^{1-2n} T_n \left( \frac{2x - (b+a)}{b-a} \right)
			$$
	\end{enumerate}

	Приведенный многочлен Чебышёва (со старшим коэффициентом 1)
	$$
		\bar {T_n} = 2^{1 - n}T_n
	$$
	
	\begin{state} (\hyperlink {lects.19}{Утверждение 3})\\
		Приведенный многочлен Чебышёва на $[-1,1]$ наименее отклоняется от нуля. То есть 
		$$
			\underset{[-1,1]}{max}|P_n(x)| \ge \underset{[-1,1]}{max} |\bar{T_n}(x)| = 2^{1-n},
		$$
		где $P_n(x)$ - любой многочлен степени $n$ с коэффициентом 1 при $n$-ой степени.
	\end{state}

	При фиксированном $n$ выбор корней $T_n(x)$ в качестве узлов для многочлена Лагранжа является оптимальным.
	(\hyperlink {lects.20}{Минимизация оценки погрешности})

	

\section {Интерполяционные сплайны. Конструкция и обоснование кубического сплайна.}
	\hyperlink {lects.22}{Лекции}\\
	$a = x_0 < x_1 < ... < x_n = b$ -- сетка (узлы интерполяции).\\
	$P_m(x)$ - множество всех многочленов, степени не выше $m$.
	\begin{defn}
		Полиномиальный и интерполяционный сплайн \hyperlink {lects.22}{Лекции}
	\end{defn}
	Построим {\bf кубический} интерполяционный сплайн. Обозначим через $M_i$ значения
	второй производной $S_3^{''}(x)$ кубического сплайна в узлах $\{x_i\}$, и будем для
	простоты считать, что расстояния между узлами одинаковы, т.е. $x_i - x_{i-1} = h$.
	\begin{state} (\hyperlink {lects.22}{О вторых производных кубического сплайна})\\
	Величины $M_0, M_1, ..., M_n$ удовлетворяют СЛУ $CM=d$, где
	$$
	c_{ij} =
	\begin{cases}
	1/6 &\text{при } j=i\pm 1,\\
	2/3 &\text{при } j=i,\\
	0   &\text{при } |j-i|>1;\\
	\end{cases}
	\,\,\,\, d_i = \frac{f_{i+1}-2f_i+f_{i-1}}{h^2},\,\,\, i,j = 1,...,n-1.
	$$
	\end{state}
	В рассмотренной системе число уравнений на 2 меньше числа неизвестных. Если наложены
	ограничения $M_0 = M_n = 0$, то сплайн называется {\it естественным}.
	(\hyperlink {lects.23}{Лекции, 2ой абзац})
	\begin{state} (\hyperlink {lects.23}{О естественном сплайне})\\
	Естественный сплайн $S_3(x)$ доставляет минимум
	\hyperlink {lects.23}{функционалу энергии} на множестве
	$\Phi = \{\phi\in C^{(2)}[a,b]:\,\,\phi(x_i)=f(x_i)\}$.
	\end{state}
	\noindent \hyperlink{lects.24}{Еще 2 утверждения о естественных сплайнах.}

\section {Понятие об аппроксимационных сплайнах.}
	\hyperlink {lects.25}{Лекции}\\
	В отличии от интерполяционных не требуют пересчета всех коэффициентов при изменении одного (нескольких) значений $f_i$. Потому что строятся локально на каждом $[x_{i-1}, x_i]$ и зависят лишь от некотрого постоянного числа соседних $f_i$ (1, 2, 3 - обычно немного).
	
	Построение аппроксимационного сплайна 3ей степени (\hyperlink {lects.25}{Лекции}).
	
	Оценки для второй и первой производной и для разности (для сплайна 3ей степени) (\hyperlink {lects.27}{Лекции}):
	$$
		f(x) \in C^2[0,1], \ \ \ \ \ |f''(x)|_{[0,1]} < A_2
	$$
	\begin{itemize}
		\item Для второй производной $|B''_2(x_k)| \le A_2$
		\item Для первой производной $\underset{x}{max}|f'(x)-B'_2(x)| \le C_1 h A_2 \ \ \ \ \ \  \ C_1 = \frac{5}{2}$
		\item Просто для разности $\underset{x}{max}|f(x)-B_2(x)| \le C_0 A_2 h^2$
			
	\end{itemize}
	
	
	

\section {Наилучшее приближение в линейном нормированном пространстве.}
	\hyperlink {lects.28}{Лекции}\\
	\begin{defn}
	Задан элемент $f$ линейного нормированного пространства $\mathcal{L}$. Хотим найти его наилучшее приближение комбинацией заданных независимых элементов $g_1, ..., g_n \in \mathcal{L}$. Т.е. найти $x = \sum\limits_{j = 1}^{n}c_j^0g_j$ такой, что
	$$
		||f - x|| = ||f - \sum\limits_{j = 1}^{n}c_j^0g_j|| = \underset{c_1,...,c_n}{inf} ||f - \sum\limits_{j = 1}^{n}c_jg_j||
	$$
	\end{defn}

	Всегда существует, но не обязательно единственный. (\hyperlink {lects.28}{утв 1})
	
	\begin{defn}  
		Пространство $\mathcal{L}$ строго нормированно, если из условия
		$$
			||f+g|| = ||f||+||g|| \ \ \ \ \ \ \ \ \ \ ||f||,||g|| \ne 0
		$$
		следует $f = \alpha g, \alpha \ne 0$.
	\end{defn}

	В случае строго нормированного элемент наилучшего приближения единственный (\hyperlink {lects.29}{утв 2})

\section {Наилучшее приближение в гильбертовом пространстве.}
	\hyperlink {lects.29}{Лекции}\\
	Т.е. есть скалярное произведение $(x,y)$ и норма $||x||=\sqrt{(x,x)}$. \\
	Гильбертово пространство - строго нормированное (\hyperlink {lects.29}{утв 3}) \\
	
	Поиск наилучшего приближения - решение СЛАУ. (всегда имеет единственное решение \hyperlink {lects.30}{утв 4})
	$$
		Ac = b, \ \ \ \ \ \ a_{ij} = (g_i, g_j) \ \ \ b_i = (f, g_i)
	$$
	
	Пример, приводящий к матрице Гильберта (\hyperlink {lects.31}{Лекции}):
	
	$\mathcal{L}$ - пространство вещественных функций с ограниченным интегралом $\int\limits_0^1 f^2(x)dx < \infty $ и скалярным произведением $(f,g) = \int\limits_0^1 f(x)g(x)dx$. В качестве $g_1, ..., g_n$ выберем систему многочленов $1, x, ..., x^{n-1}$. Тогда элементы матрицы $a_{ij} = (x^{i-1},x^{j-1}) = \frac{1}{i+j-1}$. Это матрица Гильберта, на ней решения находятся с большой погрешностью из-за влияния машинной точности. 

\section {Дискретное преобразование Фурье. Идея быстрого дискретного преобразования Фурье.}
	\hyperlink {lects.32}{Лекции}\\
	\textbf{Традиционное преобразование Фурье}. Пусть $f(x)$ - периодична, период = 1 ($f(x+1) = f(x)$):
	$$
		f(x) = \sum\limits_{q = -\infty}^{\infty} a_q e^{2\pi i qx},  \ \ \  \ \ \ \sum\limits_{q = \infty}^{\infty}|a_q| < \infty
	$$
	\textbf{Дискретное преобразование Фурье}. Берем сетку с шагом $\frac{1}{N}$ с узлами $x_l = \frac{l}{N}$. Рассматриваем $f(x)$ только в узлах этой решетки, приводим подобные в сумме, получаем:
	$$
		f(x_l) = \sum\limits_{q = 0}^{N-1} A_q e^{2\pi i qx_l} \ \ \ \ \ A_q = \sum\limits_{s = \infty}^{\infty} a_{q+sN}
	$$
	Словами: \textit{Дискретное преобразование Фурье} - отображение вектора значений функций $f_l, l = 0, ..., N-1$ в вектор коэффициентов $A_q, q = 0, ..., N-1$ разложения по базисным функциям $g_q(x_l) = e^{2\pi i qx_l}$. Эти функции образуют ортонормировнную (т.е. $(g_l,g_k) = 0, l \ne k$) систему относительно скалярного произведения вида $(f,g) = \frac{1}{N}\sum\limits_{l = 0}^{N-1}f_l \bar{g_l}$ (\hyperlink {lects.32}{Док-во}). 
	
	Хотим уметь считать $A_q$. Применяем наилучшее равномерное приближение в Гильбертовом пространстве, получаем конечные формулы:
	$$
		A_q = (f, g_q) = \frac{1}{N}\sum\limits_{l = 0}^{N-1}f_le^{-2\pi i qx_l}
	$$
	
	На практике дискретное преобразование Фурье записывают так (\hyperlink {lects.33}{Зачем так?}):
	$$
		f_l = \sum\limits_{-\frac{N}{2} < q \le \frac{N}{2}} A_q e ^{2\pi i qx}
	$$
	потому что пределы $[0,N-1]$, вообще говоря, можно двигать как угодно.
	
	Но функция $f(x)$ только примерно совпадает со своим дискретным разложением (этот способ аппроксимации носит название тригонометрической интерполяции еще), между узлами может быть разброс (\hyperlink {lects.33}{Пример (4):} $f(x) = a_0 + a_{N-1}e^{2\pi i (N-1)x}$).
	
	\textbf{Идея быстрого преобразования Фурье} \\
	Подсчет всех $A_q$ - это $O(N^2)$ операций. Ускорим.\\
	Перепишем хитро $A_q$ (\hyperlink {lects.34}{Подробно}):
	$$
		A_q = A(q_1, q_2) = \frac{1}{p_2}\sum\limits_{l_2 = 0}^{p_2 - 1} B(q_1, l_2)e^{-2\pi i \frac{ql_2}{N}}
	$$
	$$
		B(q_1, l_2) = \frac{1}{p_1}\sum\limits_{l_1 = 0}^{p_1 - 1}f_{l_2 + l_1p_2}e^{-2\pi i \frac{q_1l_1}{p_1}}
	$$
	тут $p_1p_2 = N$, индексы узла $l$ и коэффициента $A$ $q$ представлены в виде деления на $p_1, p_2$ с остатком: $q = q_1 + p_1q_2, l = l_2 + p_2l_1$.
	
	Теперь чтоб посчитать все $B$ надо $O(p_2p_1^2)$, а чтоб все $A_q$, зная все $B$, надо $O(p_1^2p_2)$. 
	Как получше выбирать $p_1$ и $p_2$: \hyperlink {lects.34}{Конец страницы.}

\section {Наилучшее равномерное приближение многочленами.}
	\hyperlink {lects.35}{Лекции}\\
	$\mathcal{L}$ - пространство ограниченных вещественных функций на $[a,b]$. $||f(x)|| = \underset{[a,b]}{sup}|f(x)|$.
	
	Ищем \textbf{наилучшее приближение} $f\in \mathcal{L}$ в виде
	$$
		Q_n^0 (x) = \sum\limits_{j=0}^{n}a_j^0x^j
	$$
	являющееся решением задачи
	$$
		||f - Q_n^0|| = \underset{\{a_j\}}{inf}||f-Q_n|| = \underset{\{a_j\}}{inf} \underset{x\in [a,b]}{sup} |f(x) - \sum\limits_{j=0}^{n} a_jx^j|
	$$
	
	Мн-н $Q_n^0$ называется \textbf{многочленом наилучшего равномерного приближения (МНРП) степени n для $f(x)$)}, если для любого многочлена $Q_n$ степени n справедливо $$||f-Q_n^0|| \le ||f-Q_n||$$. Такой элемент всегда есть (как элемент наилучшего приближения в нормированном пр-ве) и единственный для непрерывных функций (утверждение ниже).
	
	\begin{theorem}
		\hyperlink {lects.35}{Валле-Пуссена} Пусть существуют n+2 точки $a \le x_0 < x_1 < ... < x_{n+1} \le b$ и мн-н $Q_n$ такие, что 
		$$
			sgn(f(x_i) - Q_n(x)) \times (-1)^i = const
		$$
		т.е. разность $f(x) - Q_n(x)$ меняет знак между узлами, то
		$$
			||f-Q_n^0|| \ge \mu = \underset{i = 0, ..., n+1}{min} |f(x_i) - Q_n(x_i)|
		$$
	\end{theorem}
	\begin{theorem}
		\hyperlink {lects.35}{Чебышёва}. Чтобы $Q_n$ был МНРП необходимо и достаточно существования на $[a,b]$ по крайней мере n+2 точек $x_0 < ... < x_{n+1}$, что
		$$
			f(x_i) - Q_n(x_i) = \alpha (-1)^i ||f-Q_n||
		$$
		где $i = 0, ..., n+1$ и $\alpha = 1$ или $\alpha = -1$ для всех i сразу.\\
		Тут $x_i$ называют точками Чебышевского альтернанса.
	\end{theorem}

	\textbf{\hyperlink {lects.36}{Пример} к теореме Чебышёва}\\
	$f(x) = sin100x$ МНРП степени 90 на $[0,\pi]$ будет $Q_{90} \equiv 0$. Тут $f(x_i) - Q_{90}(x_i) = (-1)^i$, а узлы $x_i = \frac{\pi/2 +\pi i}{100}, i = 0, ... , 99$.
	
	\begin{state}
		МНРП непрерывной функции единственный (\hyperlink {lects.36}{Утв})
	\end{state}
	\textbf{Следствие} Если $f(x)$ - непрерывная и четная (нечетная) ф-ия относительно $\frac{a+b}{2}$, то МНРП - четный (нечетный) относительно $\frac{a+b}{2}$ мн-н.
	
	\textbf{\hyperlink {lects.36}{Пример} нарушения единственности МНРП и теоремы Чебышёва для разрывной функции}
	$$
		f(x) = sgn(x), x\in[-1,1] \ \ \ \ \ Q_1(x) = \alpha x \ \ \ \forall \alpha \in [0,2] 
	$$
	

\section {Квадратурные формулы интерполяционного типа.}
	\hyperlink {lects.37}{Лекции}\\
	Рассматриваем интеграл
	$$
		I(f) = \int_{\Omega}p(x)f(x)dx
	$$
	где $\Omega$ - Конечный или бесконечный промежуток (будем считать $[a,b]$), $f \in F$ - функция из некоторого класса, $p(x)$ - весовая функция (измерима на $\Omega$, $\not\equiv 0$ на $\Omega$ и произведение $p(x)g(x) \ \ \forall g \in F$ суммируемо). \\
	Хотим посчитать интеграл, строим \textbf{линейную квадратурную формулу}
	$$
		S_n(f) = \sum\limits_{i - 1}^{n} c_if(x_i)
	$$
	тут $c_i$ - коэффициенты квадратуры, $x_i$ - узлы квадратуры.\\
	Считаем, что выполнено 
	$$
		I(1) = S_n(1) \ \ \ \ \ \ \ \ \ \text{т.е.} \sum\limits_{i = 1}^n c_i = \int_a^b p(x) dx
	$$
	\textbf{Погрешностью} квадратурной формулы назовем $R_n(f) = I(f) - S_n(f)$, а погрешностью на классе - $R_n(F) = \underset{f\in F}{sup} |R_n(f)|$
	
	\textbf{Квадратурные формулы интерполяционного типа}, т.е. такие, где вместо $f(x)$ используется интерполяционный многочлен Лагранжа (степени n-1).
	$$
		S_n(f) = \int_a^b p(x) L_{n-1}(x) dx
	$$
	явные формулы для коэффициентов и погрешности:
	$$
		c_i = \int_a^b p(x)\Phi_i (x) dx, \ \ \ \ \ \ \ R_n = \frac{||f^{(n)}(x)||}{n!} \int_a^b p(x)|\omega_n(x)| dx
	$$
	где $||f^{(n)}(x)|| = \underset{[a,b]}{max}|f^{(n)}(x)|, \omega_n(x) = \prod\limits_{i = 1}^n (x - x_i), \Phi_i(x) = \prod\limits_{j = 1, j\ne i}^n \frac{x - x_j}{x_i - x_j}, L_{n-1} = \sum\limits_{i = 1}^n f(x_i) \Phi_i(x)$ \\
	
	\begin{defn}
		Квадратурная формула имеет \textbf{алгебраический порядок точности  m}, если $S_n(P_m) = I(P_m)$ для любого многочлна степени не выше m, и $\exists \ \ Q_{m+1}$ (многочлен степени m+1), для которого $S_n(Q_{m+1}) \ne I(Q_{m+1})$. 
	\end{defn}
	\begin{state}
		Квадратурная формула $S_n$ имеет порядок точности $m \ge n-1$ тогда и только тогда, когда она интерполяционная квадратурная формула. (\hyperlink {lects.38}{Утв 1})
	\end{state}

	\textbf{Формулы Ньютона-Котеса} (квадратурные формулы интерполяционного типа при $p(x) = 1$ и на системе равноотстоящих узлов)
	\begin{itemize}
		\item n = 1 (Формулы прямоугольников)
		$$
			S_1(f) = (b-a)f\left(\frac{a+b}{2}\right) \ \ \ \ \ R_1 = ||f'(x)||\frac{(b - a)^2}{4}
		$$
		Если воспользоваться симметричностью коэффициентов (утв. ниже, \hyperlink {lects.39}{Леции}), то можно получить более точную  
		$$
			\tilde{R_1} = ||f''(x)||\frac{(b-a)^3}{24}
		$$
		\item n = 2 (Формулы трапеций)
		$$
			S_2(f) = \frac{b - a}{2}(f(a) + f(b)) \ \ \ \ \ R_2 = ||f''(x)||\frac{(b-a)^3}{12}
		$$
		\item n = 3 (Формулы Симпсона)
		$$
			S_3(f) = \frac{b - a}{6}\left(f(a) + 4f\left(\frac{b+a}{2}\right) + f(b)\right) \ \ \ \ \ R_3 = ||f'''(x)||\frac{(b-a)^4}{192}
		$$
		И более точная через четвертую производную
		$$
			\tilde{R_3} = ||f''''(x)||\frac{(b-a)^5}{2880}
		$$
	\end{itemize}

	\begin{state}
		Для формул Ньютона-Котеса справедлива симметричность коэффициентов $c_k = c_{n+1-k}$ (\hyperlink {lects.40}{Утв 2})
	\end{state}

	\textbf{\hyperlink {lects.39}{Недостаток}} формул Ньютона-Котеса - неустойчивость. Появляются отрицательные $c_i$ и $\underset{n\rightarrow \infty}{lim} \left(\sum\limits_{i=1}^n |c_i^{(n)}| \right) = \infty$. А Сравнивая величины $S_n(f)$ и приближенную $S_n(f^*)$ имеем 
	$$
		|S_n(f) - S_n(f^*)| \le \epsilon \sum\limits_{i = 1}^n |c_i|
	$$
	где $\epsilon$ - машинная точность. Поэтому формулами пользуются только при небольших n ($\le7$)

\section {Ортогональные многочлены и квадратуры Гаусса.}
	\hyperlink {lects.40}{Лекции}\\
	

\section {Составные квадратурные формулы. Правило Рунге для оценки погрешности.}
	\hyperlink {lects.44}{Лекции}\\
	\textbf{Составные квадратурные формулы:}
	Пусть $h = (b-a)/N$ и $x_k = a + kh, \quad k = 0, 1 \ldots N-1$. Введем обозначения: $I^{(k)}(f) = \int_{x_k}^{x_{k+1}} p(x)f(x)dx,$ $\quad S^{(k)}_n(f) := S_n(f)$ для отрезка $[x_k, x_{k+1}], \quad k = 0, \ldots, N-1$. Исходный интеграл равен $I(f) = \sum_{k=0}^{N-1}I^{(k)}(f)$, соответственно составная квадартурная формула принимает вид $S^N_n = \sum_{k=0}^{N-1} S_n^{(k)}(f)$, а для ее погрешности справедливо неравенство $|R_n^N(f)| \leq \sum_{k=0}^{N-1}|R_n^{(k)}(f)|$.
	
	 Далее в лекциях пример составной формулы трапеций: \hyperlink {lects.44}{Составная формула трапеций}
	 
	 Объем вычислительной работы при использовании составных формул растет линейно по N, а оценка погрешности убывает существенно быстрее. Скорость убывания погрешности напрямую зависит от порядка точности формулы.
	 \\ \\ 
	 \textbf{Правило Рунге для оценки погрешности:}\\
	 (Зачем нужно:) При разбиении отрезка на элементарные части важно учитывать поведение интегрируемой функции. Если о ней заранее ничего не известно, то можно проводить разбиение постепенно шаг за шагом, например слева направо. Для очередного элементарного отрезка длины h необходимо уметь оценивать погрешность и принимать решение об уменьшении или увеличении шага интегрирования. 
	 \\ \\
	 Пусть на отрезке длины h используется квадратурная формула $S_h(f)$, точная для многочленов степени не выше m-1. Разложим f(x) в ряд Тейлора в середине отрезка (точка c):
	 $$I(f) - S_h(f) = Df^{(m)}(c)h^{m+1} + O(h^{m+2}), \quad D \not= 0$$ 
	 Обозначим через $S_{h/2}(f)$ составную формулу, полученную с помощью формулы $S_h(f)$ для двух половинок отрезка длины h. Тогда при том же D находим:
	 $$I(f) - S_{h/2}(f) = Df^{(m)}(c) \frac{h^{m+1}}{2^m} + O(h^{m+2})$$
	 Следовательно, с точностью $O(h^{m+2})$ справедливо \textbf{правило Рунге:}
	 $$\boxed {I(f) - S_{h/2}(f) \approx \frac{S_{h/2}(f) - S_h(f)}{2^m - 1}}$$
	 Поэтому, если мы хотим найти $I(f)$ с абсолютной погрешностью $\epsilon$ на всем отрезке $[a, b]$, то каждый шаг h следует выбирать из условия:
	 $$|\frac{S_{h/2}(f) - S_h(f)}{2^m - 1}| \leq \frac{h}{b-a}\epsilon$$

\section {Основные приёмы для вычисления нерегулярных интегралов.}
	\hyperlink {lects.45}{Лекции}\\
	Пусть для вычисления интеграла $I(f) = \int_a^b p(x)f(x)dx$ имеется некоторая квадратурная формула $S_n(f)$. рассмотрим оценку погрешности:
	$$|I(f) - S_n(f)| \leq D max_{[a, b]}|f^{(m)}(x)|(b-a)^{m+1}, \quad D \not = 0$$

	Она теряет всякий смыслв двух случаях: если по крайней мере один из пределов интегрирования равен бесконечности, или m-я производная функции f(x) не ограничена (не существует) на $[a, b]$. Это (формальная неприменимость оценки погрешности квадратурной формулы) и заложено в понятие \textbf{нерегулярных интегралов}. 
	
	Далее в лекциях рассмотрен пример избавления от нерегулярности для $I = \int_0^{\infty} \frac{dx}{(x+1)\sqrt{x}}$. \hyperlink {lects.45}{Это здесь}
	
	Были использованы следующие приемы:
	\begin{itemize}
		\item выделение бесконечности, то есть $\int_0^{\infty} = \int_0^1 + \int_1^{\infty}$
		\item замена переменных $x = \frac{1}{z}, \quad dx = -\frac{dz}{z^2}$
		\item интегрирование по частям (устранение особености)
		\item \hyperlink {lects.46}{выделение весовой функции}
		
	\end{itemize}
	
\section {Метод прогонки для решения трёхдиагональных систем. Корректность и устойчивость метода прогонки.}
	\hyperlink {lects.48}{Лекции}\\
	Решаем систему уравнений с трехдиагональной матрицей:
	
	\begin{equation*}
	 \begin{cases}
	   c_0 y_0 - b_0 y_1 = f_0, &i = 0,\\
	   -a_i y_{i-1} + c_i y_i - b_i y_{i+1} = f_i, &1 \le i \le N-1, \\
	   -a_N y_{N-1} + c_N y_N = f_N, & i = N
	 \end{cases}
	\end{equation*}
	Основная идея метода состоит в представлении решения в виде:
	$$y_i = \alpha_{i+1} y_{i+1} + \beta_{i+1}, \ \ i = N-1, N-2, \dots , 0$$
	Где $\alpha_i$, $\beta_i$, $y_N$ определяются по элементам матрицы и правой части.
	Вывод алгоритма \hyperlink {lects.48}{Вывод}
	
	{\bf Сам алгоритм}:
	
	Сначала рекуррентно вычисляются прогоночные коэффициенты $\alpha_i, \beta_i$:
	$$
	   \alpha_1 = \frac{b_0}{c_0}, \ \ \alpha_{i+1} = \frac{b_i}{c_i - a_i \alpha_i}
	$$
	$$
	   \beta_1 = \frac{f_0}{c_0}, \ \ \beta_{i+1} = \frac{f_i + a_i \beta_i}{c_i - a_i \alpha_i}
	$$
	Затем вычиляется $y_N$:
	$$
	   y_N = \frac{f_N + a_N \beta_N}{c_N - a_N \alpha_N},
	$$
	а по нему "снизу вверх" вычисляются остальные $y_i$:
	$$
	   y_i = \alpha_{i+1} y_{i+1} + \beta_{i+1}
	$$
	Это формулы \emph{правой} прогонки. Формулы \emph{левой} прогонки - исключение неизвестных идет в другом порядке. (Коэффициенты - снизу вверх, $y_i$ - сверху вниз).

	\begin{center}
	Корректность и устойчивость метода прогонки 
	\end{center} 

	Проблемы: обращение в нуль знаменателя во время вычислений ("корректность"), неустойчивость (ошибка $\varepsilon$ при вычислении $y_N$ приводит к ошибке $(\alpha_1 \dots \alpha_N) \cdot \varepsilon$ для $y_0$) 

	\begin{state}
		(Достаточное условие корректности и устойчивости) Пусть коэффициенты системы $\in \mathbb{R}$ и такие, что: все $c_0, c_N$, $a_i, b_i, \ \ i = 1, \dots, N-1$ отличны от 0 и 
	
		$$|c_i| \ge |a_i|+|b_i|, i = 1, \dots, N-1, \ \ \ |c_0| \ge |b_0|, |c_N| \ge |a_N|$$
		И хотя бы одно неравенство строгое. Тогда выполнены неравенства:
		$$c_i - a_i\alpha_i \ne 0, \ \ \ |\alpha_i| \le 1, i = 1, \dots, N$$
		дающие устойчивость и корректность.
(\hyperlink {lects.51}{Утв})
	\end{state}
	
	Стоимость вычислений $O(N)$, постоянная в главном члене ассимптотики $\le 8$
\section {Прямые методы решения систем линейных уравнений. Методы Гаусса и Холецкого.}
	\hyperlink {lects.51}{Лекции}\\

\section {Прямые методы решения систем линейных уравнений. Методы отражений и вращений.}
	\hyperlink {lects.54}{Лекции}\\

\section {Число обусловленности. Неравенства для ошибки и невязки.}
	\hyperlink {lects.56}{Лекции}\\

\section {Метод простой итерации решения систем линейных уравнений.}
	\hyperlink {lects.58}{Лекции}\\
	Дана СЛАУ $Ax = b$. Преобразуем её к виду $x = Gx + c$. Если решение этой системы находится как предел последовательности
	$$
	    x^{k+1} = Gx^{k} + c,
	$$
	то такой процесс называется {\it методом простой итерации} (далее МПИ). $G$ - {\it оператор перехода}.\\
	Для систем со знакоопредленными матрицами МПИ обычно строится в виде
	$$
	    \frac{x^{k+1} - x^{k}}{\tau} + Ax^{k} = b,
	$$
	т.е. $G = I - \tau A, \,\, c = \tau b$. $\tau$ - итерационный параметр.
	\begin{state} (\hyperlink {lects.58}{Достаточное условие сходимости МПИ})\\
	Если $||G|| < 1$, то система имеет единственное решение и итерационный процесс сходится
	к решению со скоростью геометрической прогрессии.
	\end{state}
    \begin{state} (\hyperlink {lects.59}{Критерий сходимости МПИ})\\
    Пусть система имеет единственное решение. Итерационный процесс сходится к решению системы тогда и только тогда, когда все собственные значения матрицы $G$ по модулю меньше 1. ($|\lambda_{G}| < 1$)
    \end{state}
    Пусть $A = A^* > 0$. Б.о.о. можем считать, что $\lambda(A) \in [m, M],\,\, m > 0$.
    Метод $ \frac{x^{k+1} - x^{k}}{\tau} + Ax^{k} = b$ сходится при $0 < \tau < \frac{2}{M}$.

\newpage
\section {Оптимальный одношаговый итерационный метод.}
	\hyperlink {lects.60}{Лекции}\\
	Хотим выяснить, при каком $\tau$ сходимость метода
	$\frac{x^{k+1} - x^{k}}{\tau} + Ax^{k} = b,$ будет наилучшей.
	\begin{state} (\hyperlink {lects.60}{Оптимальный $\tau$})\\
	При условии $A = A^T > 0, \,\, \lambda(A) \in [m, M]$ оптимальное значение
	$$\tau_0 = \frac{2}{m+M}$$
	При этом имеет место скорость сходимости $q_0 = \frac{M-m}{M+m} < 1$ (в норме 2).
	\end{state}

\section {Оптимальный циклический итерационный метод.}
	\hyperlink {lects.61}{Лекции}\\
	Рассмотрим следующий алгоритм с переменным итерационным параметром:
	$$
	    \frac{x^{k+1} - x^{k}}{\tau_{k+1}} + Ax^{k} = b.
	$$
	Будем считать, что допускается изменение параметра $\tau$ в зависимости от номера итерации циклическим образом с периодом $N$.
	\begin{state} (\hyperlink {lects.61}{Оптимальный $\tau_{k}$})\\
	При условии $A = A^T > 0, \,\, \lambda(A) \in [m, M]$ оптимальные значения $\tau_k$
	равны обратным величинам корней многочлена Чебышева степени $N$ на отрезке $[m, M]$:
	$$
	\tau_k^{-1} = \frac{M+m}{2} + \frac{M-m}{2}\cos\frac{\pi(2k-1)}{2N}, \,\, k = 1,...,N.
	$$
	При этом имеет место скорость сходимости
	$q_1 = \frac{\sqrt{M}-\sqrt{m}}{\sqrt{M}+\sqrt{m}}$ (в норме 2).
	\end{state}
	Здесь появляется важный аспект {\bf упорядочивания шагов}. В каком порядке брать $\tau_k$?
	Ответ: \hyperlink {lects.62}{процедура упорядочивания шагов}.

\section {Обобщённый метод простой итерации.}
	\hyperlink {lects.63}{Лекции}\\
	Если $M/m \gg 1$, то для "улучшения"{} исходной задачи можно перейти к некоторой равносильной системе $B^{-1}Ax = B^{-1}b$ при условии невырожденности $B$:
	$\det(B) \ne 0$.\\
	Эта процедура называется {\it предобуславливанием}.(\hyperlink {lects.63}{подробнее})\\
	ОМПИ часто записывают в виде:
	$$
	    B\frac{x^{k+1} - x^{k}}{\tau} + Ax^{k} = b.
	$$
	Требование $B = B^T > 0$ для сходимости в общем случае не обязательно.
	\begin{state} (\hyperlink {lects.63}{Условие сходимости ОМПИ})\\
	Пусть $A = A^T > 0, \,\, \tau > 0$. Тогда ОМПИ сходится для любого начального
	приближения при условии $B - \frac{\tau}{2}A > 0$.
	\end{state}
	Кратко о \hyperlink {lects.64}{методах релаксации} (Якоби, Гаусса-Зейделя и SOR).
	Подробно эти методы будут описаны в следующих билетах.

\section {Методы Якоби и Гаусса -- Зейделя.}
	\hyperlink {lects.65}{Лекции}\\
	Общий вид этих методов - см. билет 22.\\
	Представим матрицу системы $Ax = b$ в виде $A = L + D + R$, $D$ - диагональ, $L$ и
	$R$ - соотв. левая нижняя и правая верхняя треугольные матрицы с нулевыми диагоналями.
	Будем считать, что все диагональные элементы отличны от нуля.\\
	\begin{defn}
	Невырожденная матрица $A$ обладает свойством {\it диагонального преобладания}, если для
	всех $i$ справедливо
	$$
	    \sum_{j=1,j\ne i}^{n}|a_{ij}| \le q|a_{ii}|, \,\, 0 \le q < 1.
	$$
	\end{defn}
	\noindent{\bf Метод Якоби} ($B = D, \, \tau = 1$):
	$$
	    Dx^{k+1} + (L+R)x^{k} = b.
	$$
	\begin{state} (\hyperlink {lects.65}{Условие сходимости метода Якоби})\\
	Если матрица системы $A$ обладает диагональным преобладанием, то метод Якоби сходится
	с произвольного начального приближения.
	\end{state}
	\noindent{\bf Метод Гаусса-Зейделя} ($B = D + L, \, \tau = 1$):
	$$
	    (D+L)x^{k+1} + Rx^{k} = b.
	$$
	\begin{state} (\hyperlink {lects.65}{Условие сходимости метода Гаусса-Зейделя})\\
	Если матрица системы $A$ обладает диагональным преобладанием, то метод Гаусса-Зейделя
	сходится с произвольного начального приближения.
	\end{state}

\section {Метод верхней релаксации.}
	\hyperlink {lects.67}{Лекции}\\
	Общий вид этих методов - см. билет 22.\\
	\noindent{\bf Метод верхней релаксации (SOR)} ($B = D + \omega L, \, \tau = \omega$):
	$$
	    (D+\omega L)x^{k+1} + (\omega R + (\omega - 1)D)x^{k} = \omega b.
	$$
	Здесь итерационный параметр $\omega$ традиционно называется {\it параметром релаксации}.
	Другая запись:
	$$
	    (D+\omega L)\frac{x^{k+1}-x^{k}}{\omega} + Ax^{k} = b.
	$$
	\begin{state} (\hyperlink {lects.67}{Условие сходимости метода SOR})\\
	Пусть $A = A^T > 0$. Тогда для сходимости метода SOR с произвольного начального
	приближения необходимо и достаточно выполнение неравенства $0 < \omega < 2$.
	\end{state}
	В качестве частного случая ($\omega = 1$) имеем сходимость метода Гаусса-Зейделя для
	симметричных положительно определенных матриц.

\section {Метод наискорейшего градиентного спуска.}
	\hyperlink {lects.68}{Лекции}\\
	Недостатком оптимального одношагового МПИ является необходимость знания $M$ и $m$.
	Наша цель - построение алгоритма (наискорейшего градиентного спуска), имеющего
	аналогичную скорость сходимости, но не использующего информацию о границах спектра.\\
	Заменим исходную СЛАУ задачей отыскания минимума функционала
	$$
	   F(x) = (Ax, x) - 2(b,x).
    $$
    Простейшими из известных методов минимизации функционала являются методы градиентного
    спуска, в которых приближения определяются формулой
    $$
        x^{k+1} = x^k - \delta_k\cdot grad(F(x^k)) = x^k - \Delta_k (Ax^k - b), \,
        \Delta_k = 2\delta_k.
    $$
    Здесь $\delta_k$ - параметр метода, и его выбор определяет конкретный алгоритм.
    Например, его можно определить из условия
    $$
        \delta_k: F(x^{k+1}) = F(x^k - \delta_k\cdot grad(F(x^k))) \rightarrow \min.
    $$
    В этом случае метод называется {\bf методом наискорейшего градиентного спуска} (МНГС).\\
    Обозначим $F_0(y) = (A(y-x), y-x) = ||y-x||_A^2$, где $x$ - точное решение.
    \begin{state} (\hyperlink {lects.69}{Скорость сходимости МНГС})\\
	Пусть $A = A^T > 0, \,\, \lambda(A) \in [m, M], \,\, m > 0$. Тогда приближения $x^k$
	в МНГС удовлетворяют оценке
	$$
	    F_0(x^k) \le \left(\frac{M-m}{M+m}\right)^{2k}F_0(x^0).
	$$
	Заметим, что ассимптотическая скорость тут такая же как у оптимального одношагового МПИ,
	но информация о границах спектра не требуется.
	\end{state}
	Рассчетные формулы в МНГС:
	$$
	    r^k = Ax^k - b,\,\,\, \Delta_k = \frac{(r^k, r^k)}{(Ar^k, r^k)}, \,\,\,
	    x^{k+1} = x^k - \Delta_k r^{k}.
	$$
	\hyperlink {lects.70}{Подробнее} об их недостатках и возможных улучшениях
	(внизу страницы).

\section {Линейная задача наименьших квадратов. Метод нормального уравнения.}
	\hyperlink {lects.71}{Лекции}\\
	Пусть требуется решить СЛУ с прямоугольной матрицей $A$ размерности $m\times n$:
	$$
	    A_{m\times n} x = b, \,\, x\in \mathds{R}^n, \,\, b \in \mathds{R}^m.
	$$
	Рассмотрим 3 случая:\\
	$1)\, m = n, \det(A) \ne 0.$ В этом случае задача имеет единственное решение
	$x = A^{-1}b$ и для вектора невязки $r = b - Ax$ справедливо $||r|| = 0$.\\
	$2)\, m < n, \rk(A) = m.$ Задача недоопределена и исходная система имеет подпространство решений размерности $(n - m)$, причем для каждого решения $||r|| = 0$.\\
	$3)\, m > n, \rk(A) = n.$ Этот случай представляет наибольший интерес.\\
	Будем считать, что $m>n$ и $\rk(A) = n$ (случай 3). Для задач такого рода Гаусс предложил следующую постановку: решением системы $Ax=b$ {\it в смысле наименьших
	квадратов} называется вектор $x$, минимизирующий норму вектора невязки
	$\min\limits_{y}||b - Ay||_2 = ||b - Ax||_2$. Такая постановка называется
	{\bf задачей наименьших квадратов (ЗНК)}.\\
	\noindent{\bf Метод нормального уравнения.}\\
	{\it Суть: умножаем матрицу $A$ и правую часть $b$ на $A^T$ слева, чтобы система стала квадратной.}\\
	$A^T Ax = A^T b$ -- {\it нормальная} система уравнений с квадратной матрицей $A^T A$
	размерности $n\times n$.
	\begin{theorem} (\hyperlink {lects.71}{Теорема Гаусса})\\
	Пусть $m \ge n, \,\, \rk(A) = n$. Тогда нормальная система уравнений имеет единственное
	решение.
	\end{theorem}
	\begin{state} (\hyperlink {lects.71}{Метод нормального уравнения})\\
	Пусть $m \ge n, \,\, \rk(A) = n$. Вектор $x$ -- решение ЗНК $\min\limits_{y}||b-Ay||_2$
	тогда и только тогда, когда $x$ -- решение системы $A^T Ax = A^T b$.
	\end{state}
	Метод нормального уравнения прост в реализации, но чувствителен к ошибкам округления.

\section {Линейная задача наименьших квадратов. Методы QR-разложения и сингулярного разложения.}
	\hyperlink {lects.72}{Лекции}\\
	{\bf ЗНК (задача наим. квадратов)}: $\min\limits_{y}||b - Ay||_2 = ||b - Ax||_2$,\,\, $x$ -- решение в смысле наим. квадратов.\\

	\noindent{\bf Метод $QR$-разложения.}\\
	Этот метод более устойчив к вычислительной погрешности, чем метод нормального уравнения.
	Соответствующее разложение $A = QR$ при $Q^TQ = I, \,\, \det R \ne 0$ можно построить
	методом ортогонализации Грама-Шмидта (\hyperlink {lects.72}{его алгоритм}).
	\begin{state} (\hyperlink {lects.72}{Существование $QR$-разложения})\\
	Пусть $m \ge n, \,\, \rk(A) = n$. Тогда существуют и единственны {\bf ортогональная}
	матрица $Q$ размера $m\times n$ такая, что $Q^T Q = I_n$, и {\bf верхнетреугольная}
	$n\times n$ матрица $R$ с положительными диагональными элементами такие, что $A=QR$.
	\end{state}
	\begin{state} (\hyperlink {lects.72}{Метод $QR$-разложения})\\
	Пусть $m \ge n, \,\, \rk(A) = n$ и известно представление $A=QR$. Тогда решением ЗНК
	является решение системы $Rx = Q^T b$.
	\end{state}
	Формально метод более трудоемкий, но построив однажды такое разложение, можно быстро
	решать задачи с разными правыми частями.
	\noindent{\bf Метод сингулярного (SVD) разложения.}\\
	Метод применяется для решения наилучшим образом плохо обусловленных и вырожденных задач.
	\begin{state} (\hyperlink {lects.73}{Сингулярное (SVD) разложение})\\
	Пусть $A$ -- произвольная матрица размера $m\times n$, причем $m \ge n$. Тогда
	справедливо {\bf сингулярное разложение} $A = U\Sigma V^T$, где
	\begin{itemize}
	\item $U_{m\times n}:\,\, U^TU = I_n$,
	\item $V_{n\times n}:\,\, V^TV = I_n$,
	\item $\Sigma_{n\times n}$ -- диагональная матрица с элементами $\sigma_1 \ge \sigma_2
	\ge ... \ge \sigma_n \ge 0$.
	\end{itemize}
	\end{state}
	Столбцы матрицы $U$ называют {\it левыми сингулярными векторами} матрицы $A$, столбцы
	матрицы $V$ --  {\it правыми сингулярными векторами}, величины $\sigma_i$ --
	{\it сингулярными числами}.\\
	Построив SVD-разложение можно установить, является ли задача вырожденной ($\sigma_n=0$),
    невырожденной ($\sigma_n \ne 0$) или "хорошей"{} ($\sigma_1 / \sigma_n$ не слишком
    велико). Если матрица $A$ квадратная и симметричная, то $\sigma_i = |\lambda_i|$.
    \begin{state} (\hyperlink {lects.73}{Метод SVD-разложения})\\
	Пусть $m \ge n, \,\, \rk(A) = n$ и известно представление $A = U\Sigma V^T$.
	Тогда решением ЗНК является вектор вида $x = V\Sigma^{-1}U^T b$.
	\end{state}
	\hyperlink {lects.73}{Что-то еще про реальные вычисления.}

\section {Общая идея и примеры проекционных методов.}
	\hyperlink {lects.74}{Лекции}\\
	\noindent{\bf Общая идея проекционных методов:}\\
	В зависимости от текущего приближения $x^l \in \mathds{R}^n$ и номера итерации $l$
	выбирают два $m$-мерных подпространства $\mathcal{K}$ и $\mathcal{L}$. Следующее
	приближение $x^{l+1}$ ищут в виде $x^{l+1} = x^l + k, \,\, k\in \mathcal{K}$ из
	условия $r^{l+1} \bot\, \mathcal{L}$, где $r^{l+1} = b - Ax^{l+1}$.\\
	Таким образом, следует построить вектор поправки $k$ из подпространства $\mathcal{K}$,
	обеспечивающий ортогональность вектора невязки $r^{l+1}$ подпространству $\mathcal{L}$.
	Различные правила выбора этих подпространств приводят к разным рассчетным формулам.
	\noindent{\bf Примеры:}\\
	1) \hyperlink {lects.74}{Метод наискорейшего градиентного спуска}\\
	2) \hyperlink {lects.74}{Метод Гаусса-Зейделя}

\section {Пространства Крылова. Понятие о методе сопряженных градиентов.}
	\hyperlink {lects.74}{Лекции}\\
	Пусть пространства $\mcL$ зависят от номера итерации и $\mcL^1 \subset \mcL^2 \subset
	... \subset \mcL^l \subset ... \subset \mcL^n = \mathds{R}^n$. Тогда точное решение
	системы будет получено не позднее, чем за $n$ шагов. Если же цепочка $\mcL^l$ задается
	некоторым оптимальным образом, то можно рассчитывать, что требуемая точность будет
	достигнута значительно раньше.\\
	Эффективные алгоритмы удается построить, если в качестве пространства $\mcK^l$ выбрать
	{\bf пространство Крылова} $\mcK^l = \text{span}\{r, Ar, ..., A^{l-1}r\}$ размерности
	$l$ для $r = b - Ax^0$. При этом пространство $\mcL^l$ определяется либо как
	$\mcL^l = \mcK^l$ (1), либо $\mcL^l = A\mcK^l$ (2). В {\bf методе сопряженных
	градиентов} используется (1) в предположении $A = A^T > 0$.\\
	\hyperlink {lects.75}{Подробнее} о теории метода.\\
	\noindent{\bf Рассчетные формулы метода сопряженных градиентов:}\\
	\begin{tabular}{cl}
	    $x^l = x^{l-1} + \alpha_l k_l$, & $\alpha_l = \frac{(r^{l-1},k_l)}{(Ak_l,k_l)}$,\\
	    $k_{l+1} = r^l + \beta_l k_l$,  & $\beta_l = -\frac{(r^l,Ak_l)}{(Ak_l,k_l)},
	    \,\,k_1 = r^0.$
    \end{tabular}\\
    В методе сопряженных градиентов получаемые приближения удовлетворяют неравенству
    $$
        F_0(x^l)\le\frac{1}{T_l^2\left(-\frac{M+m}{M-m}\right)}F_0(x^0),\,\,
        F_0(x^l) = ||x-x^l||_A^2 = (A(x-x^l),x-x^l),
    $$
    где $T_l(x)$ -- многочлен Чебышева $l$-й степени.\\
    Справедлива следующая оценка скорости сходимости:
    $$
        ||x-x^l||_2 \le \sqrt{\frac{M}{m}}\frac{2q^l}{1+q^{2l}}||x^*-x^0||_2,\,\,\,
        \text{где } q = \frac{\sqrt{M}-\sqrt{m}}{\sqrt{M}+\sqrt{m}}.
    $$
    \hyperlink {lects.76}{Подробности.}

\section {Частичная проблема собственных значений.}
	\hyperlink {lects.78}{Лекции}\\
	$A$ \--- матрица $n*n$. Ищем собственный вектор $x \ne 0$ и собственное значение $\lambda$ : $Ax = \lambda x$.
	\textbf{Степенной метод вычисления максимального по модулю с.з.}:\\
	\begin{center}
	\label{step_met}
	$x^{k+1} = Ax^k$,  $\lambda^{(k)} = \frac{(x^{k+1}, x^k)}{\|x^k\|^2_2}$, $x^k \ne 0$,  $k=0,1,2...$.
	\end{center}
	\textbf{Утверждение}\\
	Пусть $A$ \--- матрица простой структуры (базисные векторы $\{e_i\}^n_1$ - образуют простой базис в $C^n$). Пусть далее $|\lambda_1| > |\lambda_2| \geq |\lambda_3| \geq ... \geq |\lambda_n|$ и $L=span\{e_2,e_3,...,e_n\}$. Тогда для степенного метода (\ref{step_met}), при условии $x^0 \notin L$ справедлива оценка $\lambda^{(k)} = \lambda_1 + O(|\lambda_2/\lambda_1|^k)$.
	
	$\square$ Кратко.\\
	Разложим начальное приближение по собственным векторам, оттуда получим такие же разложения для $x^k, x^{k+1}$. Оцениваем рост их скалярного произведения: $(x^{k+1}, x^k) = c^2_1 \lambda^{2k+1}_1 (e_1, e_1) + 0(|\lambda^{k+1}_1\lambda^k_2|)$.
	Рассматриваем величину $\lambda^{(k)}$, как отношение этой штуки к квадрату нормы $x^k$,  получаем $\lambda_1 + O(|\lambda2/\lambda_1|^k)$. А если матрица симметричная, то можно, учитывая ортогональность с.в. получить степень $2k$. 
	$\square$
	
	
	Для поиска наименьшего с.з. матрицы можно использовать \textbf{метод обратной итерации}. (степенной метод для $A^{-1}$):\\
	\begin{center}
	$x^k := x^k / \|x^k\|_2, Ax^{k+1} = x^k, \lambda^{(k)} = \frac{(x^k, x^{k+1})}{(x^{k+1}, x^{k+1})}$
	\end{center}
	
	\textbf{Замечание}: На каждом шаге придется решать систему: $Ax^{k+1} = x^k$.
\\
Если у нас вырожденная или близкая к тому матрица, то можно использовать эти методы со сдвигом, т.е. применить их к матрице $A-cE$, с достаточно малым $c$.  Если известно приближение к собственному значению $\lambda$ \--- $\overline{\lambda}$, то метод простой итерации с $c = \overline{\lambda}$ - очень быстро сходится. Также, в качестве сдвига $c$ можно взять \textbf{отношение Рэлея}: $R_A(x) = \frac{(Ax, x)}{(x, x)}$, что приводит к кубической сходимости.
	
	

\section {Полная проблема собственных значений. QR-алгоритм.}
	\hyperlink {lects.79}{Лекции}\\
	
Пусть $A$ - матрица $n*n$.
\\
\\
\textbf{$QR$-алгоритм}:
\begin{itemize}
\item Процесс итерационный
\item Начальное условие: $A_0 = A$
\item На каждом шаге находим разложение $A_k = Q_kR_k$ и  вычисляем $A_{k+1} = R_k Q_k $
\item Цель алгоритма - получение предельной матрицы $R_{\infty}$, из диагонали которой извлекается информация о  модулях с.з. исходной матрицы. Сами же значения определяются из структуры $Q_{\infty}$

\item В общем случае алгоритм сходится \textit{по форме} к блочно-треугольной матрице $R$, на диагонали которой модули с.з.
\end{itemize}
\textbf{Утверждение }:
Если $А$ - нормальная вещественная матрица ($A^TA = AA^T$), то последовательность верхнетреугольных матриц $\{R_k\}$ из $QR$-алгоритма сходится к диагональной матрице.
\\
$\square$Кратко. (Все нормы считаем $\|\|_2$, диагональные элементы матрицы R - больше нуля)\\

Рассматриваем две матрицы с соседних шагов алгоритма. Для краткости - $A$ и $B$. \\
Переход от $A$ к $B$: $A = QR$, $B = RQ$\\

Обозначим $b^i, a^i, r^i$ \--- столбцы, а $b_i, a_i, r_i$ - строки матриц $B, A$ и $R$, соответственно. 
Заметим:\\
1) $\|b^i\| = \|r^i\|, \|a_i\| = \|r_i\|$ \--- евклидова норма инвариантна относительно ортогональных преобразований\\
2) $\|a_i\| = \|a^i\|, \|b_i\| = \|b^i\|$ \--- нормальность матриц\\
\\
Вводим $\triangle_m$, как сумму разностей квадратов норм $b_i$ и $a_i$, и выражаем ее через $r_{ij}$:
\begin{center}
$\triangle_m = \sum\limits^m_{i=1} \sum\limits^n_{j=m+1} |r_{ij}|^2, m =2, ... , n-1$
\end{center}

Для каждой матрицы $A_k$ задаем последовательность $\delta^{(k)}_m = \sum\limits^m_{i=1} \|a^{(k)}_i\|$. Из предыдущего равенства, эти последовательности сходятся, и соответствующие им последовательности $\triangle^{(k)}_m = \delta^{(k+1)}_m - \delta^{(k)}_m$ \--- сходятся к нулю, а вместе с ними \--- все наддиагональные элементы матриц $R_k$.

Далее выводим сходимость последовательностей $\{r^{(k)}_{ii}\}_k$, из чего следует существование предела последовательности $\{R_k\}$. $\square$



\section {Метод простой итерации для нелинейных уравнений.}
	\hyperlink {lects.82}{Лекции}\\
	
$H$	 - полное метрическое пространство с метрикой $\rho(x,y)$.\\
$g:H \rightarrow H$.\\

\textbf{Метод простой итерации} для решения уравнения $x = g(x)$ \--- это алгоритм вида $x^{n+1} = g(x^n)$ с некоторым заданным начальным приближением $x^0$.\\
\\
Отображение $g(x)$ называется \textbf{сжимающим}, если для любых $x, y \in H$ справедливо неравенство $\rho(g(x), g(y)) \leq q\rho(x,y)$ с постоянной $0 \leq q < 1$.\\
\\
\textbf{Утверждение}: Если отображение $g(x)$ \--- сжимающее, то уравнение $x=g(x)$  имеет единственное решение $z$ и справедливо неравенство:
\begin{center}
$\rho (z, x^n) \leq \frac{q^na}{1-q}$, где $a = \rho (x^0, x^1)$
\end{center}

Наибольшее число $k$ называется \textbf{порядком метода}, если существуют положительные конечные постоянные $C_1$ и $C_2$ такие, что справедливо неравенство:
\begin{center}
$\rho(x^{n+1}, z) \leq C_2 [\rho(x^n,z)]^k$ 
при условии
 $ \rho (x^n,z) \leq C_1, \forall n \geq 0$
\end{center}

Метод простой итерации - первый порядок с $C_2 = q < 1$\\

Больше порядок - больше сходимость к решению.
	

\section {Метод Ньютона.}
	\hyperlink {lects.83}{Лекции}\\

\subsection {Метод Ньютона}
$z$ - решение уравнения $F(x) = 0$. $x^n$ - приближение к $z$.
\begin{center}
$F(x^n) + F'(x^n)(x^{n+1} - x^n) = 0$, $x^{n+1}$ - решение уравнения
\end{center}
Откуда получилось - \hyperlink {lects.83}{абзац 3, после матрицы Якоби}.

\textbf{Обозначение} $\Omega_h = \{x: \|z-x\|_H < h\}$ - открытая $h$-окрестность решения.

\hyperlink {lects.84}{Утверждение 2}:\\
При условиях:\\
Для некоторых $h,a_1,a_2: 0 <h, 0<a_1,a_2<\infty$:\\
1)$\|(F'(x))^{-1}_y\|_H \leq a_1\|y\|_Y$, $\forall x \in \Omega_h, y \in Y$\\
2)$\|F(u_1)-F(u_2) - F'(u_2)(u_1-u_2)\|_Y \leq a_2\|u_1-u_2\|^2_Y$, $\forall u1,u2 \in \Omega_h$
\\ и $x^0 \in \Omega_b$ метод Ньютона сходится с оценкой погрешности 
\begin{center}
$\|x^n-z\|_H \leq c^{-1} (c\| x^0 - z\|_H)^{2^n}$, 
\end{center} т.е. имеет второй порядок сходимости.

\textbf{Модифицированный метод Ньютона}: 
$x^{n+1} - x^n = -[F'(x^0)]^{-1}F(x^n)$ \\
Меньше вычислений, но медленнее сходится. Подробнее - \hyperlink {lects.84}{в конце страницы}

\subsection {Методы установления}

Обобщение за счет введения переменного итерационного параметра $\triangle_n$: $x^{n+1} - x^n= -\triangle_n [F'(x^n)]^{-1}F(x^n)$. \hyperlink {lects.85}{В конце страницы}
\\
\\
Обобщение на уравнение второго порядка: \hyperlink {lects.86}{последняя здоровая формула}

	

\section {Явный метод Эйлера для обыкновенных дифференциальных уравнений (ОДУ). Устойчивость. Локальная и глобальная ошибки.}
	\hyperlink {lects.87}{Лекции}\\
	Задача Коши для ОДУ: 
	$y'(x)=f(x,y), y(x_0) = y_0$\\
	
	Пусть $x_{n+1} = x_n + h, n \geq 0, h > 0$ - Постоянный шаг интегрирования. Обозначим за $y_n$ приближенное значение точного решения $y(x_n)$. Тогда метод Эйлера для задачи Коши для ОДУ можно записать в виде:\\
	\begin{center}
	$y_{n+1} = y_n + h f(x_n, y_n)$, $y_0$ задано, $n \geq 0$
	\end{center}

Даже для устойчивого ОДУ -  метод может быть как устойчивым, так и неустойчивым.\\
\\
Локальная и глобальная ошибки - \hyperlink {lects.88}{2-й абзац}


\section {Явные методы Рунге -- Кутты.}
	\hyperlink {lects.89}{Лекции}\\
	
	Явные методы Рунге-Кутты - одношаговые методы, т.е. при известном решении в точке $x$ \--- $y(x)$, требуется построить приближение к $y(x+h)$ в точке $x+h$. $h$ - шаг интегрирования.
	
	Достоинства методов:
	\begin{itemize}
	\item Одношаговость даёт схожесть исходной постановки с дифференциальной задачей
	\item Легко менять шаг интегрирования
	\item Вычисления идут по явным формулам, без вспомогательных задач
	\end{itemize}
	Недостатки методов:
	\begin{itemize}
	\item Трудоемкость (k-тый порядок требует k вычислений правой части)
	\item Ограничение на устойчивость 
	\end{itemize}
	\hyperlink {lects.90}{Подробнее - в конце страницы}\\

	\textbf{Построение}: \hyperlink {lects.89}{с конца первого абзаца}\\

\section {Неявные одношаговые методы решения ОДУ.}
	\hyperlink {lects.91}{Лекции}\\
	Интегрируем уравнение $y' = f (x,y)$ на отрезке $[x_n, x_{n+1}]$. Получаем
	\begin{center}
	$y(x_{n+1}) = y (x_n) + I$, где $I = \int^{x_{n+1}}_{x_n} f(x,y(x))dx$
	\end{center}
	Заменяя $I$ на всякое можно получить различные неявные методы:
	\begin{itemize}
	\item[1] Эйлера: $y_{n+1} = y_n + hf(x_{n+1}, y_{n+1})$. \\
	Замена на $(x_{n+1}-x_n)f(x_{n+1}, y(x_{n+1}))$\\
	Порядок точности $s=1$\\
	
	
	\item[2] Кранка-Николсона: $y_{n+1} = y_n + h/2(f(x_n,y_n) + f(x_{n+1}, y_{n+1}))$. \\
	Замена на приближение по методу трапеций\\
	Порядок точности $s=2$\\
	
	\end{itemize}
	
	Т.к. $y_{n+1}$ входит в формулу неявно, то эти методы не просто реализовать.  Часто, для решения таких уравнений используется метод Ньютона или метод функциональной итерации.
	

\section {Многошаговые методы решения ОДУ.}
	\hyperlink {lects.92}{Лекции}\\
	Большой плюс многошаговых методов перед одношаговыми - использование уже вычисленных значений и правых частей в дальнейшем.\\
	Для равноотстоящих узлов общая формула k-шагового метода выглядит следующим образом:\\
	$y_{n+1} = Ф(f;x_{n+1},x_n,...,x_{n-k+1},y_n,...,y_{n-k+1})$\\
	Общая формула двушагового метода:\\
	$y_{n+1} = \alpha_1y_n +\alpha_2y_{n-1} + h*(\beta_0f_{n+1} + \beta_1f_n + \beta_2f_{n-1})$\\ 
	
	Пример - получение метода Адамса - \hyperlink {lects.92}{низ страницы}\\\\

	
	Никакой многошаговый метод не может быть абсолютно устойчивым (даже неявный), если его порядок выше второго. Пример - \hyperlink {lects.93}{3-ий абзац}\\
	\\
	Трудности использования методов: 
	\begin{itemize}
	\item Недостаточно значений для старта алгоритма
	\item Смена шага интегрирования влечет изменение значений к-тов
	\end{itemize}

\section {Основы метода конечных элементов: вариационная постановка задачи, метод Ритца, базисные функции.}
	\hyperlink {lects.97}{Лекции}\\
	
	\textbf{Вариационная постановка задачи} - задачу нахождения классического решения диффура можно заменить на задачу отыскания некоторого функционала, который строится в виде $J(v) = (Lv,v) - 2(f,v)$ или же
	\begin{center}
	$J(v) = \int^1_0 [k(x)(v'(x))^2 + p(x)v^2(x) - 2f(x)v(x)]dx$
	\end{center}
	Подробнее об этом всем - \hyperlink {lects.97}{вся эта страница}\\
	\\
	\hyperlink {lects.98}{Утверждение 1} Пусть $u \in H$,  тогда справедливо
	\begin{center}
	$\|u(x)\|^2_{L_2(0,1)} \leq \|u'(x)\|^2_{L_2(0,1)}$, т.е. $\int^1_0 u^2(x)dx \leq \int^1_0 (u'(x))^2dx$
	\end{center}
	
   \hyperlink {lects.98}{Утверждение 2}
	Пусть достаточно гладкая функция
	 $y$ (например $y \in C^{(2)}[0,1]$, что потребуется в доказательстве при интегрировании по частям) доставляет минимум функционалу $J(v)$ на пространстве $H$.  Тогда справедливы равенства:
	\begin{itemize}
	\item[1] $a(y,v) = (f,v) \forall v \in H;$
	\item[2]$y'(x) = 0$
	\end{itemize}
	
	\hyperlink {lects.99}{Утверждение 3} Пусть $y$ - классическое решение краевой задачи, тогда оно доставляет единственный минимум функционалу $J(v)$ на пространстве $H$.\\
	\hyperlink {lects.99}{Метод Ритца минимизации функционала $J(v)$}:\\
	Ищем приближенное решение в виде $y^n(x) = \sum\limits^n_{j=1}c_j\phi_j(x) \in S^n$, где $\{\phi_j(x)\}^n_{j=1}$ \--- фиксированный набор лин./нез. функций из $H$, $c_j$ \--- к-ты, которые нужно определить.\\
	Рассматриваем $J(y^n)$,  это выражение \--- квадратичное относительно к-тов $c_j$, поэтому задача поиска решения/минимизации это функционала сводится к решению системы линейных уравнений:
	\begin{center}
	$\frac{\partial J(y^n)}{\partial c_i} = 0, i=1,..,n$ или $Ac=b$
	\end{center}
	с симметричной положительно определенной матрицей.\\
	Подробности \hyperlink {lects.100}{см. здесь}

\section {Оценка точности приближения кусочно -- линейными функциями.}
	\hyperlink {lects.102}{Лекции}\\
	
	Ищем оценку достаточно гладких функций из $H$ при приближении эл-тами из $S^n$. \\
	Фиксируем разбиение отрезка с постоянным шагом $x_j = jh, 0 \leq j \leq n, h = 1/n$\\
    Ставим в соответствие функции $y(x)$ \---
	функцию $ y_I (x) = \sum\limits^n_{j=0} y(x_j)\phi_j(x)$\\
	
	\hyperlink {lects.102}{Утверждение}  Пусть $\|y"\|^2 = \int^1_0 [y"(x)]^2dx < \infty$. Тогда
	$\|y' - y'_I\| \leq \frac{h}{\pi}\|y"\|$,  $\|y-y_I\| \leq (\frac{h}{\pi})^2\|y"\|$.

\section {Проекционная теорема в методе конечных элементов.}
	\hyperlink {lects.103}{Лекции}
	\begin{theorem} (\hyperlink {lects.103}{Проекционная теорема МКЭ})\\
	Пусть $y$ -- точка минимума функционала $J(v) = a(v,v) - 2(f,v)$ на пространстве
	$H$, $S^n$ -- конечномерное подпространство $H$. Тогда $y^n$ (точка минимума
	функционала $J(v)$ на $S^n$) существует, единственна и обладает след. свойствами:
	\begin{itemize}
	\item $a(y^n,v^n) = (f,v^n)\,\,\, \forall v^n \in S^n$.
	\item функция $y^n$ есть проекция $y$ на $S^n$ по отношению к энергетическому скалярному
	произведению $a(u,v)$, или, другими словами, ошибка $y-y^n$ ортогональна $S^n$:\,\,
	$a(y-y^n,v^n) = 0 \,\,\, \forall v^n \in S^n$.
	\item минимум $J(v^n)$ и минимум $a(y-v^n, y-v^n)$, где $v^n$ пробегает подпространство
	$S^n$, достигается на одной и той же функции $y^n$, так что:
	$a(y-y^n,y-y^n) = \min\limits_{v^n\in S^n} a(y-v^n,y-v^n)$.
	\end{itemize}
	\end{theorem}
	\noindent$\square$ \hyperlink {lects.104}{Доказательство} $\blacksquare$\\
	Основной смысл теоремы заключается в том, что аппроксимация решения $y$ элементами
	из $S^n$ и устойчивость непрерывной задачи автоматически обеспечивают сходимость в
	энергетической норме $(a(v,v) = ||v||_*^2)$. Это свойство позволяет переложить
	громоздкую техническую работу по вычислению коэффициентов схемы на плечи компьютера,
	т.е. порождает возможность высокой технологичности процесса построения схем МКЭ.
	\hyperlink {lects.105}{Подробнее о следствиях из этой теоремы.}

\section {Система уравнений в методе конечных элементов.}
	\hyperlink {lects.106}{Лекции}\\

\section {Решение модельной задачи методом Фурье.}
	\hyperlink {lects.108}{Лекции}\\

\section {Исследование устойчивости модельной задачи методом Фурье.}
	\hyperlink {lects.111}{Лекции}\\

\section {Метод стрельбы для решения трехдиагональных систем.}
	\hyperlink {lects.112}{Лекции}\\

\section {Пример аппроксимации уравнения и краевых условий.}
	\hyperlink {lects.115}{Лекции}\\

\section {Определения аппроксимации и устойчивости.}
	\hyperlink {lects.118}{Лекции}\\

\section {Определение сходимости. Теорема А.Ф.Филиппова.}
	\hyperlink {lects.120}{Лекции}\\

\section {Интегро -- интерполяционный метод.}
	\hyperlink {lects.121}{Лекции}\\

\section {Исследование устойчивости методом априорных оценок.}
	\hyperlink {lects.125}{Лекции}\\

\section {Метод конечных разностей для уравнения Пуассона.}
	\hyperlink {lects.128}{Лекции}\\

\section {Спектральный признак устойчивости и примеры его применения для аппроксимаций гиперболического уравнения.}
	\hyperlink {lects.130}{Лекции}\\

\section {Принцип замороженных коэффициентов.}
	\hyperlink {lects.132}{Лекции}\\

\section {Исследование устойчивости простейших схем для уравнения теплопроводности в равномерной метрике.}
	\hyperlink {lects.134}{Лекции}\\

\section {Исследование устойчивости схемы с весами для уравнения теплопроводности в интегральной метрике.}
	\hyperlink {lects.136}{Лекции}\\




\includepdf[pages=-, link, linkname = lects]{ch-m_II-20.pdf}
\end{document}
