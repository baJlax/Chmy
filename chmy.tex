\documentclass[specialist, subf, href, colorlinks=true, 12pt, times, mtpro, final]{disser}
\usepackage [russian] {babel}
\usepackage [utf8] {inputenc}
\usepackage {amsmath}
\usepackage {amsthm}
\usepackage {amssymb}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{amsfonts}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage{algorithm}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{pdfpages}
\usepackage{dsfont}

\theoremstyle{definition}
\newtheorem{defn}{Определение}[section]
\newtheorem{example}{Пример}[section]
\newtheorem{state}{Утверждение}[section]
\newtheorem{theorem}{Теорема}[section]

\definecolor{linkcolor}{HTML}{0000FF}
\definecolor{urlcolor}{HTML}{0000FF}
\hypersetup{pdfstartview = FitH, linkcolor = linkcolor, urlcolor = urlcolor, colorlinks = true}

\def\rk{\text{rank}}
\def\mcL{\mathcal{L}}
\def\mcK{\mathcal{K}}

\begin{document}

\tableofcontents

\section {Погрешность метода и вычислительная погрешность. Пример неустойчивого алгоритма.}
    \hyperlink {lects.12}{Лекции} \\
    \begin{defn}
    	\textbf{\hyperlink {lects.13}{Машинная точность}} - максимальная величина $\varepsilon$, для которой верно $1+\varepsilon=1$. \\
    	Или более точно (но непонятно зачем)
    	$$
    		\varepsilon = \frac{1}{2}\beta^{-t}
    	$$
    	где $\beta$ - основание системы счисления (у нас 2), а $t$ - длина мантиссы.\\
    	$\varepsilon \approx 6 \cdot 10^{-8}$ для float\\
    	$\varepsilon \approx 10^{-16}$ для double
    \end{defn}
	\begin{defn}
		\textbf{\hyperlink {lects.13}{Приближенное значение}} (функции). Пусть есть значение $f(x)$, тогда значение $f^*(x)$ будем считать приближенным, если их относительная погрешность меньше машинной точности, т.е.
		$$
			\left| \frac{f^*(x) - f(x)}{f(x)} \right| < \varepsilon \ \ \ \ \ \ \ \forall x \ \in [x-h,x+h]
		$$
	\end{defn}
    \textbf{Описание численного метода:}\\
    Постановка задачи $\rightarrow$ Приближенный метод решения $\rightarrow$ Оценка погрешности (\textit{погрешность метода}) $\rightarrow$ Оценка погрешности с учетом округлений (\textit{влияние вычислительной погрешности})  \\
    \textbf{Пример:} Хотим приблизить $f(x_0)$ по какой-то формуле $F(f, x_0, h, \text{мб еще от чего-то})$. \\
    \textbf{Оценка погрешности метода} - оценка  величины $|F(f,x_0,h...) - f(x_0)|$, получение погрешности в виде $Ch^k$, где $C$ не зависит от $h$. \\
    \textbf{Оценка вычислительной погрешности} - оценка величины $|F(f^*, x_0, h, ...) - f(x_0)|$, получение погрешности в виде $Ch^k + M\varepsilon$, где $C$ - из оценки выше, а $M$ - постоянная, возможно зависящая от $h$.\\
    Подробней и пример с приближением $f'(x_0)$: \hyperlink {lects.12}{Вся эта и следующая страницы}

    \begin{example}
        Пример неустойчивого алгоритма (\hyperlink {lects.14}{Лекции})\\
        Пусть требуется вычислить последовательность интегралов:
        $$
            \int_0^{2\pi}{x^n e^{x-1}dx}, n = 1,2,3,...,N.
        $$
        Для построения численного алгоритма проведем интегрирование по частям:
        $$
        \begin{aligned}
            & I_n = \int_0^{2\pi}{x^n d(e^{x-1})} = x^n e^{x-1} \big|_0^1 - \int_0^{2\pi}{n x^{n-1} e^{x-1}dx} = 1 - n  I_{n-1} \\
            & I_1 = \frac{1}{e}
        \end{aligned}
        $$
        Легко заметить, что при отсутствии ошибок округления погрешность метода равна нулю (точный метод!). Что будет при реальных вычислениях? Рассмотрим ситуацию, когда погрешность возникает только вследствие определения величины $I_n$. Введем обозначение для ошибки $z_n = I_n - I_n^*$. Тогда
        $$
        \begin{aligned}
            & I_n^* = 1 - n I_{n-1}^* \\
            & z_n = - n z_{n-1} = n! (-1)^{n+1} z_1
        \end{aligned}
        $$
        Погрешность очень быстро (факториально!) растёт. Очень скоро это приведет к сильному искажению искомого результата.
    \end{example}
    \noindent Можно ли исправить эту ситуацию? Ответ: да, \hyperlink {lects.15}{вот так}.\\
    {\it(Если кратко, нужно просто вычислять их в обратном
    порядке, тогда ошибка на каждом шаге будет убывать. А начальное значение можно взять
    любым (например нулем), т.к. пока дойдем до нужного номера, ошибка исчезнет.)}

\section {Алгебраическая интерполяция. Многочлен Лагранжа.}
    \hyperlink {lects.15}{Лекции}\\
    Отрезок $[a,b]$ разбит на $n$ узлов $a = x_1 < x_2 < ... < x_n = b$ и в них заданы $f_i$ - значения функции $f$ в $x_i$. Требуется найти многочлен $L_{n-1}(x)$ степени $n - 1$ чтоб $L_{n - 1}(x_i) = f_i$.

    \noindent\textbf{Многочлен Лагранжа $L_{n - 1}(x)$}
    $$
        \begin {array}{lr}
        L_{n - 1}(x) = \sum\limits_{i = 1}^{n} f_i \Phi_i (x), & \Phi_i(x) = \prod\limits_{j = 1, j\ne i}^{n} \frac{x-x_j}{x_i - x_j} \\
        \end {array}
    $$

    \begin{state} (\hyperlink {lects.16}{Лекции})\\
    Пусть n-я производная функции $f(x)$ непрерывна на $[a,b]$, тогда $\forall x \in [a,b] \ \  \exists \xi \in [a,b]$, что справедливо
        $$
            \begin{array}{lr}
            f(x) - L_{n-1}(x) = \frac {f^{(n)}(\xi)}{n!} \omega_n(x), & \omega_n(x) = \prod\limits_{i - 1}^{n}(x-x_i)
            \end{array}
        $$
    \end{state}
    \noindent{\bfСледствие}:
    $$
        ||f(x) - L_{n-1}(x) || \le \frac{||f^{(n)}(x)||}{n!} ||\omega_n||
    $$
    $$
        ||\cdot|| = \underset{x\in [a,b]}{sup} |\cdot|
    $$


\section {Константа Лебега интерполяционного процесса для равноотстоящих узлов.}
    \hyperlink {lects.17}{Лекции}\\
    {\bfКонстанта Лебега интерполяционного процесса} (\hyperlink {lects.17}{Подробно})
    $$
        \begin{array}{lr}
        \lambda_n = \underset{x\in [a,b]}{max} \sum\limits_{i = 1}^n |\Phi_i(x)|, & \Phi_i(x) = \prod\limits_{j = 1, j\ne i}^{n} \frac{x-x_j}{x_i - x_j} \\
        \end{array}
    $$
    \textbf{Нужна она в оценке:}
    $$
        \begin{array}{lr}
        ||L_{n-1} - L_{n-1}^*|| \le \varepsilon ||f(x)|| \lambda_n \\
        \end{array}
    $$
    
    \begin{state} (\hyperlink {lects.17}{Утверждение 1})\\
    $\lambda_n$ при растягивании отрезка (и разбиения в том числе) не зависит от длины отрезка интерполяции.
    \end{state}
    
    \noindent \hyperlink{lects.17}{Утверждение 2.} (Оценка снизу для $\lambda_n\,(n\ge 2)$ для равноотстоящих узлов)
    $$
        \lambda \ge K \frac {2^n}{n^{3/2}}, 
    $$ 
    где $K$ не зависит от $n$. \\
    

\section {Многочлены Чебышёва и их свойства.}
    \hyperlink {lects.18}{Лекции}\\
    \textbf{Способы определения:}
    \begin{enumerate}
        \item Рекуррентно 
              $$
                \begin{array}{lcr}
                    T_0(x) = 1, & T_1(x) = x, & T_{n+1}(x) = 2xT_n(x) - T_{n-1}(x)
                \end{array}
              $$
        \item Тригонометрически $x\in[-1,1]$
              $$
                T_n(x) = cos(n \ arccos(x))
              $$
        \item Разностное уравнение
              $$
                T_n(x) = \frac{1}{2}\left( \left( x + \sqrt{x^2 - 1} \right)^n + \left( x - \sqrt{x^2 - 1} \right)^n \right)
              $$
    \end{enumerate}
    \textbf{Свойства}
    \begin{enumerate}
        \item $|T_n(x)| \le 1$ при $x \in [-1,1]$
        \item $T_n(-x) = (-1)^nT_n(x)$
        \item Коэффициент при старшем члене равен $2^{n - 1}$
        \item Все нули находятся на $[-1,1]$ в точках
            $$
                x_k = cos \frac{\pi (2k -1)}{2n} \ \ \ k = 1, ... , n
            $$
        \item Экстремумов на $[-1,1]$ $n+1$ штук в точках
            $$
                x_m = cos \frac{\pi m}{n} \ \ \ m = 0, ... , n
            $$ 
        \item Отображение на отрезок 
            $$
                T_n^{[a,b]} (x) = (b - a)^n 2^{1-2n} T_n \left( \frac{2x - (b+a)}{b-a} \right)
            $$
    \end{enumerate}

    Приведенный многочлен Чебышёва (со старшим коэффициентом 1)
    $$
        \bar {T_n} = 2^{1 - n}T_n
    $$
    
    \begin{state} (\hyperlink {lects.19}{Утверждение 3})\\
        Приведенный многочлен Чебышёва на $[-1,1]$ наименее отклоняется от нуля. То есть 
        $$
            \underset{[-1,1]}{max}|P_n(x)| \ge \underset{[-1,1]}{max} |\bar{T_n}(x)| = 2^{1-n},
        $$
        где $P_n(x)$ - любой многочлен степени $n$ с коэффициентом 1 при $n$-ой степени.
    \end{state}

    При фиксированном $n$ выбор корней $T_n(x)$ в качестве узлов для многочлена Лагранжа является оптимальным.
    (\hyperlink {lects.20}{Минимизация оценки погрешности})

    

\section {Интерполяционные сплайны. Конструкция и обоснование кубического сплайна.}
    \hyperlink {lects.22}{Лекции}\\
    $a = x_0 < x_1 < ... < x_n = b$ -- сетка (узлы интерполяции).\\
    $P_m(x)$ - множество всех многочленов, степени не выше $m$.
    \begin{defn}
        Полиномиальный и интерполяционный сплайн \hyperlink {lects.22}{Лекции}
    \end{defn}
    Построим {\bf кубический} интерполяционный сплайн. Обозначим через $M_i$ значения
    второй производной $S_3^{''}(x)$ кубического сплайна в узлах $\{x_i\}$, и будем для
    простоты считать, что расстояния между узлами одинаковы, т.е. $x_i - x_{i-1} = h$.
    \begin{state} (\hyperlink {lects.22}{О вторых производных кубического сплайна})\\
    Величины $M_0, M_1, ..., M_n$ удовлетворяют СЛУ $CM=d$, где
    $$
    c_{ij} =
    \begin{cases}
    1/6 &\text{при } j=i\pm 1,\\
    2/3 &\text{при } j=i,\\
    0   &\text{при } |j-i|>1;\\
    \end{cases}
    \,\,\,\, d_i = \frac{f_{i+1}-2f_i+f_{i-1}}{h^2},\,\,\, i,j = 1,...,n-1.
    $$
    \end{state}
    В рассмотренной системе число уравнений на 2 меньше числа неизвестных. Если наложены
    ограничения $M_0 = M_n = 0$, то сплайн называется {\it естественным}.
    (\hyperlink {lects.23}{Лекции, 2ой абзац})
    \begin{state} (\hyperlink {lects.23}{О естественном сплайне})\\
    Естественный сплайн $S_3(x)$ доставляет минимум
    \hyperlink {lects.23}{функционалу энергии} на множестве
    $\Phi = \{\phi\in C^{(2)}[a,b]:\,\,\phi(x_i)=f(x_i)\}$.
    \end{state}
    \noindent \hyperlink{lects.24}{Еще 2 утверждения о естественных сплайнах.}

\section {Понятие об аппроксимационных сплайнах.}
    \hyperlink {lects.25}{Лекции}\\
    В отличии от интерполяционных не требуют пересчета всех коэффициентов при изменении одного (нескольких) значений $f_i$. Потому что строятся локально на каждом $[x_{i-1}, x_i]$ и зависят лишь от некотрого постоянного числа соседних $f_i$ (1, 2, 3 - обычно немного).
    
    Построение аппроксимационного сплайна 3ей степени (\hyperlink {lects.25}{Лекции}).
    
    Оценки для второй и первой производной и для разности (для сплайна 3ей степени) (\hyperlink {lects.27}{Лекции}):
    $$
        f(x) \in C^2[0,1], \ \ \ \ \ |f''(x)|_{[0,1]} < A_2
    $$
    \begin{itemize}
        \item Для второй производной $|B''_2(x_k)| \le A_2$
        \item Для первой производной $\underset{x}{max}|f'(x)-B'_2(x)| \le C_1 h A_2 \ \ \ \ \ \  \ C_1 = \frac{5}{2}$
        \item Просто для разности $\underset{x}{max}|f(x)-B_2(x)| \le C_0 A_2 h^2$
            
    \end{itemize}
    
    
    

\section {Наилучшее приближение в линейном нормированном пространстве.}
    \hyperlink {lects.28}{Лекции}\\
    \begin{defn}
    Задан элемент $f$ линейного нормированного пространства $\mathcal{L}$. Хотим найти его наилучшее приближение комбинацией заданных независимых элементов $g_1, ..., g_n \in \mathcal{L}$. Т.е. найти $x = \sum\limits_{j = 1}^{n}c_j^0g_j$ такой, что
    $$
        ||f - x|| = ||f - \sum\limits_{j = 1}^{n}c_j^0g_j|| = \underset{c_1,...,c_n}{inf} ||f - \sum\limits_{j = 1}^{n}c_jg_j||
    $$
    \end{defn}

    Всегда существует, но не обязательно единственный. (\hyperlink {lects.28}{утв 1})
    
    \begin{defn}  
        Пространство $\mathcal{L}$ строго нормированно, если из условия
        $$
            ||f+g|| = ||f||+||g|| \ \ \ \ \ \ \ \ \ \ ||f||,||g|| \ne 0
        $$
        следует $f = \alpha g, \alpha \ne 0$.
    \end{defn}

    В случае строго нормированного элемент наилучшего приближения единственный (\hyperlink {lects.29}{утв 2})

\section {Наилучшее приближение в гильбертовом пространстве.}
    \hyperlink {lects.29}{Лекции}\\
    Т.е. есть скалярное произведение $(x,y)$ и норма $||x||=\sqrt{(x,x)}$. \\
    Гильбертово пространство - строго нормированное (\hyperlink {lects.29}{утв 3}) \\
    
    Поиск наилучшего приближения - решение СЛАУ. (всегда имеет единственное решение \hyperlink {lects.30}{утв 4})
    $$
        Ac = b, \ \ \ \ \ \ a_{ij} = (g_i, g_j) \ \ \ b_i = (f, g_i)
    $$
    
    \noindent {\bfПример}, приводящий к матрице Гильберта (\hyperlink{lects.31}{Лекции}):\\
    $\mathcal{L}$ - пространство вещественных функций с ограниченным интегралом $\int\limits_0^1 f^2(x)dx < \infty $ и скалярным произведением $(f,g) = \int\limits_0^1 f(x)g(x)dx$. В качестве $g_1, ..., g_n$ выберем систему многочленов $1, x, ..., x^{n-1}$. Тогда элементы матрицы $a_{ij} = (x^{i-1},x^{j-1}) = \frac{1}{i+j-1}$. Это матрица Гильберта, на ней решения находятся с большой погрешностью из-за влияния машинной точности. 

\section {Дискретное преобразование Фурье. Идея быстрого дискретного преобразования Фурье.}
    \hyperlink {lects.32}{Лекции}\\
    \textbf{Традиционное преобразование Фурье}. Пусть $f(x)$ - периодична, период = 1 (т.е. $f(x+1) = f(x)$):
    $$
        f(x) = \sum\limits_{q = -\infty}^{\infty} a_q e^{2\pi i qx},  \ \ \  \ \ \ \sum\limits_{q = \infty}^{\infty}|a_q| < \infty
    $$
    \textbf{Дискретное преобразование Фурье}. Берем сетку с шагом $\frac{1}{N}$ с узлами $x_l = \frac{l}{N}$. Рассматриваем $f(x)$ только в узлах этой решетки, приводим подобные в сумме, получаем:
    $$
        f(x_l) = \sum\limits_{q = 0}^{N-1} A_q e^{2\pi i qx_l} \ \ \ \ \ A_q = \sum\limits_{s = - \infty}^{\infty} a_{q+sN}
    $$
    Словами: \textit{Дискретное преобразование Фурье} - отображение вектора значений функций $f_l, l = 0, ..., N-1$ в вектор коэффициентов $A_q, q = 0, ..., N-1$ разложения по базисным функциям $g_q(x_l) = e^{2\pi i qx_l}$. Эти функции образуют ортонормировнную (т.е. $(g_l,g_k) = 0, l \ne k$) систему относительно скалярного произведения вида $(f,g) = \frac{1}{N}\sum\limits_{l = 0}^{N-1}f_l \bar{g_l}$ (\hyperlink {lects.32}{Док-во}). 
    
    Хотим уметь считать $A_q$. Применяем наилучшее равномерное приближение в Гильбертовом пространстве, получаем конечные формулы:
    $$
        A_q = (f, g_q) = \frac{1}{N}\sum\limits_{l = 0}^{N-1}f_le^{-2\pi i qx_l}
    $$
    
    На практике дискретное преобразование Фурье записывают так (\hyperlink {lects.33}{Зачем так?}):
    $$
        f_l = \sum\limits_{-\frac{N}{2} < q \le \frac{N}{2}} A_q e ^{2\pi i qx}
    $$
    потому что пределы $[0,N-1]$, вообще говоря, можно двигать как угодно.
    
    Но функция $f(x)$ только примерно совпадает со своим дискретным разложением (этот способ аппроксимации носит название тригонометрической интерполяции еще), между узлами может быть разброс (\hyperlink {lects.33}{Пример (4):} $f(x) = a_0 + a_{N-1}e^{2\pi i (N-1)x}$).
    
    \textbf{Идея быстрого преобразования Фурье} \\
    Подсчет всех $A_q$ - это $O(N^2)$ операций. Ускорим.\\
    Перепишем хитро $A_q$ (\hyperlink {lects.34}{Подробно}):
    $$
        A_q = A(q_1, q_2) = \frac{1}{p_2}\sum\limits_{l_2 = 0}^{p_2 - 1} B(q_1, l_2)e^{-2\pi i \frac{ql_2}{N}}
    $$
    $$
        B(q_1, l_2) = \frac{1}{p_1}\sum\limits_{l_1 = 0}^{p_1 - 1}f_{l_2 + l_1p_2}e^{-2\pi i \frac{q_1l_1}{p_1}}
    $$
    тут $p_1p_2 = N$, индексы узла $l$ и коэффициента $A$ $q$ представлены в виде деления на $p_1, p_2$ с остатком: $q = q_1 + p_1q_2, l = l_2 + p_2l_1$.
    
    Теперь чтоб посчитать все $B$ надо $O(p_2p_1^2)$, а чтоб все $A_q$, зная все $B$, надо $O(p_1^2p_2)$. 
    Как получше выбирать $p_1$ и $p_2$: \hyperlink {lects.34}{Конец страницы.}

\section {Наилучшее равномерное приближение многочленами.}
    \hyperlink {lects.35}{Лекции}\\
    $\mathcal{L}$ - пространство ограниченных вещественных функций на $[a,b]$. $||f(x)|| = \underset{[a,b]}{sup}|f(x)|$.
    
    Ищем \textbf{наилучшее приближение} $f\in \mathcal{L}$ в виде
    $$
        Q_n^0 (x) = \sum\limits_{j=0}^{n}a_j^0x^j
    $$
    являющееся решением задачи
    $$
        ||f - Q_n^0|| = \underset{\{a_j\}}{inf}||f-Q_n|| = \underset{\{a_j\}}{inf} \underset{x\in [a,b]}{sup} |f(x) - \sum\limits_{j=0}^{n} a_jx^j|
    $$
    
    Мн-н $Q_n^0$ называется \textbf{многочленом наилучшего равномерного приближения (МНРП) степени n для $f(x)$)}, если для любого многочлена $Q_n$ степени n справедливо $$||f-Q_n^0|| \le ||f-Q_n||$$ Такой элемент всегда есть (как элемент наилучшего приближения в нормированном пр-ве) и единственный для непрерывных функций (утверждение ниже).
    
    \begin{theorem}
        \hyperlink {lects.35}{Валле-Пуссена} Пусть существуют n+2 точки $a \le x_0 < x_1 < ... < x_{n+1} \le b$ и мн-н $Q_n$ такие, что 
        $$
            sgn(f(x_i) - Q_n(x_i)) \cdot (-1)^i = const
        $$
        т.е. разность $f(x) - Q_n(x)$ меняет знак между узлами, то
        $$
            ||f-Q_n^0|| \ge \mu = \underset{i = 0, ..., n+1}{min} |f(x_i) - Q_n(x_i)|
        $$
    \end{theorem}
    \begin{theorem}
        \hyperlink {lects.35}{Чебышёва}. Чтобы $Q_n$ был МНРП необходимо и достаточно существования на $[a,b]$ по крайней мере n+2 точек $x_0 < ... < x_{n+1}$, что
        $$
            f(x_i) - Q_n(x_i) = \alpha (-1)^i ||f-Q_n||
        $$
        где $i = 0, ..., n+1$ и $\alpha = 1$ или $\alpha = -1$ для всех i сразу.\\
        Тут $x_i$ называют точками Чебышевского альтернанса.
    \end{theorem}

    \noindent\textbf{\hyperlink {lects.36}{Пример} к теореме Чебышёва}\\
    $f(x) = sin100x$ МНРП степени 90 на $[0,\pi]$ будет $Q_{90} \equiv 0$. Тут $f(x_i) - Q_{90}(x_i) = (-1)^i$, а узлы $x_i = \frac{\pi/2 +\pi i}{100}, i = 0, ... , 99$.
    
    \begin{state}
        МНРП непрерывной функции единственный (\hyperlink {lects.36}{Утв})
    \end{state}
    \noindent\textbf{Следствие} Если $f(x)$ - непрерывная и четная (нечетная) ф-ия относительно $\frac{a+b}{2}$, то МНРП - четный (нечетный) относительно $\frac{a+b}{2}$ мн-н.
    
    \noindent\textbf{\hyperlink {lects.36}{Пример} нарушения единственности МНРП и теоремы Чебышёва для разрывной функции}
    $$
        f(x) = sgn(x), x\in[-1,1] \ \ \ \ \ Q_1(x) = \alpha x \ \ \ \forall \alpha \in [0,2] 
    $$
    

\section {Квадратурные формулы интерполяционного типа.}
    \hyperlink {lects.37}{Лекции}\\
    Рассматриваем интеграл
    $$
        I(f) = \int_{\Omega}p(x)f(x)dx
    $$
    где $\Omega$ - Конечный или бесконечный промежуток (будем считать $[a,b]$), $f \in F$ - функция из некоторого класса, $p(x)$ - весовая функция (измерима на $\Omega$, $\not\equiv 0$ на $\Omega$ и произведение $p(x)g(x) \ \ \forall g \in F$ суммируемо). \\
    Хотим посчитать интеграл, строим \textbf{линейную квадратурную формулу}
    $$
        S_n(f) = \sum\limits_{i = 1}^{n} c_if(x_i)
    $$
    тут $c_i$ - коэффициенты квадратуры, $x_i$ - узлы квадратуры.\\
    Считаем, что выполнено 
    $$
        I(1) = S_n(1) \ \ \ \ \ \ \ \ \ \text{т.е.} \sum\limits_{i = 1}^n c_i = \int_a^b p(x) dx
    $$
    \textbf{Погрешностью} квадратурной формулы назовем $R_n(f) = I(f) - S_n(f)$, а погрешностью на классе - $R_n(F) = \underset{f\in F}{sup} |R_n(f)|$
    
    \textbf{Квадратурные формулы интерполяционного типа}, т.е. такие, где вместо $f(x)$ используется интерполяционный многочлен Лагранжа (степени n-1).
    $$
        S_n(f) = \int_a^b p(x) L_{n-1}(x) dx
    $$
    явные формулы для коэффициентов и погрешности:
    $$
        c_i = \int_a^b p(x)\Phi_i (x) dx, \ \ \ \ \ \ \ R_n = \frac{||f^{(n)}(x)||}{n!} \int_a^b p(x)|\omega_n(x)| dx
    $$
    где $||f^{(n)}(x)|| = \underset{[a,b]}{max}|f^{(n)}(x)|, \omega_n(x) = \prod\limits_{i = 1}^n (x - x_i), \Phi_i(x) = \prod\limits_{j = 1, j\ne i}^n \frac{x - x_j}{x_i - x_j}, L_{n-1} = \sum\limits_{i = 1}^n f(x_i) \Phi_i(x)$ \\
    
    \begin{defn}
        Квадратурная формула имеет \textbf{алгебраический порядок точности  m}, если $S_n(P_m) = I(P_m)$ для любого многочлна степени не выше m, и $\exists \ \ Q_{m+1}$ (многочлен степени m+1), для которого $S_n(Q_{m+1}) \ne I(Q_{m+1})$. 
    \end{defn}
    \begin{state}
        Квадратурная формула $S_n$ имеет порядок точности $m \ge n-1$ тогда и только тогда, когда она интерполяционная квадратурная формула. (\hyperlink {lects.38}{Утв 1})
    \end{state}

    \textbf{Формулы Ньютона-Котеса} (квадратурные формулы интерполяционного типа при $p(x) = 1$ и на системе равноотстоящих узлов)
    \begin{itemize}
        \item n = 1 ({\bf Формулы прямоугольников})
        $$
            S_1(f) = (b-a)f\left(\frac{a+b}{2}\right) \ \ \ \ \ R_1 = ||f'(x)||\frac{(b - a)^2}{4}
        $$
        Если воспользоваться нечётностью $n$ (утв. ниже, \hyperlink {lects.39}{Лекции}), то можно получить более точную  
        $$
            \tilde{R_1} = ||f''(x)||\frac{(b-a)^3}{24}
        $$
        \item n = 2 ({\bf Формулы трапеций})
        $$
            S_2(f) = \frac{b - a}{2}(f(a) + f(b)) \ \ \ \ \ R_2 = ||f''(x)||\frac{(b-a)^3}{12}
        $$
        \item n = 3 ({\bf Формулы Симпсона})
        $$
            S_3(f) = \frac{b - a}{6}\left(f(a) + 4f\left(\frac{b+a}{2}\right) + f(b)\right) \ \ \ \ \ R_3 = ||f'''(x)||\frac{(b-a)^4}{192}
        $$
        И более точная через четвертую производную
        $$
            \tilde{R_3} = ||f''''(x)||\frac{(b-a)^5}{2880}
        $$
    \end{itemize}

    \begin{state}
        Для формул Ньютона-Котеса справедлива симметричность коэффициентов $c_k = c_{n+1-k}$ (\hyperlink {lects.40}{Утв 2})
    \end{state}

    \textbf{\hyperlink {lects.39}{Недостаток}} формул Ньютона-Котеса - неустойчивость. Появляются отрицательные $c_i$ и $\underset{n\rightarrow \infty}{lim} \left(\sum\limits_{i=1}^n |c_i^{(n)}| \right) = \infty$. А Сравнивая величины $S_n(f)$ и приближенную $S_n(f^*)$ имеем 
    $$
        |S_n(f) - S_n(f^*)| \le \varepsilon \sum\limits_{i = 1}^n |c_i|
    $$
    где $\varepsilon$ - машинная точность. Поэтому формулами пользуются только при небольших n ($\le7$)

\section {Ортогональные многочлены и квадратуры Гаусса.}
    \hyperlink {lects.40}{Лекции}\\
    Хотим построить квадратурную формулу
    $$
        S_n(f) = \sum\limits_{i = 1}^n c_i f(x_i)
    $$ 
    точную для многочленов максимально высокой степени (2n-1). Весовая функция почти всюду положительна. Неизвестными считаем и $c_i$ и  $x_i$ (т.е. $2n$ свободных параметров), правильно выбрав которые хотим добиться точности на многочленах степени $2n-1$. \\
    \textbf{Ортогональные многочлены (\hyperlink {lects.40}{Конец страницы}).}
    Скалярное произведение будем использовать следующего вида
    $$
        (f,g) = \int_a^b p(x)f(x)g(x) dx
    $$
    Пусть есть на $[a,b]$ система ортогональных $(\psi_i, \psi_j) = 0, \ i\ne j,$ многочленов с весом $p(x)$.
    $$
        \{1, \psi_1 (x), ..., \psi_k(x), ...\}
    $$
    тогда любой многочлен $P_l(x)$ степени $l$ ортогонален каждому $\psi_k$ степени $>l$, потому что представляется в виде линейной комбинации 
    $$
        P_l(x) = \sum\limits_{i = 0}^l \alpha_i \psi_i (x) \ \ \ \ \ \ \ \ \ \text{степень } \psi_i \le l
    $$
    \begin{defn}
        \textbf{Ортогональный многочлен} степени $n$ - многочлен, ортогональный любому многочлену степени $<n$.
    \end{defn}
    \noindent\textbf{Он единственный с точностью до домножения на константу.} \\
    
    Как строить систему ортогональных многочленов? - Выбрав какой-то вес $p(x)$ и задав тем самым скалярное пpoизведение, берем систему линейно независимых мн-в $\{1,x,x^2,...,x^k,...\}$ и ортогонализуем методом Грамма-Шмидта с нашим скалярным произведением.\\
    \textbf{Наиболее известные} системы ортогональных многочленов:
    \begin{enumerate}
        \item Многочлены Лежандра. (получаются при весе $p(x) = 1$ на отрезке $[-1.1]$) (\hyperlink {lects.41}{Пример построение первых 4х})
        $$
            \psi_0(x) = 1
        $$
        $$
            \psi_1(x) = x
        $$
        $$
            \psi_2(x) = x^2 - \frac{1}{3}
        $$
        $$
            \psi_3(x) = x^3 - \frac{3}{5}x
        $$
        и т.д.
        $$
            \psi_n(x) = \frac{C_n}{2^n n!} \frac{d^n}{dx^n} (x^2 - 1)^n
        $$
        Тут $C_n$ - какая-нибудь ненулевая константа, чтоб удовлетворяла посчитанным формулам (на ортогональность не влияет)
        \item Многочлены Чебышёва (получаются при весе $p(x) = \frac{1}{\sqrt{1-x^2}}$ на отрезке $[-1, 1]$)
        $$
            T_n(x) = \frac{1}{2}\left( \left( x + \sqrt{x^2 - 1} \right)^n + \left( x - \sqrt{x^2 - 1} \right)^n \right)
        $$
        \item Многочлены Лагерра (получаются при весе $p(x) = e^{-x}$ на отрезке $[0, \infty)$)
        $$
            L_n(x) = \frac{e^x}{n!}\frac{d^n}{dx^n}(e^{-x}x^n)
        $$
        \item Многочлены Эрмита (получаются при весе $p(x) = e^{-x^2}$ на отрезке $(-\infty, \infty)$)
        $$
            H_n(x) = (-1)^n e^{x^2} \frac{d^n}{dx^n}(e^{-x^2})
        $$
    \end{enumerate}

    \begin{state}
        Ортогональный произвольному многочлену степени $<n$ многочлен $\psi_n$ степени $n$ на интервале $(a,b)$ имеет ровно $n$ корней. (\hyperlink {lects.41}{Утв 3})
    \end{state}

    \begin{defn}
        \textbf{Квадратура Гаусса} - квадратура $S_n$ на $n$ точках, точная на многочленах $2n-1$ степени.
    \end{defn}
    
    \begin{state}
        Для $n \ge 1$ существует единственная квадратурная формула Гаусса. (\hyperlink {lects.42}{Утв 3})
    \end{state}
    \begin{proof} (Основные моменты) \\
        Контрпример для $P_{2n}$:
        $$
            P_{2n} = (x-x_1)^2 (x-x_2)^2 ... (x-x_n)^2 \ \ \ \ \ \ \ \ 0=S_n(P_{2n}) \ne I(P_{2n}) > 0
        $$
        Далее сперва предполагается, что квадратура существует. И показываем, что тогда узлами квадратуры были выбраны корни ортогонального многочлена.
        А коэффициенты после этого восстанавливаются по формулам $c_i = \int\limits_a^b p(x)\Phi_i(x)dx$, где $p(x)$ - наш вес, а $\Phi_i(x) = \prod\limits_{j = 1, j\ne i}^n \frac{x-x_j}{x_i-x_j}$. \\
        Потом показываем, что построенная так квадратура является квадратурой Гаусса.\\
        А поскольку ортогональный многочлен с точностью до константы единственный, то набор корней у него единственный. Существование квадратуры вытекает из явных формул для $c_i$.
        
        \textbf{Итог:}
        \begin{itemize}
            \item Узлы квадратуры Гаусса $(x_i)$- корни ортогонального многочлена степени $n$.
            \item Коэффициенты  $c_i = \int\limits_a^b p(x)\Phi_i(x)dx$
        \end{itemize}
        
    \end{proof}
    \noindent\textbf{Пример построения квадратуры Гаусса} для $n=3, \ p(x) = 1$ на отрезке $[-1,1]$ (\hyperlink {lects.42}{Конец страницы}) 
    $$
        S_3(f) = \frac{1}{9} \left( 5f\left( -\sqrt{\frac{3}{5}} \right) + 8f(0) + 5f\left( \sqrt{\frac{3}{5}} \right) \right)
    $$

\section {Составные квадратурные формулы. Правило Рунге для оценки погрешности.}
    \hyperlink {lects.44}{Лекции}\\
    \textbf{Составные квадратурные формулы:}
    Пусть $h = (b-a)/N$ и $x_k = a + kh, \quad k = 0, 1 \ldots N-1$. Введем обозначения: $I^{(k)}(f) = \int_{x_k}^{x_{k+1}} p(x)f(x)dx,$ $\quad S^{(k)}_n(f) := S_n(f)$ для отрезка $[x_k, x_{k+1}], \quad k = 0, \ldots, N-1$. Исходный интеграл равен $I(f) = \sum_{k=0}^{N-1}I^{(k)}(f)$, соответственно составная квадартурная формула принимает вид $S^N_n = \sum_{k=0}^{N-1} S_n^{(k)}(f)$, а для ее погрешности справедливо неравенство $|R_n^N(f)| \leq \sum_{k=0}^{N-1}|R_n^{(k)}(f)|$.
    
     Далее в лекциях пример составной формулы трапеций: \hyperlink {lects.44}{Составная формула трапеций}
     
     Объем вычислительной работы при использовании составных формул растет линейно по N, а оценка погрешности убывает существенно быстрее. Скорость убывания погрешности напрямую зависит от порядка точности формулы.
     \\ \\ 
     \textbf{Правило Рунге для оценки погрешности:}\\
     (Зачем нужно:) При разбиении отрезка на элементарные части важно учитывать поведение интегрируемой функции. Если о ней заранее ничего не известно, то можно проводить разбиение постепенно шаг за шагом, например слева направо. Для очередного элементарного отрезка длины h необходимо уметь оценивать погрешность и принимать решение об уменьшении или увеличении шага интегрирования. 
     \\ \\
     Пусть на отрезке длины h используется квадратурная формула $S_h(f)$, точная для многочленов степени не выше m-1. Разложим f(x) в ряд Тейлора в середине отрезка (точка c):
     $$I(f) - S_h(f) = Df^{(m)}(c)h^{m+1} + O(h^{m+2}), \quad D \not= 0$$ 
     Обозначим через $S_{h/2}(f)$ составную формулу, полученную с помощью формулы $S_h(f)$ для двух половинок отрезка длины h. Тогда при том же D находим:
     $$I(f) - S_{h/2}(f) = Df^{(m)}(c) \frac{h^{m+1}}{2^m} + O(h^{m+2})$$
     Следовательно, с точностью $O(h^{m+2})$ справедливо \textbf{правило Рунге:}
     $$\boxed {I(f) - S_{h/2}(f) \approx \frac{S_{h/2}(f) - S_h(f)}{2^m - 1}}$$
     Поэтому, если мы хотим найти $I(f)$ с абсолютной погрешностью $\varepsilon$ на всем отрезке $[a, b]$, то каждый шаг h следует выбирать из условия:
     $$\left|\frac{S_{h/2}(f) - S_h(f)}{2^m - 1}\right| \leq \frac{h}{b-a}\varepsilon$$

\section {Основные приёмы для вычисления нерегулярных интегралов.}
    \hyperlink {lects.45}{Лекции}\\
    Пусть для вычисления интеграла $I(f) = \int_a^b p(x)f(x)dx$ имеется некоторая квадратурная формула $S_n(f)$. рассмотрим оценку погрешности:
    $$|I(f) - S_n(f)| \leq D \max\limits_{[a, b]}|f^{(m)}(x)|(b-a)^{m+1}, \quad D \not = 0$$

    Она теряет всякий смысл в двух случаях: если по крайней мере один из пределов интегрирования равен бесконечности, или $m$-я производная функции $f(x)$ не ограничена (не существует) на $[a, b]$. Это (формальная неприменимость оценки погрешности квадратурной формулы) и заложено в понятие \textbf{нерегулярных интегралов}. 
    
    Далее в лекциях рассмотрен пример избавления от нерегулярности для $I = \int_0^{\infty} \frac{dx}{(x+1)\sqrt{x}}$. \hyperlink {lects.45}{Это здесь}
    
    Были использованы следующие приемы:
    \begin{itemize}
        \item выделение бесконечности, то есть $\int_0^{\infty} = \int_0^1 + \int_1^{\infty}$
        \item замена переменных $x = \frac{1}{z}, \quad dx = -\frac{dz}{z^2}$
        \item интегрирование по частям (устранение особености)
        \item \hyperlink {lects.46}{выделение весовой функции}
        
    \end{itemize}
    
\section {Метод прогонки для решения трёхдиагональных систем. Корректность и устойчивость метода прогонки.}
    \hyperlink {lects.48}{Лекции}\\
    Решаем систему уравнений с трехдиагональной матрицей:
    
    \begin{equation*}
     \begin{cases}
       c_0 y_0 - b_0 y_1 = f_0, &i = 0,\\
       -a_i y_{i-1} + c_i y_i - b_i y_{i+1} = f_i, &1 \le i \le N-1, \\
       -a_N y_{N-1} + c_N y_N = f_N, & i = N
     \end{cases}
    \end{equation*}
    Основная идея метода состоит в представлении решения в виде:
    $$y_i = \alpha_{i+1} y_{i+1} + \beta_{i+1}, \ \ i = N-1, N-2, \dots , 0$$
    Где $\alpha_i$, $\beta_i$, $y_N$ определяются по элементам матрицы и правой части.\\
    Вывод алгоритма: \hyperlink {lects.48}{Вывод}
    
    \noindent{\bf Сам алгоритм}:
    
    Сначала рекуррентно вычисляются прогоночные коэффициенты $\alpha_i, \beta_i$:
    $$
       \alpha_1 = \frac{b_0}{c_0}, \ \ \alpha_{i+1} = \frac{b_i}{c_i - a_i \alpha_i}
    $$
    $$
       \beta_1 = \frac{f_0}{c_0}, \ \ \beta_{i+1} = \frac{f_i + a_i \beta_i}{c_i - a_i \alpha_i}
    $$
    Затем вычиляется $y_N$:
    $$
       y_N = \frac{f_N + a_N \beta_N}{c_N - a_N \alpha_N},
    $$
    а по нему "снизу вверх"{} вычисляются остальные $y_i$:
    $$
       y_i = \alpha_{i+1} y_{i+1} + \beta_{i+1}
    $$
    Это формулы \emph{правой} прогонки. Формулы \emph{левой} прогонки - исключение неизвестных идет в другом порядке. (Коэффициенты - снизу вверх, $y_i$ - сверху вниз).

    \begin{center}
    {\bf Корректность и устойчивость метода прогонки}
    \end{center} 

    Проблемы: обращение в нуль знаменателя во время вычислений ("корректность"), неустойчивость (ошибка $\varepsilon$ при вычислении $y_N$ приводит к ошибке $(\alpha_1 \dots \alpha_N)\cdot \varepsilon$ для $y_0$) 

    \begin{state} (\hyperlink {lects.50}{Достаточное усл. корректности и устойчивости})\\
    Пусть коэффициенты системы $\in \mathbb{R}$ и такие, что: все $c_0, c_N$, $a_i, b_i, \ \ i = 1, \dots, N-1$ отличны от 0, и
    $$|c_i| \ge |a_i|+|b_i|, i = 1, \dots, N-1, \ \ \ |c_0| \ge |b_0|, |c_N| \ge |a_N|$$
    И хотя бы одно неравенство строгое. Тогда выполнены неравенства:
    $$c_i - a_i\alpha_i \ne 0, \ \ \ |\alpha_i| \le 1, i = 1, \dots, N,$$
    дающие устойчивость и корректность.
    \end{state}
    
    Стоимость вычислений $O(N)$, постоянная в главном члене ассимптотики $\le 8$

\section {Прямые методы решения систем линейных уравнений. Методы Гаусса и Холецкого.}
    \hyperlink {lects.51}{Лекции}\\
    Решается СЛАУ $Ax = b$, $\det(A) \ne 0$.
    \begin{defn}
    Метод называется \textbf{прямым} (или точным), если в предположении отсутствия округления чисел он дает точное решение после конечного числа арифметических и логических операций.
    \end{defn}
    
    Для всех прямых методов оценка погрешности (но не учет влияния вычислительной погрешности) равна 0. Общая идея прямых методов - разложить матрицу в более простые для обращения (треугольная матрица        - легко обратима). Далее описан обратный ход метода Гаусса (если он нужен - добавлю). \hyperlink {lects.51}{[обращение верхнетреугольной матрицы]}. Число арифметических операций - $2n \cdot n/2 =         n^2$. Умножение матрицы на вектор - $2n^2 - n$. Построение разложений в прямых методах - $O(n^3)$. 
    
    \begin{center} \textbf{Метод Гаусса} \end{center}
    \hyperlink {lects.52}{Метод Гаусса} основан на разложении $A = LR$, L - нижнетреугольная ($l_{ii} = 1$), R - верхнетругольная.
{
    Число арифметических операций - $2n^3/3 + O(n^2)$. Матрица предполагается \emph{строго регулярной}, т.е. все ведущие подматрицы невырождены.
    
    \noindent{\bf Сам алгоритм}:

    Пусть исходная матрица $A = A^{(0)}$. Первый шаг - из каждого последующего уравнения вычитаем первое, умноженное на $m_{i1} := a_{i1}/a_{11}$, что равносильно умножению на матрицу $M^{(1)}$: $A^{(1)}x \equiv M^{(1)} A^{(0)}x = M^{(1)}b$, где общий вид $M^{(r)}$:
    \begin{equation*}
    M^{(r)} = \left(
    \begin{array}{cccc}
        I_{r-1,r-1} & & 0 & \\
                   & 1& \ldots & 0\\
        0          & \vdots& \ddots & \vdots\\
                   & -m_{nr} & \ldots & 1
        \end{array}
        \right), \ \ 
        m_{ir} = \frac{a_{ir}^{(r-1)}}{a_{rr}^{(r-1)}}, \ \
        i \ge r + 1
    \end{equation*}
    Общий вид процесса: $M^{(r)}A^{(r-1)} = A^{(r)}, \ 2 \le r \le n-1$.

    В результате получим
    $$M^{(n-1)} \dots M^{(1)} Ax = M^{(n-1)} \dots M^{(1)} b, \ \ \text{или } Rx = c$$
    $$L = \prod \limits_{r=1}^{n-1} M^{(r)}$$
    И задача сводится к обращению верхнетреугольной матрицы $R$. 
        
    Для вычислительной устойчивости необходимо переставлять строки и/или столбцы так, чтобы ведущий элемент был максимальным по модулю.  

    \begin{center} \textbf{Метод Холецкого} \end{center}
    Решается СЛАУ $Ax = b$, $(A = A^T > 0)$. Также называется методом \emph{квадратного корня}. $A$ представляется в виде $A = R^TR$, $R$ - верхнетреугольная.
    
    Число арифметических операций - $n^3/3 + O(n^2)$.

    \hyperlink {lects.53}{Вывод} коэффициентов матрицы $R$ основан на формулах умножения $R^TR$. Итоговые формулы:

    $$
    r_{11} = \sqrt{a_{11}}, \ \ r_{1j} = \frac{a_{1j}}{r_{11}} \ \ (j > 1),
    $$
    $$
    r_{ii} = \sqrt{a_{ii} - \sum \limits_{k=1}^{i-1} r^2_{ki}} \ \ \ (i > 1),  \ \ \ \    r_{ij} = \frac{a_{ij} - \sum \limits_{k = 1}^{i-1} r_{ki} r_{kj}}{r_{ii}} \ \ \ (j > i), \ \ \ \ r_{ij} = 0 \ \ (i > j)
    $$
    После определения матрицы $R$ решение исходной системы сводится к последовательному решению двух систем с треугольными матрицами:
    $$
    R^Ty = b \ \ \text{ и } \ \  Rx = y 
    $$
\section {Прямые методы решения систем линейных уравнений. Методы отражений и вращений.}
    \hyperlink {lects.54}{Лекции}\\

\section {Число обусловленности. Неравенства для ошибки и невязки.}
    \hyperlink {lects.56}{Лекции}\\

\section {Метод простой итерации решения систем линейных уравнений.}
    \hyperlink {lects.58}{Лекции}\\
    Дана СЛАУ $Ax = b$. Преобразуем её к виду $x = Gx + c$. Если решение этой системы находится как предел последовательности
    $$
        x^{k+1} = Gx^{k} + c,
    $$
    то такой процесс называется {\it методом простой итерации} (далее МПИ). $G$ - {\it оператор перехода}.\\
    Для систем со знакоопредленными матрицами МПИ обычно строится в виде
    $$
        \frac{x^{k+1} - x^{k}}{\tau} + Ax^{k} = b,
    $$
    т.е. $G = I - \tau A, \,\, c = \tau b$. $\tau$ - итерационный параметр.
    \begin{state} (\hyperlink {lects.58}{Достаточное условие сходимости МПИ})\\
    Если $||G|| < 1$, то система имеет единственное решение и итерационный процесс сходится
    к решению со скоростью геометрической прогрессии.
    \end{state}
    \begin{state} (\hyperlink {lects.59}{Критерий сходимости МПИ})\\
    Пусть система имеет единственное решение. Итерационный процесс сходится к решению системы тогда и только тогда, когда все собственные значения матрицы $G$ по модулю меньше 1. ($|\lambda_{G}| < 1$)
    \end{state}
    Пусть $A = A^* > 0$. Б.о.о. можем считать, что $\lambda(A) \in [m, M],\,\, m > 0$.
    Метод $ \frac{x^{k+1} - x^{k}}{\tau} + Ax^{k} = b$ сходится при $0 < \tau < \frac{2}{M}$.

\section {Оптимальный одношаговый итерационный метод.}
    \hyperlink {lects.60}{Лекции}\\
    Хотим выяснить, при каком $\tau$ сходимость метода
    $\frac{x^{k+1} - x^{k}}{\tau} + Ax^{k} = b,$ будет наилучшей.
    \begin{state} (\hyperlink {lects.60}{Оптимальный $\tau$})\\
    При условии $A = A^T > 0, \,\, \lambda(A) \in [m, M]$ оптимальное значение
    $$\tau_0 = \frac{2}{m+M}$$
    При этом имеет место скорость сходимости $q_0 = \frac{M-m}{M+m} < 1$ (в норме 2).
    \end{state}

\section {Оптимальный циклический итерационный метод.}
    \hyperlink {lects.61}{Лекции}\\
    Рассмотрим следующий алгоритм с переменным итерационным параметром:
    $$
        \frac{x^{k+1} - x^{k}}{\tau_{k+1}} + Ax^{k} = b.
    $$
    Будем считать, что допускается изменение параметра $\tau$ в зависимости от номера итерации циклическим образом с периодом $N$.
    \begin{state} (\hyperlink {lects.61}{Оптимальный $\tau_{k}$})\\
    При условии $A = A^T > 0, \,\, \lambda(A) \in [m, M]$ оптимальные значения $\tau_k$
    равны обратным величинам корней многочлена Чебышева степени $N$ на отрезке $[m, M]$:
    $$
    \tau_k^{-1} = \frac{M+m}{2} + \frac{M-m}{2}\cos\frac{\pi(2k-1)}{2N}, \,\, k = 1,...,N.
    $$
    При этом имеет место скорость сходимости
    $q_1 \approx \frac{\sqrt{M}-\sqrt{m}}{\sqrt{M}+\sqrt{m}}$ (в норме 2).
    \end{state}
    Здесь появляется важный аспект {\bf упорядочивания шагов}. В каком порядке брать $\tau_k$?
    Ответ: \hyperlink {lects.62}{процедура упорядочивания шагов}.

\section {Обобщённый метод простой итерации.}
    \hyperlink {lects.63}{Лекции}\\
    Если $M/m \gg 1$, то для "улучшения"{} исходной задачи можно перейти к некоторой равносильной системе $B^{-1}Ax = B^{-1}b$ при условии невырожденности $B$:
    $\det(B) \ne 0$.\\
    Эта процедура называется {\it предобуславливанием}.(\hyperlink {lects.63}{подробнее})\\
    ОМПИ часто записывают в виде:
    $$
        B\frac{x^{k+1} - x^{k}}{\tau} + Ax^{k} = b.
    $$
    Требование $B = B^T > 0$ для сходимости в общем случае не обязательно.
    \begin{state} (\hyperlink {lects.63}{Условие сходимости ОМПИ})\\
    Пусть $A = A^T > 0, \,\, \tau > 0$. Тогда ОМПИ сходится для любого начального
    приближения при условии $B - \frac{\tau}{2}A > 0$.
    \end{state}
    Кратко о \hyperlink {lects.64}{методах релаксации} (Якоби, Гаусса-Зейделя и SOR).
    Подробно эти методы будут описаны в следующих билетах.

\section {Методы Якоби и Гаусса -- Зейделя.}
    \hyperlink {lects.65}{Лекции}\\
    Общий вид этих методов - см. билет 22.\\
    Представим матрицу системы $Ax = b$ в виде $A = L + D + R$, $D$ - диагональ, $L$ и
    $R$ - соотв. левая нижняя и правая верхняя треугольные матрицы с нулевыми диагоналями.
    Будем считать, что все диагональные элементы отличны от нуля.
    \begin{defn}
    Невырожденная матрица $A$ обладает свойством {\it диагонального преобладания}, если для
    всех $i$ справедливо
    $$
        \sum_{j=1,j\ne i}^{n}|a_{ij}| \le q|a_{ii}|, \,\, 0 \le q < 1.
    $$
    \end{defn}
    \noindent{\bf Метод Якоби} ($B = D, \, \tau = 1$):
    $$
        Dx^{k+1} + (L+R)x^{k} = b.
    $$
    \begin{state} (\hyperlink {lects.65}{Условие сходимости метода Якоби})\\
    Если матрица системы $A$ обладает диагональным преобладанием, то метод Якоби сходится
    с произвольного начального приближения.
    \end{state}
    \noindent{\bf Метод Гаусса-Зейделя} ($B = D + L, \, \tau = 1$):
    $$
        (D+L)x^{k+1} + Rx^{k} = b.
    $$
    \begin{state} (\hyperlink {lects.65}{Условие сходимости метода Гаусса-Зейделя})\\
    Если матрица системы $A$ обладает диагональным преобладанием, то метод Гаусса-Зейделя
    сходится с произвольного начального приближения.
    \end{state}

\section {Метод верхней релаксации.}
    \hyperlink {lects.67}{Лекции}\\
    Общий вид этих методов - см. билет 22.\\
    \noindent{\bf Метод верхней релаксации (SOR)} ($B = D + \omega L, \, \tau = \omega$):
    $$
        (D+\omega L)x^{k+1} + (\omega R + (\omega - 1)D)x^{k} = \omega b.
    $$
    Здесь итерационный параметр $\omega$ традиционно называется {\it параметром релаксации}.
    Другая запись:
    $$
        (D+\omega L)\frac{x^{k+1}-x^{k}}{\omega} + Ax^{k} = b.
    $$
    \begin{state} (\hyperlink {lects.67}{Условие сходимости метода SOR})\\
    Пусть $A = A^T > 0$. Тогда для сходимости метода SOR с произвольного начального
    приближения необходимо и достаточно выполнение неравенства $0 < \omega < 2$.
    \end{state}
    В качестве частного случая ($\omega = 1$) имеем сходимость метода Гаусса-Зейделя для
    симметричных положительно определенных матриц.

\section {Метод наискорейшего градиентного спуска.}
    \hyperlink {lects.68}{Лекции}\\
    Недостатком оптимального одношагового МПИ является необходимость знания $M$ и $m$.
    Наша цель - построение алгоритма (наискорейшего градиентного спуска), имеющего
    аналогичную скорость сходимости, но не использующего информацию о границах спектра.\\
    Заменим исходную СЛАУ задачей отыскания минимума функционала
    $$
       F(x) = (Ax, x) - 2(b,x).
    $$
    Простейшими из известных методов минимизации функционала являются методы градиентного
    спуска, в которых приближения определяются формулой
    $$
        x^{k+1} = x^k - \delta_k\cdot grad(F(x^k)) = x^k - \Delta_k (Ax^k - b), \,
        \Delta_k = 2\delta_k.
    $$
    Здесь $\delta_k$ - параметр метода, и его выбор определяет конкретный алгоритм.
    Например, его можно определить из условия
    $$
        \delta_k: F(x^{k+1}) = F(x^k - \delta_k\cdot grad(F(x^k))) \rightarrow \min.
    $$
    В этом случае метод называется {\bf методом наискорейшего градиентного спуска} (МНГС).\\
    Обозначим $F_0(y) = (A(y-x), y-x) = ||y-x||_A^2$, где $x$ - точное решение.
    \begin{state} (\hyperlink {lects.69}{Скорость сходимости МНГС})\\
    Пусть $A = A^T > 0, \,\, \lambda(A) \in [m, M], \,\, m > 0$. Тогда приближения $x^k$
    в МНГС удовлетворяют оценке
    $$
        F_0(x^k) \le \left(\frac{M-m}{M+m}\right)^{2k}F_0(x^0).
    $$
    Заметим, что ассимптотическая скорость тут такая же как у оптимального одношагового МПИ,
    но информация о границах спектра не требуется.
    \end{state}
    \noindentРассчетные формулы в МНГС:
    $$
        r^k = Ax^k - b,\,\,\, \Delta_k = \frac{(r^k, r^k)}{(Ar^k, r^k)}, \,\,\,
        x^{k+1} = x^k - \Delta_k r^{k}.
    $$
    \hyperlink {lects.70}{Подробнее} об их недостатках и возможных улучшениях
    (внизу страницы).

\section {Линейная задача наименьших квадратов. Метод нормального уравнения.}
    \hyperlink {lects.71}{Лекции}\\
    Пусть требуется решить СЛУ с прямоугольной матрицей $A$ размерности $m\times n$:
    $$
        A_{m\times n} x = b, \,\, x\in \mathds{R}^n, \,\, b \in \mathds{R}^m.
    $$
    Рассмотрим 3 случая:\\
    $1)\, m = n, \det(A) \ne 0.$ В этом случае задача имеет единственное решение
    $x = A^{-1}b$ и для вектора невязки $r = b - Ax$ справедливо $||r|| = 0$.\\
    $2)\, m < n, \rk(A) = m.$ Задача недоопределена и исходная система имеет подпространство решений размерности $(n - m)$, причем для каждого решения $||r|| = 0$.\\
    $3)\, m > n, \rk(A) = n.$ Этот случай представляет наибольший интерес.\\
    Будем считать, что $m>n$ и $\rk(A) = n$ (случай 3). Для задач такого рода Гаусс предложил следующую постановку: решением системы $Ax=b$ {\it в смысле наименьших
    квадратов} называется вектор $x$, минимизирующий норму вектора невязки
    $\min\limits_{y}||b - Ay||_2 = ||b - Ax||_2$. Такая постановка называется
    {\bf задачей наименьших квадратов (ЗНК)}.\\
    \noindent{\bf Метод нормального уравнения.}\\
    {\it Суть: умножаем матрицу $A$ и правую часть $b$ на $A^T$ слева, чтобы система стала квадратной.}\\
    $A^T Ax = A^T b$ -- {\it нормальная} система уравнений с квадратной матрицей $A^T A$
    размерности $n\times n$.
    \begin{theorem} (\hyperlink {lects.71}{Теорема Гаусса})\\
    Пусть $m \ge n, \,\, \rk(A) = n$. Тогда нормальная система уравнений имеет единственное
    решение.
    \end{theorem}
    \begin{state} (\hyperlink {lects.71}{Метод нормального уравнения})\\
    Пусть $m \ge n, \,\, \rk(A) = n$. Вектор $x$ -- решение ЗНК $\min\limits_{y}||b-Ay||_2$
    тогда и только тогда, когда $x$ -- решение системы $A^T Ax = A^T b$.
    \end{state}
    Метод нормального уравнения прост в реализации, но чувствителен к ошибкам округления.

\section {Линейная задача наименьших квадратов. Методы QR-разложения и сингулярного разложения.}
    \hyperlink {lects.72}{Лекции}\\
    {\bf ЗНК (задача наим. квадратов)}: $\min\limits_{y}||b - Ay||_2 = ||b - Ax||_2$,\,\, $x$ -- решение в смысле наим. квадратов.\\
    \noindent{\bf Метод $QR$-разложения.}\\
    Этот метод более устойчив к вычислительной погрешности, чем метод нормального уравнения.
    Соответствующее разложение $A = QR$ при $Q^TQ = I, \,\, \det R \ne 0$ можно построить
    методом ортогонализации Грама-Шмидта (\hyperlink {lects.72}{его алгоритм}).
    \begin{state} (\hyperlink {lects.72}{Существование $QR$-разложения})\\
    Пусть $m \ge n, \,\, \rk(A) = n$. Тогда существуют и единственны {\bf ортогональная}
    матрица $Q$ размера $m\times n$ такая, что $Q^T Q = I_n$, и {\bf верхнетреугольная}
    $n\times n$ матрица $R$ с положительными диагональными элементами такие, что $A=QR$.
    \end{state}
    \begin{state} (\hyperlink {lects.72}{Метод $QR$-разложения})\\
    Пусть $m \ge n, \,\, \rk(A) = n$ и известно представление $A=QR$. Тогда решением ЗНК
    является решение системы $Rx = Q^T b$.
    \end{state}
    Формально метод более трудоемкий, но построив однажды такое разложение, можно быстро
    решать задачи с разными правыми частями.
    \noindent{\bf Метод сингулярного (SVD) разложения.}\\
    Метод применяется для решения наилучшим образом плохо обусловленных и вырожденных задач.
    \begin{state} (\hyperlink {lects.73}{Сингулярное (SVD) разложение})\\
    Пусть $A$ -- произвольная матрица размера $m\times n$, причем $m \ge n$. Тогда
    справедливо {\bf сингулярное разложение} $A = U\Sigma V^T$, где
    \begin{itemize}
    \item $U_{m\times n}:\,\, U^TU = I_n$,
    \item $V_{n\times n}:\,\, V^TV = I_n$,
    \item $\Sigma_{n\times n}$ -- диагональная матрица с элементами $\sigma_1 \ge \sigma_2
    \ge ... \ge \sigma_n \ge 0$.
    \end{itemize}
    \end{state}
    Столбцы матрицы $U$ называют {\it левыми сингулярными векторами} матрицы $A$, столбцы
    матрицы $V$ --  {\it правыми сингулярными векторами}, величины $\sigma_i$ --
    {\it сингулярными числами}.\\
    Построив SVD-разложение можно установить, является ли задача вырожденной ($\sigma_n=0$),
    невырожденной ($\sigma_n \ne 0$) или "хорошей"{} ($\sigma_1 / \sigma_n$ не слишком
    велико). Если матрица $A$ квадратная и симметричная, то $\sigma_i = |\lambda_i|$.
    \begin{state} (\hyperlink {lects.73}{Метод SVD-разложения})\\
    Пусть $m \ge n, \,\, \rk(A) = n$ и известно представление $A = U\Sigma V^T$.
    Тогда решением ЗНК является вектор вида $x = V\Sigma^{-1}U^T b$.
    \end{state}
    \hyperlink {lects.73}{Что-то еще про реальные вычисления.}

\section {Общая идея и примеры проекционных методов.}
    \hyperlink {lects.74}{Лекции}\\
    \noindent{\bf Общая идея проекционных методов:}\\
    В зависимости от текущего приближения $x^l \in \mathds{R}^n$ и номера итерации $l$
    выбирают два $m$-мерных подпространства $\mathcal{K}$ и $\mathcal{L}$. Следующее
    приближение $x^{l+1}$ ищут в виде $x^{l+1} = x^l + k, \,\, k\in \mathcal{K}$ из
    условия $r^{l+1} \bot\, \mathcal{L}$, где $r^{l+1} = b - Ax^{l+1}$.\\
    Таким образом, следует построить вектор поправки $k$ из подпространства $\mathcal{K}$,
    обеспечивающий ортогональность вектора невязки $r^{l+1}$ подпространству $\mathcal{L}$.
    Различные правила выбора этих подпространств приводят к разным рассчетным формулам.\\
    \noindent{\bf Примеры:}\\
    1) \hyperlink {lects.74}{Метод наискорейшего градиентного спуска}\\
    2) \hyperlink {lects.74}{Метод Гаусса-Зейделя}

\section {Пространства Крылова. Понятие о методе сопряженных градиентов.}
    \hyperlink {lects.74}{Лекции}\\
    Пусть пространства $\mcL$ зависят от номера итерации и $\mcL^1 \subset \mcL^2 \subset
    ... \subset \mcL^l \subset ... \subset \mcL^n = \mathds{R}^n$. Тогда точное решение
    системы будет получено не позднее, чем за $n$ шагов. Если же цепочка $\mcL^l$ задается
    некоторым оптимальным образом, то можно рассчитывать, что требуемая точность будет
    достигнута значительно раньше.\\
    Эффективные алгоритмы удается построить, если в качестве пространства $\mcK^l$ выбрать
    {\bf пространство Крылова} $\mcK^l = \text{span}\{r, Ar, ..., A^{l-1}r\}$ размерности
    $l$ для $r = b - Ax^0$. При этом пространство $\mcL^l$ определяется либо как
    $\mcL^l = \mcK^l$ (1), либо $\mcL^l = A\mcK^l$ (2). В {\bf методе сопряженных
    градиентов} используется (1) в предположении $A = A^T > 0$.\\
    \hyperlink {lects.75}{Подробнее} о теории метода.\\
    \noindent{\bf Рассчетные формулы метода сопряженных градиентов:}\\
    \begin{tabular}{cl}
        $x^l = x^{l-1} + \alpha_l k_l$, & $\alpha_l = \frac{(r^{l-1},k_l)}{(Ak_l,k_l)}$,\\
        $k_{l+1} = r^l + \beta_l k_l$,  & $\beta_l = -\frac{(r^l,Ak_l)}{(Ak_l,k_l)},
        \,\,k_1 = r^0.$
    \end{tabular}\\
    В методе сопряженных градиентов получаемые приближения удовлетворяют неравенству
    $$
        F_0(x^l)\le\frac{1}{T_l^2\left(-\frac{M+m}{M-m}\right)}F_0(x^0),\,\,
        F_0(x^l) = ||x-x^l||_A^2 = (A(x-x^l),x-x^l),
    $$
    где $T_l(x)$ -- многочлен Чебышева $l$-й степени.\\
    Справедлива следующая оценка скорости сходимости:
    $$
        ||x-x^l||_2 \le \sqrt{\frac{M}{m}}\frac{2q^l}{1+q^{2l}}||x^*-x^0||_2,\,\,\,
        \text{где } q = \frac{\sqrt{M}-\sqrt{m}}{\sqrt{M}+\sqrt{m}}.
    $$
    \hyperlink {lects.76}{Подробности.}

\section {Частичная проблема собственных значений.}
    \hyperlink {lects.78}{Лекции}\\
    $A$ \--- матрица $n\times n$. Ищем собственный вектор $x \ne 0$ и собственное значение $\lambda$ : $Ax = \lambda x$.
    \textbf{Степенной метод вычисления максимального по модулю с.з.}:\\
    \begin{center}
    \label{step_met}
    $x^{k+1} = Ax^k$,  $\lambda^{(k)} = \frac{(x^{k+1}, x^k)}{\|x^k\|^2_2}$, $x^k \ne 0$,  $k=0,1,2...$.
    \end{center}
    \textbf{Утверждение}\\
    Пусть $A$ \--- матрица простой структуры (базисные векторы $\{e_i\}^n_1$ - образуют простой базис в $\mathbb{C}^n$). Пусть далее $|\lambda_1| > |\lambda_2| \geq |\lambda_3| \geq ... \geq |\lambda_n|$ и $L=span\{e_2,e_3,...,e_n\}$. Тогда для степенного метода (\ref{step_met}), при условии $x^0 \notin L$ справедлива оценка $\lambda^{(k)} = \lambda_1 + O(|\lambda_2/\lambda_1|^k)$.
    
    \noindent$\square$ Кратко.\\
    Разложим начальное приближение по собственным векторам, оттуда получим такие же разложения для $x^k, x^{k+1}$. Оцениваем рост их скалярного произведения: $(x^{k+1}, x^k) = c^2_1 \lambda^{2k+1}_1 (e_1, e_1) + 0(|\lambda^{k+1}_1\lambda^k_2|)$.
    Рассматриваем величину $\lambda^{(k)}$, как отношение этой штуки к квадрату нормы $x^k$,  получаем $\lambda_1 + O(|\lambda2/\lambda_1|^k)$. А если матрица симметричная, то можно, учитывая ортогональность с.в. получить степень $2k$. 
    $\blacksquare$
    
    
    Для поиска наименьшего с.з. матрицы можно использовать \hyperlink{lects.79}{\bfметод обратной итерации}. (степенной метод для $A^{-1}$):
    \begin{center}
    $x^k := x^k / \|x^k\|_2,\ \  Ax^{k+1} = x^k,\ \  \lambda^{(k)} = \frac{(x^k, x^{k+1})}{(x^{k+1}, x^{k+1})}$
    \end{center}
    
    \noindent\textbf{Замечание}: На каждом шаге придется решать систему: $Ax^{k+1} = x^k$.
\\
Если у нас вырожденная или близкая к тому матрица, то можно использовать эти методы со сдвигом, т.е. применить их к матрице $A-cE$, с достаточно малым $c$.  Если известно приближение к собственному значению $\lambda$ \--- $\overline{\lambda}$, то метод простой итерации с $c = \overline{\lambda}$ - очень быстро сходится. Также, в качестве сдвига $c$ можно взять \textbf{отношение Рэлея}: $R_A(x) = \frac{(Ax, x)}{(x, x)}$, что приводит к кубической сходимости.
    
    

\section {Полная проблема собственных значений. QR-алгоритм.}
    \hyperlink {lects.79}{Лекции}\\
\noindent Пусть $A$ - матрица $n\times n$.
\\
\textbf{$QR$-алгоритм}:
\begin{itemize}
\item Процесс итерационный
\item Начальное условие: $A_0 = A$
\item На каждом шаге находим разложение $A_k = Q_kR_k$ и  вычисляем $A_{k+1} = R_k Q_k $
\item Цель алгоритма - получение предельной матрицы $R_{\infty}$, из диагонали которой извлекается информация о  модулях с.з. исходной матрицы. Сами же значения определяются из структуры $Q_{\infty}$

\item В общем случае алгоритм сходится \textit{по форме} к блочно-треугольной матрице $R$, на диагонали которой модули с.з.
\end{itemize}
\begin{state} (\hyperlink {lects.79}{стр. 79, внизу})
Если $A$ - нормальная вещественная матрица ($A^TA = AA^T$), то последовательность верхнетреугольных матриц $\{R_k\}$ из $QR$-алгоритма сходится к диагональной матрице.
\end{state}
$\square$ Кратко. (Все нормы считаем $\|\|_2$, диагональные элементы матрицы R - больше нуля)\\
Рассматриваем две матрицы с соседних шагов алгоритма. Для краткости - $A$ и $B$. \\
Переход от $A$ к $B$: $A = QR$, $B = RQ$\\
Обозначим $b^i, a^i, r^i$ \--- столбцы, а $b_i, a_i, r_i$ - строки матриц $B, A$ и $R$, соответственно. 
Заметим:\\
1) $\|b^i\| = \|r^i\|, \|a_i\| = \|r_i\|$ \--- евклидова норма инвариантна относительно ортогональных преобразований\\
2) $\|a_i\| = \|a^i\|, \|b_i\| = \|b^i\|$ \--- нормальность матриц\\
Вводим $\triangle_m$, как сумму разностей квадратов норм $b_i$ и $a_i$, и выражаем ее через $r_{ij}$:
\begin{center}
$\triangle_m = \sum\limits^m_{i=1} \sum\limits^n_{j=m+1} |r_{ij}|^2, m =2, ... , n-1$
\end{center}

Для каждой матрицы $A_k$ задаем последовательность $\delta^{(k)}_m = \sum\limits^m_{i=1} \|a^{(k)}_i\|$. Из предыдущего равенства, эти последовательности сходятся, и соответствующие им последовательности $\triangle^{(k)}_m = \delta^{(k+1)}_m - \delta^{(k)}_m$ \--- сходятся к нулю, а вместе с ними \--- все наддиагональные элементы матриц $R_k$.

Далее выводим сходимость последовательностей $\{r^{(k)}_{ii}\}_k$, из чего следует существование предела последовательности $\{R_k\}$. $\blacksquare$



\section {Метод простой итерации для нелинейных уравнений.}
    \hyperlink {lects.82}{Лекции}\\
$H$  - полное метрическое пространство с метрикой $\rho(x,y)$.\\
$g:H \rightarrow H$.\\
\textbf{Метод простой итерации} для решения уравнения $x = g(x)$ \--- это алгоритм вида $x^{n+1} = g(x^n)$ с некоторым заданным начальным приближением $x^0$.\\
Отображение $g(x)$ называется \textbf{сжимающим}, если для любых $x, y \in H$ справедливо неравенство $\rho(g(x), g(y)) \leq q\rho(x,y)$ с постоянной $0 \leq q < 1$.
\begin{state} (\hyperlink {lects.82}{стр. 82})\\
Если отображение $g(x)$ \--- сжимающее, то уравнение $x=g(x)$  имеет единственное решение $z$ и справедливо неравенство:
$$
\rho (z, x^n) \leq \frac{q^na}{1-q},\  \text{где } a = \rho (x^0, x^1)
$$
\end{state}

Наибольшее число $k$ называется \textbf{порядком метода}, если существуют положительные конечные постоянные $C_1$ и $C_2$ такие, что справедливо неравенство:
\begin{center}
$\rho(x^{n+1}, z) \leq C_2 [\rho(x^n,z)]^k$ 
при условии
 $ \rho (x^n,z) \leq C_1, \forall n \geq 0$
\end{center}

Метод простой итерации - первый порядок с $C_2 = q < 1$\\
Больше порядок - больше сходимость к решению.
    

\section {Метод Ньютона.}
    \hyperlink {lects.83}{Лекции}\\
\noindent {\bf Метод Ньютона}\\
$z$ - решение уравнения $F(x) = 0$. $x^n$ - приближение к $z$.
\begin{center}
$F(x^n) + F'(x^n)(x^{n+1} - x^n) = 0$, \ \ $x^{n+1}$ - решение уравнения
\end{center}
Откуда получилось - \hyperlink {lects.83}{абзац 3, после матрицы Якоби}.

\noindent\textbf{Обозначение:} $\Omega_h = \{x: \|z-x\|_H < h\}$ - открытая $h$-окрестность решения.

\begin{state}(\hyperlink {lects.84}{Утверждение 2})\\
Пусть для некоторых $h,a_1,a_2: 0 <h, 0<a_1,a_2<\infty$:\\
1)$\|(F'(x))^{-1}_y\|_H \leq a_1\|y\|_Y$, $\forall x \in \Omega_h, y \in Y$\\
2)$\|F(u_1)-F(u_2) - F'(u_2)(u_1-u_2)\|_Y \leq a_2\|u_1-u_2\|^2_Y$, $\forall u1,u2 \in \Omega_h$\\
При условиях 1), 2) и $x^0 \in \Omega_b$ метод Ньютона сходится с оценкой погрешности 
\begin{center}
$\|x^n-z\|_H \leq c^{-1} (c\| x^0 - z\|_H)^{2^n}$, 
\end{center} т.е. имеет второй порядок сходимости.
\end{state}

\noindent{\bf Модифицированный метод Ньютона}: 
$x^{n+1} - x^n = -[F'(x^0)]^{-1}F(x^n)$ \\
Меньше вычислений, но медленнее сходится. Подробнее - \hyperlink {lects.84}{в конце страницы}

\noindent {\bf Методы установления}

Обобщение за счет введения переменного итерационного параметра $\triangle_n$: $x^{n+1} - x^n= -\triangle_n [F'(x^n)]^{-1}F(x^n)$. \hyperlink {lects.85}{В конце страницы}\\
Обобщение на уравнение второго порядка: \hyperlink {lects.86}{последняя здоровая формула}

    

\section {Явный метод Эйлера для обыкновенных дифференциальных уравнений (ОДУ). Устойчивость. Локальная и глобальная ошибки.}
    \hyperlink {lects.87}{Лекции}\\
    {\bfЗадача Коши} для ОДУ: 
    $$y'(x)=f(x,y),\ \  y(x_0) = y_0$$
    Пусть $x_{n+1} = x_n + h,\  n \geq 0,\  h > 0$ - постоянный шаг интегрирования. Обозначим за $y_n$ приближенное значение точного решения $y(x_n)$. Тогда {\bf метод Эйлера} для задачи Коши для ОДУ можно записать в виде:
$$
    y_{n+1} = y_n + h f(x_n, y_n),\ \  y_0 \text{ задано},\ \  n \geq 0.
$$

Даже для устойчивого ОДУ -  метод может быть как устойчивым, так и неустойчивым.\\
Локальная и глобальная ошибки - \hyperlink {lects.88}{2-й абзац}


\section {Явные методы Рунге -- Кутты.}
    \hyperlink {lects.89}{Лекции}\\
    
    Явные методы Рунге-Кутты - одношаговые методы, т.е. при известном решении в точке $x$ \--- $y(x)$, требуется построить приближение к $y(x+h)$ в точке $x+h$. $h$ - шаг интегрирования.
    
    \noindentДостоинства методов:
    \begin{itemize}
    \item Одношаговость даёт схожесть исходной постановки с дифференциальной задачей
    \item Легко менять шаг интегрирования
    \item Вычисления идут по явным формулам, без вспомогательных задач
    \end{itemize}
    Недостатки методов:
    \begin{itemize}
    \item Трудоемкость (k-тый порядок требует k вычислений правой части)
    \item Ограничение на устойчивость 
    \end{itemize}
    \hyperlink {lects.90}{Подробнее - в конце страницы}\\

    \noindent\textbf{Построение}: \hyperlink {lects.89}{с конца первого абзаца}\\

\section {Неявные одношаговые методы решения ОДУ.}
    \hyperlink {lects.91}{Лекции}\\
    Интегрируем уравнение $y' = f (x,y)$ на отрезке $[x_n, x_{n+1}]$. Получаем
    \begin{center}
    $y(x_{n+1}) = y (x_n) + I$, где $I = \int^{x_{n+1}}_{x_n} f(x,y(x))dx$
    \end{center}
    Заменяя $I$ на всякое можно получить различные неявные методы:
    \begin{itemize}
    \item[1] Эйлера: $y_{n+1} = y_n + hf(x_{n+1}, y_{n+1})$. \\
    Замена на $(x_{n+1}-x_n)f(x_{n+1}, y(x_{n+1}))$\\
    Порядок точности $s=1$\\
    
    
    \item[2] Кранка-Николсона: $y_{n+1} = y_n + h/2(f(x_n,y_n) + f(x_{n+1}, y_{n+1}))$. \\
    Замена на приближение по методу трапеций\\
    Порядок точности $s=2$\\
    
    \end{itemize}
    
    Т.к. $y_{n+1}$ входит в формулу неявно, то эти методы не просто реализовать.  Часто, для решения таких уравнений используется метод Ньютона или метод функциональной итерации.
    

\section {Многошаговые методы решения ОДУ.}
    \hyperlink {lects.92}{Лекции}\\
    Большой плюс многошаговых методов перед одношаговыми - использование уже вычисленных значений и правых частей в дальнейшем.\\
    Для равноотстоящих узлов общая формула k-шагового метода выглядит следующим образом:\\
    $y_{n+1} = Ф(f;x_{n+1},x_n,...,x_{n-k+1},y_n,...,y_{n-k+1})$\\
    Общая формула двушагового метода:\\
    $y_{n+1} = \alpha_1y_n +\alpha_2y_{n-1} + h*(\beta_0f_{n+1} + \beta_1f_n + \beta_2f_{n-1})$\\ 
    
    Пример - получение метода Адамса - \hyperlink {lects.92}{низ страницы}\\\\

    
    Никакой многошаговый метод не может быть абсолютно устойчивым (даже неявный), если его порядок выше второго. Пример - \hyperlink {lects.93}{3-ий абзац}\\
    \\
    Трудности использования методов: 
    \begin{itemize}
    \item Недостаточно значений для старта алгоритма
    \item Смена шага интегрирования влечет изменение значений к-тов
    \end{itemize}

\section {Основы метода конечных элементов: вариационная постановка задачи, метод Ритца, базисные функции.}
    \hyperlink {lects.97}{Лекции}\\
    Хотим решить обыкновенное дифференциальное уравнение второго порядка на отрезке $[0,1]$:
    $$
        Ly \equiv -(k(x)y')' + p(x) y =f(x)
    $$
    $$
        y(0) = 0, \ y'(1) = 1,
    $$
    где $0 \le k_1 \le k(x) \le k_2$, $0 \le p(x) \le p_2$.
    
    \textbf{Вариационная постановка задачи} - задачу нахождения классического решения диффура можно заменить на задачу отыскания минимума некоторого функционала, который строится в виде $J(v) = (Lv,v) - 2(f,v)$ или же
    \begin{center}
    $J(v) = \int^1_0 [k(x)(v'(x))^2 + p(x)v^2(x) - 2f(x)v(x)]dx$
    \end{center}
    Подробнее об этом всем - \hyperlink {lects.97}{вся эта страница}\\
    \\
    \hyperlink {lects.98}{Утверждение 1} Пусть $u \in H$,  тогда справедливо
    \begin{center}
    $\|u(x)\|^2_{L_2(0,1)} \leq \|u'(x)\|^2_{L_2(0,1)}$, т.е. $\int^1_0 u^2(x)dx \leq \int^1_0 (u'(x))^2dx$
    \end{center}
    
   \hyperlink {lects.98}{Утверждение 2}
    Пусть достаточно гладкая функция
     $y$ (например $y \in C^{(2)}[0,1]$, что потребуется в доказательстве при интегрировании по частям) доставляет минимум функционалу $J(v)$ на пространстве $H$.  Тогда справедливы равенства:
    \begin{itemize}
    \item[1] $a(y,v) = (f,v) \forall v \in H;$
    \item[2]$y'(x) = 0$
    \end{itemize}
    
    \hyperlink {lects.99}{Утверждение 3} Пусть $y$ - классическое решение краевой задачи, тогда оно доставляет единственный минимум функционалу $J(v)$ на пространстве $H$.\\
    \hyperlink {lects.99}{Метод Ритца минимизации функционала $J(v)$}:\\
    Ищем приближенное решение в виде $y^n(x) = \sum\limits^n_{j=1}c_j\varphi_j(x) \in S^n$, где $\{\varphi_j(x)\}^n_{j=1}$ \--- фиксированный набор лин./нез. функций из $H$ ($S^n$ -- конечномерное подпространство в $H$), $c_j$ \--- к-ты, которые нужно определить.\\
    Рассматриваем $J(y^n)$,  это выражение \--- квадратичное относительно к-тов $c_j$, поэтому задача поиска решения/минимизации это функционала сводится к решению системы линейных уравнений:
    \begin{center}
    $\frac{\partial J(y^n)}{\partial c_i} = 0, i=1,..,n$ или $Ac=b$
    \end{center}
    с симметричной положительно определенной матрицей ($a_{ij} = a(\varphi_i,\varphi_j), b_j = (f, \varphi_j)$).\\
    Для одномерного случая при произвольном разбиении отрезка \textbf {кусочно-линейные базисные фунции}:
    $$
        \varphi_0(x) = 
        \left\{
        \begin{aligned}
            & \frac{x_1 - x}{x_1 - x_0} \text{, при } x_0 \le x \le x_1,\\
            & 0 \text{, при } x_1 \le x \le x_n.
        \end{aligned}
        \right.
    $$
    $$
        \varphi_n(x) = 
        \left\{
        \begin{aligned}
            & 0 \text{, при } x_0 \le x \le x_{n-1},\\
            & \frac{x - x_{n-1}}{x_n - x_{n-1}} \text{, при } x_{n-1} \le x \le x_n.
        \end{aligned}
        \right.
    $$
    $$
        \varphi_j(x) = 
        \left\{
        \begin{aligned}
            & \frac{x - x_{j-1}}{x_j - x_{j-1}} \text{, при } x_{j-1} \le x \le x_{j},\\
            & \frac{x_{j+1} - x}{x_{j+1} - x_j} \text{, при } x_j \le x \le x_{j+1},\\
            & 0 \text{, иначе.}
        \end{aligned}
        \right.
    $$
    Подробности \hyperlink {lects.100}{см. здесь}

\section {Оценка точности приближения кусочно -- линейными функциями.}
    \hyperlink {lects.102}{Лекции}\\
    
    Ищем оценку достаточно гладких функций из $H$ при приближении эл-тами из $S^n$. \\
    Фиксируем разбиение отрезка с постоянным шагом $x_j = jh, 0 \leq j \leq n, h = 1/n$\\
    Ставим в соответствие функции $y(x)$ \---
    функцию $ y_I (x) = \sum\limits^n_{j=0} y(x_j)\varphi_j(x)$\\
    
    \hyperlink {lects.102}{Утверждение}  Пусть $\|y''\|^2 = \int^1_0 [y''(x)]^2dx < \infty$. Тогда
    $\|y' - y'_I\| \leq \frac{h}{\pi}\|y''\|$,  $\|y-y_I\| \leq (\frac{h}{\pi})^2\|y''\|$.

\section {Проекционная теорема в методе конечных элементов.}
    \hyperlink {lects.103}{Лекции}
    \begin{theorem} (\hyperlink {lects.103}{Проекционная теорема МКЭ})\\
    Пусть $y$ -- точка минимума функционала $J(v) = a(v,v) - 2(f,v)$ на пространстве
    $H$, $S^n$ -- конечномерное подпространство $H$. Тогда $y^n$ (точка минимума
    функционала $J(v)$ на $S^n$) существует, единственна и обладает след. свойствами:
    \begin{itemize}
    \item $a(y^n,v^n) = (f,v^n)\,\,\, \forall v^n \in S^n$.
    \item функция $y^n$ есть проекция $y$ на $S^n$ по отношению к энергетическому скалярному
    произведению $a(u,v)$, или, другими словами, ошибка $y-y^n$ ортогональна $S^n$:\,\,
    $a(y-y^n,v^n) = 0 \,\,\, \forall v^n \in S^n$.
    \item минимум $J(v^n)$ и минимум $a(y-v^n, y-v^n)$, где $v^n$ пробегает подпространство
    $S^n$, достигается на одной и той же функции $y^n$, так что:
    $a(y-y^n,y-y^n) = \min\limits_{v^n\in S^n} a(y-v^n,y-v^n)$.
    \end{itemize}
    \end{theorem}
    \noindent$\square$ \hyperlink {lects.104}{Доказательство} $\blacksquare$\\
    Основной смысл теоремы заключается в том, что аппроксимация решения $y$ элементами
    из $S^n$ и устойчивость непрерывной задачи автоматически обеспечивают сходимость в
    энергетической норме $(a(v,v) = ||v||_*^2)$. Это свойство позволяет переложить
    громоздкую техническую работу по вычислению коэффициентов схемы на плечи компьютера,
    т.е. порождает возможность высокой технологичности процесса построения схем МКЭ.
    \hyperlink {lects.105}{Подробнее о следствиях из этой теоремы.}

\section {Система уравнений в методе конечных элементов.}
    \hyperlink {lects.106}{Лекции}\\
    Хотим получить явный вид системы уравнений метода конечных элементов для одномерного случая при равномерном разбиении отрезка $[0,1]$ с кусочно-линейными базисными фунциями при постоянных $k(x) \equiv k$ и $p(x) \equiv p > 0$.\\
    $a_{ij}$ вычисляются явно:
    $$
        a_{ij} = a_(\varphi_i, \varphi_j) = \int_0^1\left[k(x) \frac{d \varphi_i}{dx} \frac{d \varphi_j}{dx} + p(x) \varphi_i \varphi_j\right]dx =
    $$
    $$
        =
        \left\{
        \begin{aligned}
            & - \frac{k}{h} + \frac{ph}{6} \text{, при } i =j-1,\\
            & \frac{2k}{h} + \frac{2ph}{3} \text{, при } i =j,\\
            & - \frac{k}{h} + \frac{ph}{6} \text{, при } i =j+1,\\
            & 0 \text{, иначе.}
        \end{aligned}
        \right.
    $$
    $b_{j} = \int\limits_{x_{j-1}}^{x_{j+1}} f \varphi_j dx$ -- явно не вычисляется, поэтому заменяем $f$ приближением:
    $$
        \begin{aligned}
            & f_I(x) = \sum_0^n f(x_j) \varphi_j(x) dx\\
        \end{aligned}
    $$
    Получаем новые $\hat{b_j}$:
    $$
        \begin{aligned}
            & \hat{b_j} = \int_{x_{j-1}}^{x_{j+1}} f_I(x) \varphi_j dx = h \frac{f(x_{j+1}) + 4f(x_j) + f(x_{j-1})}{6}\\
        \end{aligned}
    $$
    с точностью $|b_j - \hat{b_j}| = O(h^3)$.
    В итоге система принимает вид:
    $$
        \left\{
        \begin{aligned}
            & c_0 = 0,\\
            & -k \frac{c_{j+1} - 2 c_j + c_{j-1}}{h} + ph \frac{c_{j+1} + 4 c_j + c_{j-1}}{6} = h \frac{f(x_{j+1}) - 2 f(x_j) + f(x_{j-1})}{6},\\
            & k \frac{c_n - c_{n-1}}{h} + ph \frac{2c_n + c_{n-1}}{6} = h \frac{2f(x_n) + f(x_{n-1})}{6}.
        \end{aligned}
        \right.
    $$

\section {Решение модельной задачи методом Фурье.}
    \hyperlink {lects.109}{Лекции}\\

\section {Исследование устойчивости модельной задачи методом Фурье.}
    \hyperlink {lects.111}{Лекции}\\

\section {Метод стрельбы для решения трехдиагональных систем.}
    \hyperlink {lects.112}{Лекции}\\
    
\noindent Хотим найти решение трехдиагональной СЛУ:
    
\begin{equation}
\begin{aligned}
c_0 y_0 -  b_o y_1 &= f_0 \\
-a_i y_{i-1} + c_i y_i - b_i y_{i+1} &= f_i,\ i = 1,\ 2,\ ...,\ N-1 \\
-a_N y_{N-1} + c_N y_N &= f_N
\end{aligned}
\end{equation}

\noindent Считаем все $a_i,\ b_i$ отличными от нуля, иначе система разбивается на трехдиагональные подсистемы. 

\noindent \hyperlink {lects.112}{Идея в терминах дифференциальных уравнений}

\noindent \textbf{Метод решения СЛУ:} ищем $y_i = \delta u_i + (1 - \delta) v_i$, где $\delta$ - параметр и
\begin{equation}
\begin{gathered}
c_0 u_0 -  b_0 u_1 = f_0,\ c_0 v_0 -  b_0 v_1 = f_0 \\
-a_i u_{i-1} + c_i u_i - b_i u_{i+1} = f_i,\ -a_i v_{i-1} + c_i v_i - b_i v_{i+1} = f_i,\ i = 1,\ 2,\ ...,\ N-1
\end{gathered}
\end{equation}

К этим системам для однозначного определения $u_i$ и $v_i$ добавляем начальные условия на $u_0$ и $v_0$ при $b_0 \ne 0$ или на $u_1$ и $v_1$ при $b_0 = 0$ соответственно. В итоге, стартуя с них, последовательно вычисляем $u_2,\ ..,\ u_N$ и $v_2,\ ..,\ v_N$. Для завершения из последнего уравнения системы и выражения y через u и v определяем 
$$
\delta = \frac{f_N + a_N v_{N-1} - c_N v_N}{a_N (v_{N-1} - u_{N-1}) + c_N (u_{N} - v_{N})}
$$

\noindent \textbf{Примечание:} метод стрельбы - хорошее дополнение к методу прогонки, так как области их корректности почти не пересекаются.

\noindent \hyperlink {lects.113}{Пример решения системы}

\section {Пример аппроксимации уравнения и краевых условий.}
    \hyperlink {lects.115}{Лекции}\\
    Рассмотрим уравнение: $-y'' + p(x)y = f(x), y(0) = 0, y'(1) = 0$.\\
    $x_i = ih, i = 0, \dots, N$ -- сетка на $[0,1]$. Аппроксимируем уравнение: первую производную ищем в виде линейной функции от значений на двух соседних узлах $x, x+h$ методом неопределенных коэффициентов; вторую производную -- аналогично, но от значений на трех узлах $x, x\pm h$. Теперь аппроксимируем краевые: в нуле аппроксимируем точно $y_0 = 0$; в единице запишем $y(x_{N-1})$ распишем в ряд Тейлора и получим:
    $\frac{y(x_N)-y(x_{N-1})}{h} + \frac{h}{2}y''(x_N) + O(h^2)$. После чего подставим $y''$ из исходного уравнения и получим в итоге:
    \begin{center}
    	$-\frac{y_{i-1} - 2y_i + y_{i+1}}{h^2} + p_iy_i = f_i, 0 < i < N$,\\
    	$y_0 = 0, \frac{y_N - y_{N-1}}{h} + \frac{h}{2}(p_Ny_N - f_N) = 0$.
    \end{center}

\section {Определения аппроксимации и устойчивости.}
    \hyperlink {lects.118}{Лекции}\\
    $L_hu_h = f_h в D_h$ -- разностная схема для $Lu = f$.\\
    \textbf{Локальная аппроксимация.} Будем говорить, что $L_h$ локально аппроксимирует оператор $L$, если для достаточно гладкой $u \exists h_0, c, p: \forall h \leq h_0$ 		верно:
    \begin{center}
    	$| [ L_h(u)_h - (Lu)_h ]|_{x=x_i}| \leq ch^p$, где $p$ -- порядок аппроксимации
    \end{center}
    \textbf{Общая аппроксимация.} Добавим в схему граничное условие $l_hu_h = \phi_h в \Gamma_h$ для $lu=\phi$. Будем говорить, что схема аппроксимирует дифф задачу, 		если для достаточно гладких $\forall u\in U, f\in F, \phi \in \Phi \exists h_0, c_1, p_1, c_2, p_2: \forall h \leq h_0$ верны:
    \begin{center}
    	$\|L_h(u)_h - (Lu)_h\|_{F_h} + \| (f)_h - f_h\|_{F_h}\leq c_1h^{p_1}$\\
    	$\|l_h(u)_h - (lu)_h\|_{\Phi_h} + \| (\phi)_h - \phi_h\|_{\Phi_h}\leq c_2h^{p_2}$
    \end{center}
    Внутри норм -- погрешности аппроксимации. $p = min(p_1, p_2)$
    Если погрешность аппроксимации стремится к нулю при любом законе стремления шагов по различным переменным к нулю, то такая аппроксимация -- безусловная.
    \textbf{Устойчивость.} Разностная схема -- устойчива, если решение существует, единственно и непрерывно зависит от входных $f_h, \phi_h$, то есть:
    \begin{center}
    	$\|u_h^{(1)} - u_h^{(2)}\|_{U_h} \leq C_1\| f_h^{(1)} - f_h^{(2)}\|_{F_h} + C_2\| \phi_h^{(1)} - \phi_h^{(2)}\|_{\Phi_h}$\\
    \end{center}
    Устойчивость безусловна, если неравенства выполняются при любом соотношении шагов.

\section {Определение сходимости. Теорема А.Ф.Филиппова.}
    \hyperlink {lects.120}{Лекции}\\
    \textbf{Сходимость.} Решение $u_h$ разностной схемы сходится к решению $u$ дифф задачи, если
    \begin{center}
    	$\|u_h^{(1)} - u_h^{(2)}\| _{U_h} \rightarrow 0$, при $h\rightarrow 0$.\\
    \end{center}
    Порядок изменения выражения под нормой относительно $h$ -- порядок сходимости.
    \begin{theorem} (\hyperlink {lects.120}{Проекционная теорема МКЭ})\\
    	Пусть выполнены условия:
    	\begin{itemize}
    		\item Операторы $L, l, L_h, l_h$ -- линейные.
    		\item Решение дифф задачи $\exists !$
    		\item Разностная схема аппроксимирует дифф задачу с порядком p.
    		\item Разностная схема устойчива
    	\end{itemize}
    	Тогда решение разностной схемы $u_h$ сходится к решению дифф задачи с порядком не ниже $p$
    \end{theorem}

\section {Интегро -- интерполяционный метод.}
    \hyperlink {lects.121}{Лекции}\\
    Этот метод называется также методом баланса. Его используют для физических процессов, в которых есть законы сохранения. Когда выводят диффуры матфизики, то исходят из интегральных соотношений (уравнений баланса), выражающего закон сохранения для малого объема. Разностные схемы, выражающие законы сохранения, называются дивергентными или консервативными.\\
    Для получения консервативных разностных схем нужно брать уравнения баланса для узлов сетки и заменять интегралы и производные приближенными разностными выражениями. В результате получаем разностную схему. Такой метод получения консервативных разностных схем -- интегро-интерполяционный метод.

\section {Исследование устойчивости методом априорных оценок.}
    \hyperlink {lects.125}{Лекции}\\
    Хочим получить оценку вида: $\| u\|_{U} \leq \Phi(f)$,где f - входные данные. Например для задачи теплопроводности: $-(k(x)y') + p(x)y = f(x), y(0) = 0, y'(1) = 0$. Делается в 3 этапа:
    \begin{itemize}
    	\item Интегральное тождество: Умножаем на $y(x)$. Берем интеграл от 0 до 1 по $x$. В левой части берем по частям и получаем:
    	\begin{center} $\int^1_0k(x)(y')^2dx + \int^1_0p(x)y^2dx = \int^1_0fydx$. \end{center}
    	\item Неравенство для функции и ее производной: $\| u(x)\|^2_{L_2(0,1)} \leq \| u'(x)\|^2_{L_2(0,1)}$.
    	\item Априорная оценка: Возьмем интегральное тождество, слева оцениваем снизу, а сверху Коши-Буняковским: 
    	\begin{center} $\| y(x)\|_{L_2(0,1)} \leq \frac{1}{k_{min}+p_{min}}\| f(x)\|_{L_2(0,1)}$. \end{center}
    	Из этого следует единственность решения и непрерывная зависимость от входных данных.
    \end{itemize}
    Для разностного случая нужно получить оценку: $\| u_h\|_{U_h} \leq \Phi(f_h)$. (Для теплопроводности она получается такой же, как для гладкой функции.)

\section {Метод конечных разностей для уравнения Пуассона.}
    \hyperlink {lects.128}{Лекции}\\

\section {Спектральный признак устойчивости и примеры его применения для аппроксимаций гиперболического уравнения.}
    \hyperlink {lects.130}{Лекции}\\

\section {Принцип замороженных коэффициентов.}
    \hyperlink {lects.132}{Лекции}\\

\section {Исследование устойчивости простейших схем для уравнения теплопроводности в равномерной метрике.}
    \hyperlink {lects.134}{Лекции}\\

\section {Исследование устойчивости схемы с весами для уравнения теплопроводности в интегральной метрике.}
    \hyperlink {lects.136}{Лекции}\\




\includepdf[pages=-, link, linkname = lects]{ch-m_II-20.pdf}
\end{document}
